{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization and Optimization of Neural Networks - Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Recall from the last lab that we had a training accuracy close to 90% and a test set accuracy close to 76%.\n",
    "\n",
    "As with our previous machine learning work, we should be asking a couple of questions:\n",
    "- Is there a high bias? yes/no\n",
    "- Is there a high variance? yes/no\n",
    "\n",
    "Also recall that \"high bias\" is a relative concept. Knowing we have 7 classes and the topics are related, we'll assume that a 90% accuracy is pretty good and the bias on the training set is low. (We've also discussed concepts like precision, recall as well as AUC and ROC curves.)   \n",
    "\n",
    "In this lab, we'll use the notion of training/validation/test set to get better insights of how we can mitigate our variance, and we'll look at a few regularization techniques. You'll start by repeating the process from the last section: importing the data and performing preprocessing including one-hot encoding. Then, just before you go on to train the model, we'll introduce how to include a validation set. You'll then define and compile the model as before. This time, when you are presented with the `history` dictionary of the model, you will have additional data entries for not only the train and test, but the train, test and validation  and then defigning, compiling and training the model. \n",
    "\n",
    "\n",
    "## Objectives\n",
    "\n",
    "You will be able to:\n",
    "\n",
    "* Construct and run a basic model in Keras\n",
    "* Construct a validation set and explain potential benefits\n",
    "* Apply L1 and L2 regularization\n",
    "* Aplly dropout regularization\n",
    "* Observe and comment on the effect of using more data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the libraries\n",
    "\n",
    "As usual, start by importing some of the packages and modules that you intend to use. The first thing we'll be doing is importing the data and taking a random sample, so that should clue you in to what tools to import. If you need more tools down the line, you can always import additional packages later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Your code here; import some packages/modules you plan to use\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn import preprocessing\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "\n",
    "As with the previous lab, the data is stored in a file **Bank_complaints.csv**. Load and preview the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>Consumer complaint narrative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>In XX/XX/XXXX I filled out the Fedlaon applica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>I am being contacted by a debt collector for p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>I cosigned XXXX student loans at SallieMae for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>Navient has sytematically and illegally failed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>My wife became eligible for XXXX Loan Forgiven...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Product                       Consumer complaint narrative\n",
       "0  Student loan  In XX/XX/XXXX I filled out the Fedlaon applica...\n",
       "1  Student loan  I am being contacted by a debt collector for p...\n",
       "2  Student loan  I cosigned XXXX student loans at SallieMae for...\n",
       "3  Student loan  Navient has sytematically and illegally failed...\n",
       "4  Student loan  My wife became eligible for XXXX Loan Forgiven..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Your code here; load and preview the dataset\n",
    "raw_df = pd.read_csv('Bank_complaints.csv')\n",
    "raw_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Overview\n",
    "\n",
    "Before we begin to practice some of our new tools regarding regularization and optimization, let's practice munging some data as we did in the previous section with bank complaints. Recall some techniques:\n",
    "\n",
    "* Sampling in order to reduce training time (investigate model accuracy vs data size later on)\n",
    "* One-hot encoding our complaint text\n",
    "* Transforming our category labels\n",
    "* Train - test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: Generate a Random Sample\n",
    "\n",
    "Since we have quite a bit of data and training networks takes a substantial amount of time and resources, we will downsample in order to test our initial pipeline. Going forward, these can be interesting areas of investigation: how does our models performance change as we increase (or decrease) the size of our dataset?  \n",
    "\n",
    "Generate the random sample using seed 123 for consistency of results. Make your new sample have 10,000 observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here\n",
    "sample_df = raw_df.sample(n = 10000, random_state = 123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: One-hot Encoding of the Complaints\n",
    "\n",
    "As before, we need to do some preprocessing and data manipulationg before building the neural network. Last time, we guided you through the process, and now its time for you to practice that pipeline independently.  \n",
    "\n",
    "Only keep 2,000 most common words and use one-hot encoding to reformat the complaints into a matrix of vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequences type: <class 'list'>\n",
      "one_hot_results type: <class 'numpy.ndarray'>\n",
      "Found 21527 unique tokens.\n",
      "Dimensions of our coded results: (10000, 2000)\n"
     ]
    }
   ],
   "source": [
    "#Your code here; use one-hot encoding to reformat the complaints into a matrix of vectors.\n",
    "#Only keep the 2000 most common words.\n",
    "complaints = sample_df[\"Consumer complaint narrative\"] #Our raw text complaints\n",
    "\n",
    "tokenizer = Tokenizer(num_words=2000) #Initialize a tokenizer.\n",
    "\n",
    "tokenizer.fit_on_texts(complaints) #Fit it to the complaints\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(complaints) #Generate sequences\n",
    "print('sequences type:', type(sequences))\n",
    "\n",
    "one_hot_results= tokenizer.texts_to_matrix(complaints, mode='binary') #Similar to sequences, but returns a numpy array\n",
    "print('one_hot_results type:', type(one_hot_results))\n",
    "\n",
    "word_index = tokenizer.word_index #Useful if we wish to decode (more explanation below)\n",
    "\n",
    "print('Found %s unique tokens.' % len(word_index)) #Tokens are the number of unique words across the corpus\n",
    "\n",
    "\n",
    "print('Dimensions of our coded results:', np.shape(one_hot_results)) #Our coded data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: Encoding the Products\n",
    "\n",
    "Similarly, now transform the descriptive product labels to integers labels. After transforming them to integer labels, retransform them into a matrix of binary flags, one for each of the various product labels.  \n",
    "  \n",
    "  (Note: this is similar to our previous work with dummy variables: each of the various product categories will be its own column, and each observation will be a row. Each of these observation rows will have a 1 in the column associated with it's label, and all other entries for the row will be zero.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original class labels:\n",
      "['Bank account or service', 'Checking or savings account', 'Consumer Loan', 'Credit card', 'Credit reporting', 'Mortgage', 'Student loan']\n",
      "\n",
      "\n",
      "New product labels:\n",
      "[2 0 0 ... 0 6 6]\n",
      "\n",
      "\n",
      "One hot labels; 7 binary columns, one for each of the categories.\n",
      "[[0. 0. 1. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n",
      "\n",
      "\n",
      "One hot labels shape:\n",
      "(10000, 7)\n"
     ]
    }
   ],
   "source": [
    "#Your code here; transform the product labels to numerical values\n",
    "#Then transform these integer values into a matrix of binary flags\n",
    "product = sample_df[\"Product\"]\n",
    "\n",
    "le = preprocessing.LabelEncoder() #Initialize. le used as abbreviation fo label encoder\n",
    "le.fit(product)\n",
    "print(\"Original class labels:\")\n",
    "print(list(le.classes_))\n",
    "print('\\n')\n",
    "product_cat = le.transform(product)  \n",
    "#list(le.inverse_transform([0, 1, 3, 3, 0, 6, 4])) #If you wish to retrieve the original descriptive labels post production\n",
    "\n",
    "print('New product labels:')\n",
    "print(product_cat)\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "print('One hot labels; 7 binary columns, one for each of the categories.') #Each row will be all zeros except for the category for that observation.\n",
    "product_onehot = to_categorical(product_cat)\n",
    "print(product_onehot)\n",
    "print('\\n')\n",
    "\n",
    "print('One hot labels shape:')\n",
    "print(np.shape(product_onehot))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-test Split\n",
    "\n",
    "Now onto the ever familiar train-test split! Be sure to split both the complaint data (now transformed into word vectors) as well as their associated labels. Perform an appropriate train test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test label shape: (1000, 2000)\n",
      "Train label shape: (9000, 2000)\n",
      "Test shape: (1000, 7)\n",
      "Train shape: (9000, 7)\n"
     ]
    }
   ],
   "source": [
    "#Your code here\n",
    "import random\n",
    "random.seed(123)\n",
    "test_index = random.sample(range(1,10000), 1000)\n",
    "\n",
    "X_test = one_hot_results[test_index]\n",
    "X_train = np.delete(one_hot_results, test_index, 0)\n",
    "\n",
    "y_test = product_onehot[test_index]\n",
    "y_train = np.delete(product_onehot, test_index, 0)\n",
    "\n",
    "print(\"Test label shape:\", np.shape(X_test))\n",
    "print(\"Train label shape:\", np.shape(X_train))\n",
    "print(\"Test shape:\", np.shape(y_test))\n",
    "print(\"Train shape:\", np.shape(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the model using a validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Validation Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture we mentioned that in deep learning, we generally keep aside a validation set, which is used during hyperparameter tuning. Then when we have made the final model decision, the test set is used to define the final model perforance. \n",
    "\n",
    "In this example, let's take the first 1000 cases out of the training set to become the validation set. You should do this for both `train` and `label_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just run this block of code \n",
    "random.seed(123)\n",
    "val = X_train[:1000]\n",
    "train_final = X_train[1000:]\n",
    "label_val = y_train[:1000]\n",
    "label_train_final = y_train[1000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's rebuild a fully connected (Dense) layer network with relu activations in Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we used 2 hidden with 50 units in the first layer and 25 in the second, both with a `relu` activation function. Because we are dealing with a multiclass problem (classifying the complaints into 7 classes), we use a use a softmax classifyer in order to output 7 class probabilities per case.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\david\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "#Your code here; build a neural network using Keras as described above.\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# create model\n",
    "model = models.Sequential()\n",
    "model.add(Dense(50, input_dim=2000, activation='relu'))\n",
    "model.add(Dense(25, activation='relu'))\n",
    "model.add(Dense(7, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling the Model\n",
    "In the compiler, you'll be passing the optimizer, loss function, and metrics. Train the model for 120 epochs in mini-batches of 256 samples. This time, let's include the argument `validation_data` and assign it `(val, label_val)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Code Along\n",
    "\n",
    "The remaining portion of this lab will introduce you to code snippets for a myriad of different methods discussed in the lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "Ok, now for the resource intensive part: time to train our model! Note that this is where we also introduce the validation data to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "8000/8000 [==============================] - 1s 102us/step - loss: 1.9498 - acc: 0.1644 - val_loss: 1.9383 - val_acc: 0.1710\n",
      "Epoch 2/120\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.9261 - acc: 0.2064 - val_loss: 1.9201 - val_acc: 0.2170\n",
      "Epoch 3/120\n",
      "8000/8000 [==============================] - 0s 41us/step - loss: 1.9089 - acc: 0.2364 - val_loss: 1.9025 - val_acc: 0.2440\n",
      "Epoch 4/120\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.8899 - acc: 0.2567 - val_loss: 1.8816 - val_acc: 0.2700\n",
      "Epoch 5/120\n",
      "8000/8000 [==============================] - 0s 41us/step - loss: 1.8661 - acc: 0.2833 - val_loss: 1.8553 - val_acc: 0.2900\n",
      "Epoch 6/120\n",
      "8000/8000 [==============================] - 0s 41us/step - loss: 1.8350 - acc: 0.3100 - val_loss: 1.8221 - val_acc: 0.3170\n",
      "Epoch 7/120\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.7961 - acc: 0.3389 - val_loss: 1.7798 - val_acc: 0.3340\n",
      "Epoch 8/120\n",
      "8000/8000 [==============================] - 0s 40us/step - loss: 1.7487 - acc: 0.3664 - val_loss: 1.7299 - val_acc: 0.3660\n",
      "Epoch 9/120\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.6945 - acc: 0.3961 - val_loss: 1.6749 - val_acc: 0.4040\n",
      "Epoch 10/120\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.6351 - acc: 0.4361 - val_loss: 1.6158 - val_acc: 0.4360\n",
      "Epoch 11/120\n",
      "8000/8000 [==============================] - 0s 41us/step - loss: 1.5723 - acc: 0.4704 - val_loss: 1.5537 - val_acc: 0.4650\n",
      "Epoch 12/120\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.5067 - acc: 0.5109 - val_loss: 1.4898 - val_acc: 0.5010\n",
      "Epoch 13/120\n",
      "8000/8000 [==============================] - 0s 41us/step - loss: 1.4409 - acc: 0.5427 - val_loss: 1.4246 - val_acc: 0.5400\n",
      "Epoch 14/120\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 1.3762 - acc: 0.570 - 0s 40us/step - loss: 1.3754 - acc: 0.5720 - val_loss: 1.3633 - val_acc: 0.5710\n",
      "Epoch 15/120\n",
      "8000/8000 [==============================] - 0s 41us/step - loss: 1.3123 - acc: 0.5946 - val_loss: 1.3012 - val_acc: 0.5880\n",
      "Epoch 16/120\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.2523 - acc: 0.6194 - val_loss: 1.2456 - val_acc: 0.6100\n",
      "Epoch 17/120\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.1961 - acc: 0.6340 - val_loss: 1.1924 - val_acc: 0.6260\n",
      "Epoch 18/120\n",
      "8000/8000 [==============================] - 0s 41us/step - loss: 1.1441 - acc: 0.6515 - val_loss: 1.1454 - val_acc: 0.6410\n",
      "Epoch 19/120\n",
      "8000/8000 [==============================] - 0s 41us/step - loss: 1.0966 - acc: 0.6611 - val_loss: 1.1038 - val_acc: 0.6590\n",
      "Epoch 20/120\n",
      "8000/8000 [==============================] - 0s 40us/step - loss: 1.0540 - acc: 0.6733 - val_loss: 1.0636 - val_acc: 0.6640\n",
      "Epoch 21/120\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 1.0145 - acc: 0.6811 - val_loss: 1.0274 - val_acc: 0.6680\n",
      "Epoch 22/120\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.9791 - acc: 0.6923 - val_loss: 0.9961 - val_acc: 0.6780\n",
      "Epoch 23/120\n",
      "8000/8000 [==============================] - 0s 40us/step - loss: 0.9471 - acc: 0.6960 - val_loss: 0.9678 - val_acc: 0.6780\n",
      "Epoch 24/120\n",
      "8000/8000 [==============================] - 0s 40us/step - loss: 0.9178 - acc: 0.7036 - val_loss: 0.9421 - val_acc: 0.6860\n",
      "Epoch 25/120\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8914 - acc: 0.7087 - val_loss: 0.9191 - val_acc: 0.6890\n",
      "Epoch 26/120\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8674 - acc: 0.7134 - val_loss: 0.9018 - val_acc: 0.6950\n",
      "Epoch 27/120\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 0.8452 - acc: 0.721 - 0s 41us/step - loss: 0.8456 - acc: 0.7205 - val_loss: 0.8810 - val_acc: 0.6960\n",
      "Epoch 28/120\n",
      "8000/8000 [==============================] - 0s 41us/step - loss: 0.8255 - acc: 0.7241 - val_loss: 0.8637 - val_acc: 0.7020\n",
      "Epoch 29/120\n",
      "8000/8000 [==============================] - 0s 40us/step - loss: 0.8067 - acc: 0.7283 - val_loss: 0.8482 - val_acc: 0.7090\n",
      "Epoch 30/120\n",
      "8000/8000 [==============================] - 0s 41us/step - loss: 0.7899 - acc: 0.7321 - val_loss: 0.8358 - val_acc: 0.7080\n",
      "Epoch 31/120\n",
      "8000/8000 [==============================] - 0s 41us/step - loss: 0.7741 - acc: 0.7373 - val_loss: 0.8213 - val_acc: 0.7120\n",
      "Epoch 32/120\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.7592 - acc: 0.7398 - val_loss: 0.8105 - val_acc: 0.7100\n",
      "Epoch 33/120\n",
      "8000/8000 [==============================] - 0s 40us/step - loss: 0.7457 - acc: 0.7439 - val_loss: 0.8016 - val_acc: 0.7100\n",
      "Epoch 34/120\n",
      "8000/8000 [==============================] - 0s 41us/step - loss: 0.7330 - acc: 0.7471 - val_loss: 0.7895 - val_acc: 0.7110\n",
      "Epoch 35/120\n",
      "8000/8000 [==============================] - 0s 41us/step - loss: 0.7203 - acc: 0.7521 - val_loss: 0.7830 - val_acc: 0.7140\n",
      "Epoch 36/120\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.7091 - acc: 0.7554 - val_loss: 0.7712 - val_acc: 0.7160\n",
      "Epoch 37/120\n",
      "8000/8000 [==============================] - 0s 41us/step - loss: 0.6987 - acc: 0.7599 - val_loss: 0.7648 - val_acc: 0.7160\n",
      "Epoch 38/120\n",
      "8000/8000 [==============================] - 0s 41us/step - loss: 0.6886 - acc: 0.7619 - val_loss: 0.7580 - val_acc: 0.7190\n",
      "Epoch 39/120\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 0.6789 - acc: 0.7645 - val_loss: 0.7525 - val_acc: 0.7150\n",
      "Epoch 40/120\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 0.6692 - acc: 0.7675 - val_loss: 0.7452 - val_acc: 0.7220\n",
      "Epoch 41/120\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.6606 - acc: 0.7701 - val_loss: 0.7384 - val_acc: 0.7210\n",
      "Epoch 42/120\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 0.6520 - acc: 0.7738 - val_loss: 0.7349 - val_acc: 0.7220\n",
      "Epoch 43/120\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.6444 - acc: 0.7739 - val_loss: 0.7270 - val_acc: 0.7250\n",
      "Epoch 44/120\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 0.6361 - acc: 0.7784 - val_loss: 0.7242 - val_acc: 0.7240\n",
      "Epoch 45/120\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.6292 - acc: 0.7789 - val_loss: 0.7201 - val_acc: 0.7250\n",
      "Epoch 46/120\n",
      "8000/8000 [==============================] - 0s 41us/step - loss: 0.6219 - acc: 0.7820 - val_loss: 0.7135 - val_acc: 0.7280\n",
      "Epoch 47/120\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.6143 - acc: 0.7863 - val_loss: 0.7132 - val_acc: 0.7260\n",
      "Epoch 48/120\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.6082 - acc: 0.7843 - val_loss: 0.7068 - val_acc: 0.7300\n",
      "Epoch 49/120\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.6015 - acc: 0.7894 - val_loss: 0.7045 - val_acc: 0.7380\n",
      "Epoch 50/120\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.5948 - acc: 0.7925 - val_loss: 0.7030 - val_acc: 0.7320\n",
      "Epoch 51/120\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.5891 - acc: 0.7936 - val_loss: 0.6988 - val_acc: 0.7360\n",
      "Epoch 52/120\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.5829 - acc: 0.7956 - val_loss: 0.6947 - val_acc: 0.7420\n",
      "Epoch 53/120\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.5769 - acc: 0.7984 - val_loss: 0.6932 - val_acc: 0.7390\n",
      "Epoch 54/120\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.5716 - acc: 0.8001 - val_loss: 0.6904 - val_acc: 0.7380\n",
      "Epoch 55/120\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 0.5661 - acc: 0.8044 - val_loss: 0.6866 - val_acc: 0.7380\n",
      "Epoch 56/120\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 0.5607 - acc: 0.8050 - val_loss: 0.6844 - val_acc: 0.7400\n",
      "Epoch 57/120\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.5549 - acc: 0.8088 - val_loss: 0.6876 - val_acc: 0.7380\n",
      "Epoch 58/120\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.5507 - acc: 0.8079 - val_loss: 0.6806 - val_acc: 0.7420\n",
      "Epoch 59/120\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.5452 - acc: 0.8093 - val_loss: 0.6781 - val_acc: 0.7390\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/120\n",
      "8000/8000 [==============================] - 0s 41us/step - loss: 0.5402 - acc: 0.8120 - val_loss: 0.6774 - val_acc: 0.7420\n",
      "Epoch 61/120\n",
      "8000/8000 [==============================] - 0s 41us/step - loss: 0.5357 - acc: 0.8136 - val_loss: 0.6756 - val_acc: 0.7470\n",
      "Epoch 62/120\n",
      "8000/8000 [==============================] - 0s 40us/step - loss: 0.5310 - acc: 0.8134 - val_loss: 0.6715 - val_acc: 0.7430\n",
      "Epoch 63/120\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.5258 - acc: 0.8184 - val_loss: 0.6723 - val_acc: 0.7400\n",
      "Epoch 64/120\n",
      "8000/8000 [==============================] - 0s 41us/step - loss: 0.5214 - acc: 0.8219 - val_loss: 0.6702 - val_acc: 0.7440\n",
      "Epoch 65/120\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.5168 - acc: 0.8211 - val_loss: 0.6650 - val_acc: 0.7390\n",
      "Epoch 66/120\n",
      "8000/8000 [==============================] - 0s 41us/step - loss: 0.5121 - acc: 0.8226 - val_loss: 0.6685 - val_acc: 0.7390\n",
      "Epoch 67/120\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.5082 - acc: 0.8279 - val_loss: 0.6668 - val_acc: 0.7440\n",
      "Epoch 68/120\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.5039 - acc: 0.8266 - val_loss: 0.6625 - val_acc: 0.7440\n",
      "Epoch 69/120\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.4998 - acc: 0.8301 - val_loss: 0.6610 - val_acc: 0.7430\n",
      "Epoch 70/120\n",
      "8000/8000 [==============================] - 0s 40us/step - loss: 0.4954 - acc: 0.8323 - val_loss: 0.6617 - val_acc: 0.7420\n",
      "Epoch 71/120\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.4918 - acc: 0.8329 - val_loss: 0.6598 - val_acc: 0.7440\n",
      "Epoch 72/120\n",
      "8000/8000 [==============================] - 0s 41us/step - loss: 0.4875 - acc: 0.8365 - val_loss: 0.6634 - val_acc: 0.7490\n",
      "Epoch 73/120\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 0.4843 - acc: 0.834 - 0s 41us/step - loss: 0.4837 - acc: 0.8347 - val_loss: 0.6566 - val_acc: 0.7450\n",
      "Epoch 74/120\n",
      "8000/8000 [==============================] - 0s 41us/step - loss: 0.4795 - acc: 0.8399 - val_loss: 0.6576 - val_acc: 0.7440\n",
      "Epoch 75/120\n",
      "8000/8000 [==============================] - 0s 41us/step - loss: 0.4755 - acc: 0.8386 - val_loss: 0.6579 - val_acc: 0.7440\n",
      "Epoch 76/120\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.4724 - acc: 0.8407 - val_loss: 0.6563 - val_acc: 0.7470\n",
      "Epoch 77/120\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.4679 - acc: 0.8430 - val_loss: 0.6578 - val_acc: 0.7410\n",
      "Epoch 78/120\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.4649 - acc: 0.8440 - val_loss: 0.6530 - val_acc: 0.7460\n",
      "Epoch 79/120\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.4608 - acc: 0.8469 - val_loss: 0.6596 - val_acc: 0.7440\n",
      "Epoch 80/120\n",
      "8000/8000 [==============================] - 0s 40us/step - loss: 0.4576 - acc: 0.8455 - val_loss: 0.6552 - val_acc: 0.7430\n",
      "Epoch 81/120\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.4538 - acc: 0.8501 - val_loss: 0.6550 - val_acc: 0.7450\n",
      "Epoch 82/120\n",
      "8000/8000 [==============================] - 0s 40us/step - loss: 0.4506 - acc: 0.8524 - val_loss: 0.6517 - val_acc: 0.7470\n",
      "Epoch 83/120\n",
      "8000/8000 [==============================] - 0s 40us/step - loss: 0.4466 - acc: 0.8536 - val_loss: 0.6521 - val_acc: 0.7490\n",
      "Epoch 84/120\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.4431 - acc: 0.8546 - val_loss: 0.6509 - val_acc: 0.7510\n",
      "Epoch 85/120\n",
      "8000/8000 [==============================] - 0s 40us/step - loss: 0.4399 - acc: 0.8559 - val_loss: 0.6495 - val_acc: 0.7510\n",
      "Epoch 86/120\n",
      "8000/8000 [==============================] - 0s 40us/step - loss: 0.4366 - acc: 0.8566 - val_loss: 0.6545 - val_acc: 0.7500\n",
      "Epoch 87/120\n",
      "8000/8000 [==============================] - 0s 39us/step - loss: 0.4334 - acc: 0.8581 - val_loss: 0.6541 - val_acc: 0.7460\n",
      "Epoch 88/120\n",
      "8000/8000 [==============================] - 0s 41us/step - loss: 0.4295 - acc: 0.8591 - val_loss: 0.6582 - val_acc: 0.7540\n",
      "Epoch 89/120\n",
      "8000/8000 [==============================] - 0s 40us/step - loss: 0.4272 - acc: 0.8610 - val_loss: 0.6525 - val_acc: 0.7490\n",
      "Epoch 90/120\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.4236 - acc: 0.8619 - val_loss: 0.6516 - val_acc: 0.7480\n",
      "Epoch 91/120\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.4203 - acc: 0.8639 - val_loss: 0.6501 - val_acc: 0.7520\n",
      "Epoch 92/120\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.4174 - acc: 0.8628 - val_loss: 0.6533 - val_acc: 0.7480\n",
      "Epoch 93/120\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.4138 - acc: 0.8650 - val_loss: 0.6510 - val_acc: 0.7490\n",
      "Epoch 94/120\n",
      "8000/8000 [==============================] - 0s 41us/step - loss: 0.4111 - acc: 0.8665 - val_loss: 0.6536 - val_acc: 0.7510\n",
      "Epoch 95/120\n",
      "8000/8000 [==============================] - 0s 41us/step - loss: 0.4080 - acc: 0.8669 - val_loss: 0.6489 - val_acc: 0.7490\n",
      "Epoch 96/120\n",
      "8000/8000 [==============================] - 0s 41us/step - loss: 0.4047 - acc: 0.8680 - val_loss: 0.6520 - val_acc: 0.7520\n",
      "Epoch 97/120\n",
      "8000/8000 [==============================] - 0s 40us/step - loss: 0.4022 - acc: 0.8705 - val_loss: 0.6586 - val_acc: 0.7530\n",
      "Epoch 98/120\n",
      "8000/8000 [==============================] - 0s 40us/step - loss: 0.3989 - acc: 0.8718 - val_loss: 0.6510 - val_acc: 0.7540\n",
      "Epoch 99/120\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.3960 - acc: 0.8731 - val_loss: 0.6527 - val_acc: 0.7540\n",
      "Epoch 100/120\n",
      "8000/8000 [==============================] - 0s 41us/step - loss: 0.3929 - acc: 0.8735 - val_loss: 0.6562 - val_acc: 0.7500\n",
      "Epoch 101/120\n",
      "8000/8000 [==============================] - 0s 41us/step - loss: 0.3903 - acc: 0.8730 - val_loss: 0.6531 - val_acc: 0.7510\n",
      "Epoch 102/120\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.3873 - acc: 0.8757 - val_loss: 0.6502 - val_acc: 0.7510\n",
      "Epoch 103/120\n",
      "8000/8000 [==============================] - 0s 41us/step - loss: 0.3847 - acc: 0.8762 - val_loss: 0.6506 - val_acc: 0.7520\n",
      "Epoch 104/120\n",
      "8000/8000 [==============================] - 0s 39us/step - loss: 0.3819 - acc: 0.8784 - val_loss: 0.6523 - val_acc: 0.7530\n",
      "Epoch 105/120\n",
      "8000/8000 [==============================] - 0s 40us/step - loss: 0.3794 - acc: 0.8789 - val_loss: 0.6574 - val_acc: 0.7570\n",
      "Epoch 106/120\n",
      "8000/8000 [==============================] - 0s 40us/step - loss: 0.3770 - acc: 0.8789 - val_loss: 0.6534 - val_acc: 0.7530\n",
      "Epoch 107/120\n",
      "8000/8000 [==============================] - 0s 40us/step - loss: 0.3739 - acc: 0.8824 - val_loss: 0.6535 - val_acc: 0.7550\n",
      "Epoch 108/120\n",
      "8000/8000 [==============================] - 0s 40us/step - loss: 0.3708 - acc: 0.8834 - val_loss: 0.6531 - val_acc: 0.7540\n",
      "Epoch 109/120\n",
      "8000/8000 [==============================] - 0s 40us/step - loss: 0.3683 - acc: 0.8836 - val_loss: 0.6527 - val_acc: 0.7560\n",
      "Epoch 110/120\n",
      "8000/8000 [==============================] - 0s 39us/step - loss: 0.3656 - acc: 0.8839 - val_loss: 0.6549 - val_acc: 0.7600\n",
      "Epoch 111/120\n",
      "8000/8000 [==============================] - 0s 39us/step - loss: 0.3631 - acc: 0.8851 - val_loss: 0.6607 - val_acc: 0.7570\n",
      "Epoch 112/120\n",
      "8000/8000 [==============================] - 0s 41us/step - loss: 0.3603 - acc: 0.8852 - val_loss: 0.6536 - val_acc: 0.7530\n",
      "Epoch 113/120\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.3580 - acc: 0.8872 - val_loss: 0.6578 - val_acc: 0.7540\n",
      "Epoch 114/120\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 0.3536 - acc: 0.889 - 0s 41us/step - loss: 0.3552 - acc: 0.8882 - val_loss: 0.6531 - val_acc: 0.7490\n",
      "Epoch 115/120\n",
      "8000/8000 [==============================] - 0s 41us/step - loss: 0.3530 - acc: 0.8881 - val_loss: 0.6598 - val_acc: 0.7590\n",
      "Epoch 116/120\n",
      "8000/8000 [==============================] - 0s 40us/step - loss: 0.3505 - acc: 0.8904 - val_loss: 0.6558 - val_acc: 0.7550\n",
      "Epoch 117/120\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 0.3477 - acc: 0.8900 - val_loss: 0.6577 - val_acc: 0.7520\n",
      "Epoch 118/120\n",
      "8000/8000 [==============================] - 0s 40us/step - loss: 0.3456 - acc: 0.8921 - val_loss: 0.6593 - val_acc: 0.7540\n",
      "Epoch 119/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 39us/step - loss: 0.3431 - acc: 0.8949 - val_loss: 0.6576 - val_acc: 0.7560\n",
      "Epoch 120/120\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 0.3413 - acc: 0.893 - 0s 39us/step - loss: 0.3405 - acc: 0.8940 - val_loss: 0.6628 - val_acc: 0.7540\n"
     ]
    }
   ],
   "source": [
    "#Code provided; note the extra validation parameter passed.\n",
    "model_val = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data = (val, label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving Performance Results: the `history` dictionary\n",
    "\n",
    "The dictionary `history` contains four entries this time: one per metric that was being monitored during training and during validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_val_dict = model_val.history\n",
    "model_val_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 29us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 31us/step\n"
     ]
    }
   ],
   "source": [
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3418060435056686, 0.89]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6446828603744507, 0.765]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the result isn't exactly the same as before. Note that this because the training set is slightly different! We remove 1000 instances for validation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the result similarly to what we have done in the previous lab. This time though, let's include the training and the validation loss in the same plot. We'll do the same thing for the training and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "loss_values = model_val_dict['loss']\n",
    "val_loss_values = model_val_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "plt.plot(epochs, loss_values, 'g', label='Training loss')\n",
    "plt.plot(epochs, val_loss_values, 'blue', label='Validation loss')\n",
    "\n",
    "plt.title('Training & validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "acc_values = model_val_dict['acc'] \n",
    "val_acc_values = model_val_dict['val_acc']\n",
    "\n",
    "plt.plot(epochs, acc_values, 'r', label='Training acc')\n",
    "plt.plot(epochs, val_acc_values, 'blue', label='Validation acc')\n",
    "plt.title('Training & validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe an interesting pattern here: although the training accuracy keeps increasing when going through more epochs, and the training loss keeps decreasing, the validation accuracy and loss seem to be reaching a status quo around the 60th epoch. This means that we're actually **overfitting** to the train data when we do as many epochs as we were doing. Luckily, you learned how to tackle overfitting in the previous lecture! For starters, it does seem clear that we are training too long. So let's stop training at the 60th epoch first (so-called \"early stopping\") before we move to more advanced regularization techniques!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 1000 samples\n",
      "Epoch 1/60\n",
      "8000/8000 [==============================] - 1s 148us/step - loss: 1.9562 - acc: 0.1551 - val_loss: 1.9477 - val_acc: 0.1570\n",
      "Epoch 2/60\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.9235 - acc: 0.1946 - val_loss: 1.9229 - val_acc: 0.2110\n",
      "Epoch 3/60\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 1.8955 - acc: 0.2316 - val_loss: 1.8970 - val_acc: 0.2370\n",
      "Epoch 4/60\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.8664 - acc: 0.2707 - val_loss: 1.8687 - val_acc: 0.2750\n",
      "Epoch 5/60\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 1.8349 - acc: 0.3101 - val_loss: 1.8369 - val_acc: 0.3050\n",
      "Epoch 6/60\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.8002 - acc: 0.3489 - val_loss: 1.8015 - val_acc: 0.3460\n",
      "Epoch 7/60\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.7619 - acc: 0.3847 - val_loss: 1.7621 - val_acc: 0.3790\n",
      "Epoch 8/60\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.7195 - acc: 0.4171 - val_loss: 1.7188 - val_acc: 0.4190\n",
      "Epoch 9/60\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.6732 - acc: 0.4492 - val_loss: 1.6715 - val_acc: 0.4410: 0s - loss: 1.6835 - acc: 0.4\n",
      "Epoch 10/60\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.6226 - acc: 0.4791 - val_loss: 1.6202 - val_acc: 0.4820\n",
      "Epoch 11/60\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 1.5685 - acc: 0.5112 - val_loss: 1.5651 - val_acc: 0.5110\n",
      "Epoch 12/60\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.5108 - acc: 0.5441 - val_loss: 1.5083 - val_acc: 0.5300\n",
      "Epoch 13/60\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 1.4515 - acc: 0.5726 - val_loss: 1.4499 - val_acc: 0.5500\n",
      "Epoch 14/60\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.3926 - acc: 0.5936 - val_loss: 1.3935 - val_acc: 0.5630\n",
      "Epoch 15/60\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 1.3351 - acc: 0.6094 - val_loss: 1.3393 - val_acc: 0.5780\n",
      "Epoch 16/60\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 1.2805 - acc: 0.6252 - val_loss: 1.2884 - val_acc: 0.5870\n",
      "Epoch 17/60\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 1.2286 - acc: 0.6370 - val_loss: 1.2397 - val_acc: 0.6000\n",
      "Epoch 18/60\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 1.1794 - acc: 0.6527 - val_loss: 1.1962 - val_acc: 0.6140\n",
      "Epoch 19/60\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.1347 - acc: 0.6593 - val_loss: 1.1559 - val_acc: 0.6260\n",
      "Epoch 20/60\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.0927 - acc: 0.6685 - val_loss: 1.1184 - val_acc: 0.6330\n",
      "Epoch 21/60\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.0541 - acc: 0.6789 - val_loss: 1.0872 - val_acc: 0.6400\n",
      "Epoch 22/60\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.0187 - acc: 0.6866 - val_loss: 1.0563 - val_acc: 0.6500\n",
      "Epoch 23/60\n",
      "8000/8000 [==============================] - 0s 41us/step - loss: 0.9862 - acc: 0.6919 - val_loss: 1.0252 - val_acc: 0.6540\n",
      "Epoch 24/60\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.9564 - acc: 0.6975 - val_loss: 0.9992 - val_acc: 0.6550\n",
      "Epoch 25/60\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.9289 - acc: 0.7069 - val_loss: 0.9770 - val_acc: 0.6650\n",
      "Epoch 26/60\n",
      "8000/8000 [==============================] - 0s 40us/step - loss: 0.9037 - acc: 0.7137 - val_loss: 0.9546 - val_acc: 0.6690\n",
      "Epoch 27/60\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8803 - acc: 0.7175 - val_loss: 0.9340 - val_acc: 0.6760\n",
      "Epoch 28/60\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 0.8587 - acc: 0.7229 - val_loss: 0.9174 - val_acc: 0.6790\n",
      "Epoch 29/60\n",
      "8000/8000 [==============================] - 0s 41us/step - loss: 0.8388 - acc: 0.7270 - val_loss: 0.9003 - val_acc: 0.6870\n",
      "Epoch 30/60\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 0.8203 - acc: 0.7302 - val_loss: 0.8845 - val_acc: 0.6850\n",
      "Epoch 31/60\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8027 - acc: 0.7375 - val_loss: 0.8723 - val_acc: 0.6880\n",
      "Epoch 32/60\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.7875 - acc: 0.7394 - val_loss: 0.8580 - val_acc: 0.6930\n",
      "Epoch 33/60\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.7717 - acc: 0.7431 - val_loss: 0.8459 - val_acc: 0.6970\n",
      "Epoch 34/60\n",
      "8000/8000 [==============================] - 0s 41us/step - loss: 0.7574 - acc: 0.7488 - val_loss: 0.8339 - val_acc: 0.7020\n",
      "Epoch 35/60\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.7448 - acc: 0.7518 - val_loss: 0.8267 - val_acc: 0.7060\n",
      "Epoch 36/60\n",
      "8000/8000 [==============================] - 0s 41us/step - loss: 0.7321 - acc: 0.7519 - val_loss: 0.8151 - val_acc: 0.7100\n",
      "Epoch 37/60\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 0.7204 - acc: 0.756 - 0s 41us/step - loss: 0.7206 - acc: 0.7564 - val_loss: 0.8048 - val_acc: 0.7150\n",
      "Epoch 38/60\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.7092 - acc: 0.7602 - val_loss: 0.7981 - val_acc: 0.7110\n",
      "Epoch 39/60\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.6989 - acc: 0.7635 - val_loss: 0.7926 - val_acc: 0.7090\n",
      "Epoch 40/60\n",
      "8000/8000 [==============================] - 0s 41us/step - loss: 0.6890 - acc: 0.7624 - val_loss: 0.7820 - val_acc: 0.7170\n",
      "Epoch 41/60\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.6793 - acc: 0.7680 - val_loss: 0.7763 - val_acc: 0.7110\n",
      "Epoch 42/60\n",
      "8000/8000 [==============================] - 0s 41us/step - loss: 0.6701 - acc: 0.7686 - val_loss: 0.7715 - val_acc: 0.7190\n",
      "Epoch 43/60\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.6617 - acc: 0.7731 - val_loss: 0.7646 - val_acc: 0.7170\n",
      "Epoch 44/60\n",
      "8000/8000 [==============================] - 0s 41us/step - loss: 0.6531 - acc: 0.7735 - val_loss: 0.7574 - val_acc: 0.7210\n",
      "Epoch 45/60\n",
      "8000/8000 [==============================] - 0s 41us/step - loss: 0.6456 - acc: 0.7805 - val_loss: 0.7514 - val_acc: 0.7210\n",
      "Epoch 46/60\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.6380 - acc: 0.7809 - val_loss: 0.7469 - val_acc: 0.7230\n",
      "Epoch 47/60\n",
      "8000/8000 [==============================] - 0s 41us/step - loss: 0.6302 - acc: 0.7846 - val_loss: 0.7458 - val_acc: 0.7280\n",
      "Epoch 48/60\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 0.6239 - acc: 0.7840 - val_loss: 0.7374 - val_acc: 0.7230\n",
      "Epoch 49/60\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.6167 - acc: 0.7876 - val_loss: 0.7325 - val_acc: 0.7190\n",
      "Epoch 50/60\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.6098 - acc: 0.7924 - val_loss: 0.7349 - val_acc: 0.7280\n",
      "Epoch 51/60\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.6044 - acc: 0.7905 - val_loss: 0.7269 - val_acc: 0.7330\n",
      "Epoch 52/60\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 0.5996 - acc: 0.793 - 0s 45us/step - loss: 0.5984 - acc: 0.7935 - val_loss: 0.7219 - val_acc: 0.7250\n",
      "Epoch 53/60\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.5922 - acc: 0.7946 - val_loss: 0.7240 - val_acc: 0.7180\n",
      "Epoch 54/60\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.5866 - acc: 0.7963 - val_loss: 0.7148 - val_acc: 0.7310\n",
      "Epoch 55/60\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.5812 - acc: 0.7993 - val_loss: 0.7162 - val_acc: 0.7260\n",
      "Epoch 56/60\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.5754 - acc: 0.8004 - val_loss: 0.7097 - val_acc: 0.7230\n",
      "Epoch 57/60\n",
      "8000/8000 [==============================] - 0s 41us/step - loss: 0.5703 - acc: 0.8025 - val_loss: 0.7059 - val_acc: 0.7290\n",
      "Epoch 58/60\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.5655 - acc: 0.8025 - val_loss: 0.7065 - val_acc: 0.7300\n",
      "Epoch 59/60\n",
      "8000/8000 [==============================] - 0s 41us/step - loss: 0.5600 - acc: 0.8067 - val_loss: 0.7024 - val_acc: 0.7290\n",
      "Epoch 60/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 41us/step - loss: 0.5549 - acc: 0.8074 - val_loss: 0.6979 - val_acc: 0.7280\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu', input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "final_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=60,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can use the test set to make label predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'History' object has no attribute 'evaluate'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-140f9915eee8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mresults_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfinal_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_final\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_train_final\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'History' object has no attribute 'evaluate'"
     ]
    }
   ],
   "source": [
    "results_train = final_model.evaluate(train_final, label_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_test = final_model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've significantly reduced the variance, so this is already pretty good! Our test set accuracy is slightly worse, but this model will definitely be more robust than the 120 epochs one we fitted before.\n",
    "\n",
    "Now, let's see what else we can do to improve the result!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L2 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's include L2 regularization. You can easily do this in keras adding the argument kernel_regulizers.l2 and adding a value for the regularization parameter lambda between parentheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "8000/8000 [==============================] - 1s 103us/step - loss: 2.6148 - acc: 0.1565 - val_loss: 2.5988 - val_acc: 0.1580\n",
      "Epoch 2/120\n",
      "8000/8000 [==============================] - 0s 41us/step - loss: 2.5801 - acc: 0.1796 - val_loss: 2.5714 - val_acc: 0.1950\n",
      "Epoch 3/120\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 2.5548 - acc: 0.2159 - val_loss: 2.5461 - val_acc: 0.2440\n",
      "Epoch 4/120\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 2.5292 - acc: 0.2452 - val_loss: 2.5191 - val_acc: 0.2650\n",
      "Epoch 5/120\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 2.5020 - acc: 0.2669 - val_loss: 2.4912 - val_acc: 0.2900\n",
      "Epoch 6/120\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 2.4727 - acc: 0.2906 - val_loss: 2.4608 - val_acc: 0.3020\n",
      "Epoch 7/120\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 2.4396 - acc: 0.3080 - val_loss: 2.4264 - val_acc: 0.3190\n",
      "Epoch 8/120\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 2.4021 - acc: 0.3306 - val_loss: 2.3871 - val_acc: 0.3470\n",
      "Epoch 9/120\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 2.3599 - acc: 0.3560 - val_loss: 2.3441 - val_acc: 0.3730\n",
      "Epoch 10/120\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 2.3140 - acc: 0.3836 - val_loss: 2.2973 - val_acc: 0.4070\n",
      "Epoch 11/120\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 2.2643 - acc: 0.4161 - val_loss: 2.2467 - val_acc: 0.4340\n",
      "Epoch 12/120\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 2.2115 - acc: 0.4477 - val_loss: 2.1930 - val_acc: 0.4730\n",
      "Epoch 13/120\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 2.1578 - acc: 0.4774 - val_loss: 2.1391 - val_acc: 0.5020\n",
      "Epoch 14/120\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 2.1032 - acc: 0.5079 - val_loss: 2.0854 - val_acc: 0.5330\n",
      "Epoch 15/120\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 2.0484 - acc: 0.5324 - val_loss: 2.0315 - val_acc: 0.5500\n",
      "Epoch 16/120\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.9952 - acc: 0.5520 - val_loss: 1.9804 - val_acc: 0.5720\n",
      "Epoch 17/120\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 1.9430 - acc: 0.5797 - val_loss: 1.9304 - val_acc: 0.5810\n",
      "Epoch 18/120\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.8927 - acc: 0.6026 - val_loss: 1.8824 - val_acc: 0.6040\n",
      "Epoch 19/120\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.8442 - acc: 0.6222 - val_loss: 1.8365 - val_acc: 0.6050\n",
      "Epoch 20/120\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.7987 - acc: 0.6324 - val_loss: 1.7952 - val_acc: 0.6080\n",
      "Epoch 21/120\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.7559 - acc: 0.6454 - val_loss: 1.7548 - val_acc: 0.6310\n",
      "Epoch 22/120\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.7155 - acc: 0.6610 - val_loss: 1.7166 - val_acc: 0.6340\n",
      "Epoch 23/120\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.6777 - acc: 0.6721 - val_loss: 1.6809 - val_acc: 0.6470\n",
      "Epoch 24/120\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.6432 - acc: 0.6850 - val_loss: 1.6489 - val_acc: 0.6470\n",
      "Epoch 25/120\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.6100 - acc: 0.6870 - val_loss: 1.6187 - val_acc: 0.6550\n",
      "Epoch 26/120\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.5793 - acc: 0.6994 - val_loss: 1.5918 - val_acc: 0.6610\n",
      "Epoch 27/120\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.5503 - acc: 0.7065 - val_loss: 1.5660 - val_acc: 0.6740\n",
      "Epoch 28/120\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.5234 - acc: 0.7097 - val_loss: 1.5406 - val_acc: 0.6800\n",
      "Epoch 29/120\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 1.4983 - acc: 0.7173 - val_loss: 1.5190 - val_acc: 0.6820\n",
      "Epoch 30/120\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 1.4749 - acc: 0.7215 - val_loss: 1.4976 - val_acc: 0.6900\n",
      "Epoch 31/120\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 1.4525 - acc: 0.7253 - val_loss: 1.4773 - val_acc: 0.6910\n",
      "Epoch 32/120\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.4322 - acc: 0.7298 - val_loss: 1.4602 - val_acc: 0.7040\n",
      "Epoch 33/120\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.4123 - acc: 0.7350 - val_loss: 1.4434 - val_acc: 0.7040\n",
      "Epoch 34/120\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.3939 - acc: 0.7390 - val_loss: 1.4270 - val_acc: 0.7050\n",
      "Epoch 35/120\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.3766 - acc: 0.7430 - val_loss: 1.4121 - val_acc: 0.7100\n",
      "Epoch 36/120\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 1.3601 - acc: 0.7491 - val_loss: 1.3986 - val_acc: 0.7120\n",
      "Epoch 37/120\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.3448 - acc: 0.7519 - val_loss: 1.3882 - val_acc: 0.7200\n",
      "Epoch 38/120\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 1.3298 - acc: 0.7552 - val_loss: 1.3735 - val_acc: 0.7220\n",
      "Epoch 39/120\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 1.3135 - acc: 0.757 - 0s 44us/step - loss: 1.3157 - acc: 0.7570 - val_loss: 1.3607 - val_acc: 0.7210\n",
      "Epoch 40/120\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 1.3023 - acc: 0.7626 - val_loss: 1.3516 - val_acc: 0.7250\n",
      "Epoch 41/120\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.2891 - acc: 0.7660 - val_loss: 1.3390 - val_acc: 0.7220\n",
      "Epoch 42/120\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.2772 - acc: 0.7672 - val_loss: 1.3314 - val_acc: 0.7270\n",
      "Epoch 43/120\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.2655 - acc: 0.7705 - val_loss: 1.3208 - val_acc: 0.7280\n",
      "Epoch 44/120\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 1.2545 - acc: 0.7726 - val_loss: 1.3120 - val_acc: 0.7240\n",
      "Epoch 45/120\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.2434 - acc: 0.7756 - val_loss: 1.3030 - val_acc: 0.7350\n",
      "Epoch 46/120\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.2333 - acc: 0.7781 - val_loss: 1.2968 - val_acc: 0.7320\n",
      "Epoch 47/120\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.2232 - acc: 0.7810 - val_loss: 1.2889 - val_acc: 0.7370\n",
      "Epoch 48/120\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 1.2140 - acc: 0.7841 - val_loss: 1.2803 - val_acc: 0.7370\n",
      "Epoch 49/120\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.2046 - acc: 0.7826 - val_loss: 1.2764 - val_acc: 0.7360\n",
      "Epoch 50/120\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.1954 - acc: 0.7863 - val_loss: 1.2670 - val_acc: 0.7420\n",
      "Epoch 51/120\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.1862 - acc: 0.7893 - val_loss: 1.2600 - val_acc: 0.7440\n",
      "Epoch 52/120\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.1781 - acc: 0.7909 - val_loss: 1.2538 - val_acc: 0.7440\n",
      "Epoch 53/120\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.1696 - acc: 0.7927 - val_loss: 1.2489 - val_acc: 0.7370A: 0s - loss: 1.1747 - acc: 0.7\n",
      "Epoch 54/120\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 1.1613 - acc: 0.7945 - val_loss: 1.2436 - val_acc: 0.7360\n",
      "Epoch 55/120\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.1541 - acc: 0.7976 - val_loss: 1.2352 - val_acc: 0.7480\n",
      "Epoch 56/120\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.1462 - acc: 0.7986 - val_loss: 1.2309 - val_acc: 0.7490\n",
      "Epoch 57/120\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 1.1386 - acc: 0.8013 - val_loss: 1.2274 - val_acc: 0.7440\n",
      "Epoch 58/120\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.1316 - acc: 0.8030 - val_loss: 1.2203 - val_acc: 0.7490\n",
      "Epoch 59/120\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 1.1241 - acc: 0.8049 - val_loss: 1.2163 - val_acc: 0.7480\n",
      "Epoch 60/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.1174 - acc: 0.8035 - val_loss: 1.2112 - val_acc: 0.7460\n",
      "Epoch 61/120\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 1.1105 - acc: 0.8058 - val_loss: 1.2075 - val_acc: 0.7490\n",
      "Epoch 62/120\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.1037 - acc: 0.8089 - val_loss: 1.2007 - val_acc: 0.7480\n",
      "Epoch 63/120\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.0972 - acc: 0.8104 - val_loss: 1.1990 - val_acc: 0.7470\n",
      "Epoch 64/120\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.0904 - acc: 0.8125 - val_loss: 1.1973 - val_acc: 0.7460\n",
      "Epoch 65/120\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.0843 - acc: 0.8120 - val_loss: 1.1899 - val_acc: 0.7490\n",
      "Epoch 66/120\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.0780 - acc: 0.8161 - val_loss: 1.1852 - val_acc: 0.7560\n",
      "Epoch 67/120\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 1.0717 - acc: 0.8165 - val_loss: 1.1816 - val_acc: 0.7540\n",
      "Epoch 68/120\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 1.0659 - acc: 0.8190 - val_loss: 1.1806 - val_acc: 0.7550\n",
      "Epoch 69/120\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.0598 - acc: 0.8221 - val_loss: 1.1740 - val_acc: 0.7510\n",
      "Epoch 70/120\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.0539 - acc: 0.8215 - val_loss: 1.1726 - val_acc: 0.7510\n",
      "Epoch 71/120\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.0482 - acc: 0.8239 - val_loss: 1.1663 - val_acc: 0.7530\n",
      "Epoch 72/120\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.0421 - acc: 0.8246 - val_loss: 1.1628 - val_acc: 0.7570\n",
      "Epoch 73/120\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.0368 - acc: 0.8269 - val_loss: 1.1590 - val_acc: 0.7570\n",
      "Epoch 74/120\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.0315 - acc: 0.8293 - val_loss: 1.1572 - val_acc: 0.7550\n",
      "Epoch 75/120\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 1.0256 - acc: 0.8284 - val_loss: 1.1532 - val_acc: 0.7540\n",
      "Epoch 76/120\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.0204 - acc: 0.8315 - val_loss: 1.1567 - val_acc: 0.7500\n",
      "Epoch 77/120\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.0154 - acc: 0.8314 - val_loss: 1.1508 - val_acc: 0.7530\n",
      "Epoch 78/120\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 1.0100 - acc: 0.8325 - val_loss: 1.1471 - val_acc: 0.7580\n",
      "Epoch 79/120\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.0046 - acc: 0.8356 - val_loss: 1.1427 - val_acc: 0.7580\n",
      "Epoch 80/120\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.9996 - acc: 0.8370 - val_loss: 1.1405 - val_acc: 0.7550\n",
      "Epoch 81/120\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.9943 - acc: 0.8372 - val_loss: 1.1413 - val_acc: 0.7600\n",
      "Epoch 82/120\n",
      "8000/8000 [==============================] - 0s 41us/step - loss: 0.9897 - acc: 0.8396 - val_loss: 1.1354 - val_acc: 0.7560\n",
      "Epoch 83/120\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.9844 - acc: 0.8402 - val_loss: 1.1314 - val_acc: 0.7570\n",
      "Epoch 84/120\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.9796 - acc: 0.8417 - val_loss: 1.1270 - val_acc: 0.7570\n",
      "Epoch 85/120\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.9749 - acc: 0.8421 - val_loss: 1.1277 - val_acc: 0.7550\n",
      "Epoch 86/120\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.9701 - acc: 0.8438 - val_loss: 1.1225 - val_acc: 0.7610\n",
      "Epoch 87/120\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.9657 - acc: 0.8436 - val_loss: 1.1193 - val_acc: 0.7640\n",
      "Epoch 88/120\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.9605 - acc: 0.8478 - val_loss: 1.1210 - val_acc: 0.7580\n",
      "Epoch 89/120\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.9564 - acc: 0.8470 - val_loss: 1.1151 - val_acc: 0.7600\n",
      "Epoch 90/120\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.9514 - acc: 0.8476 - val_loss: 1.1141 - val_acc: 0.7590\n",
      "Epoch 91/120\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.9473 - acc: 0.8508 - val_loss: 1.1108 - val_acc: 0.7610\n",
      "Epoch 92/120\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.9428 - acc: 0.8500 - val_loss: 1.1082 - val_acc: 0.7640\n",
      "Epoch 93/120\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.9381 - acc: 0.8511 - val_loss: 1.1047 - val_acc: 0.7620\n",
      "Epoch 94/120\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.9331 - acc: 0.8549 - val_loss: 1.1068 - val_acc: 0.7650\n",
      "Epoch 95/120\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.9294 - acc: 0.8556 - val_loss: 1.1021 - val_acc: 0.7620\n",
      "Epoch 96/120\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.9251 - acc: 0.8556 - val_loss: 1.1005 - val_acc: 0.7660\n",
      "Epoch 97/120\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.9206 - acc: 0.8569 - val_loss: 1.0973 - val_acc: 0.7620\n",
      "Epoch 98/120\n",
      "8000/8000 [==============================] - 0s 41us/step - loss: 0.9166 - acc: 0.8581 - val_loss: 1.0936 - val_acc: 0.7620\n",
      "Epoch 99/120\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.9124 - acc: 0.8589 - val_loss: 1.0910 - val_acc: 0.7610\n",
      "Epoch 100/120\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.9078 - acc: 0.8604 - val_loss: 1.0907 - val_acc: 0.7640\n",
      "Epoch 101/120\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.9039 - acc: 0.8618 - val_loss: 1.0866 - val_acc: 0.7610\n",
      "Epoch 102/120\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8998 - acc: 0.8615 - val_loss: 1.0914 - val_acc: 0.7600\n",
      "Epoch 103/120\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8963 - acc: 0.8630 - val_loss: 1.0847 - val_acc: 0.7590\n",
      "Epoch 104/120\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8918 - acc: 0.8650 - val_loss: 1.0843 - val_acc: 0.7550\n",
      "Epoch 105/120\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8883 - acc: 0.8650 - val_loss: 1.0810 - val_acc: 0.7670\n",
      "Epoch 106/120\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8838 - acc: 0.8669 - val_loss: 1.0760 - val_acc: 0.7600\n",
      "Epoch 107/120\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8800 - acc: 0.8681 - val_loss: 1.0773 - val_acc: 0.7660\n",
      "Epoch 108/120\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8762 - acc: 0.8685 - val_loss: 1.0805 - val_acc: 0.7680\n",
      "Epoch 109/120\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8726 - acc: 0.8699 - val_loss: 1.0728 - val_acc: 0.7610\n",
      "Epoch 110/120\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8690 - acc: 0.8718 - val_loss: 1.0695 - val_acc: 0.7620\n",
      "Epoch 111/120\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8650 - acc: 0.8705 - val_loss: 1.0689 - val_acc: 0.7650\n",
      "Epoch 112/120\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8614 - acc: 0.8735 - val_loss: 1.0673 - val_acc: 0.7590\n",
      "Epoch 113/120\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8575 - acc: 0.8743 - val_loss: 1.0705 - val_acc: 0.7570\n",
      "Epoch 114/120\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8538 - acc: 0.8745 - val_loss: 1.0611 - val_acc: 0.7630\n",
      "Epoch 115/120\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8505 - acc: 0.8757 - val_loss: 1.0633 - val_acc: 0.7630\n",
      "Epoch 116/120\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 0.8465 - acc: 0.8755 - val_loss: 1.0609 - val_acc: 0.7640\n",
      "Epoch 117/120\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.8431 - acc: 0.8782 - val_loss: 1.0628 - val_acc: 0.7680\n",
      "Epoch 118/120\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8396 - acc: 0.8794 - val_loss: 1.0575 - val_acc: 0.7580\n",
      "Epoch 119/120\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8365 - acc: 0.8784 - val_loss: 1.0586 - val_acc: 0.7520\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/120\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8326 - acc: 0.8800 - val_loss: 1.0557 - val_acc: 0.7680\n"
     ]
    }
   ],
   "source": [
    "from keras import regularizers\n",
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu',kernel_regularizer=regularizers.l2(0.005), input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, kernel_regularizer=regularizers.l2(0.005), activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L2_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L2_model_dict = L2_model.history\n",
    "L2_model_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the training accuracy as well as the validation accuracy for both the L2 and the model without regularization (for 120 epochs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "acc_values = L2_model_dict['acc'] \n",
    "val_acc_values = L2_model_dict['val_acc']\n",
    "model_acc = model_val_dict['acc']\n",
    "model_val_acc = model_val_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L2')\n",
    "plt.plot(epochs, val_acc_values, 'g', label='Validation acc L2')\n",
    "plt.plot(epochs, model_acc, 'r', label='Training acc')\n",
    "plt.plot(epochs, model_val_acc, 'r', label='Validation acc')\n",
    "plt.title('Training & validation accuracy L2 vs regular')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of L2 regularization are quite disappointing here. We notice the discrepancy between validation and training accuracy seems to have decreased slightly, but the end result is definitely not getting better. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at L1 regularization. Will this work better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "8000/8000 [==============================] - 1s 125us/step - loss: 16.0173 - acc: 0.1740 - val_loss: 15.5772 - val_acc: 0.1760\n",
      "Epoch 2/120\n",
      "8000/8000 [==============================] - 0s 40us/step - loss: 15.2047 - acc: 0.1949 - val_loss: 14.7836 - val_acc: 0.2100\n",
      "Epoch 3/120\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 14.4226 - acc: 0.2190 - val_loss: 14.0150 - val_acc: 0.2450\n",
      "Epoch 4/120\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 13.6644 - acc: 0.2596 - val_loss: 13.2690 - val_acc: 0.2780\n",
      "Epoch 5/120\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 12.9289 - acc: 0.2923 - val_loss: 12.5452 - val_acc: 0.3210\n",
      "Epoch 6/120\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 12.2154 - acc: 0.3375 - val_loss: 11.8433 - val_acc: 0.3530\n",
      "Epoch 7/120\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 11.5250 - acc: 0.3826 - val_loss: 11.1656 - val_acc: 0.3980\n",
      "Epoch 8/120\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 10.8572 - acc: 0.4180 - val_loss: 10.5104 - val_acc: 0.4230\n",
      "Epoch 9/120\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 10.2116 - acc: 0.4585 - val_loss: 9.8779 - val_acc: 0.4610\n",
      "Epoch 10/120\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 9.5892 - acc: 0.4989 - val_loss: 9.2691 - val_acc: 0.4940\n",
      "Epoch 11/120\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 8.9888 - acc: 0.5421 - val_loss: 8.6809 - val_acc: 0.5310\n",
      "Epoch 12/120\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 8.4116 - acc: 0.5705 - val_loss: 8.1176 - val_acc: 0.5520\n",
      "Epoch 13/120\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 7.8590 - acc: 0.5904 - val_loss: 7.5785 - val_acc: 0.5850\n",
      "Epoch 14/120\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 7.3309 - acc: 0.6153 - val_loss: 7.0645 - val_acc: 0.6050\n",
      "Epoch 15/120\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 6.8280 - acc: 0.6270 - val_loss: 6.5754 - val_acc: 0.6230\n",
      "Epoch 16/120\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 6.3511 - acc: 0.6384 - val_loss: 6.1111 - val_acc: 0.6250\n",
      "Epoch 17/120\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 5.8997 - acc: 0.6471 - val_loss: 5.6755 - val_acc: 0.6290\n",
      "Epoch 18/120\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 5.4751 - acc: 0.6520 - val_loss: 5.2645 - val_acc: 0.6310\n",
      "Epoch 19/120\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 5.0760 - acc: 0.6581 - val_loss: 4.8785 - val_acc: 0.6450\n",
      "Epoch 20/120\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 4.7033 - acc: 0.6646 - val_loss: 4.5205 - val_acc: 0.6390\n",
      "Epoch 21/120\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 4.3563 - acc: 0.6718 - val_loss: 4.1884 - val_acc: 0.6500\n",
      "Epoch 22/120\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 4.0347 - acc: 0.6746 - val_loss: 3.8810 - val_acc: 0.6570\n",
      "Epoch 23/120\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 3.7393 - acc: 0.6766 - val_loss: 3.5984 - val_acc: 0.6550\n",
      "Epoch 24/120\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 3.4691 - acc: 0.6815 - val_loss: 3.3426 - val_acc: 0.6550\n",
      "Epoch 25/120\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 3.2248 - acc: 0.6833 - val_loss: 3.1112 - val_acc: 0.6700\n",
      "Epoch 26/120\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 3.0052 - acc: 0.6861 - val_loss: 2.9082 - val_acc: 0.6620\n",
      "Epoch 27/120\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 2.8116 - acc: 0.6884 - val_loss: 2.7262 - val_acc: 0.6670\n",
      "Epoch 28/120\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 2.6412 - acc: 0.6877 - val_loss: 2.5690 - val_acc: 0.6700\n",
      "Epoch 29/120\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 2.4944 - acc: 0.6891 - val_loss: 2.4333 - val_acc: 0.6710\n",
      "Epoch 30/120\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 2.3710 - acc: 0.6911 - val_loss: 2.3219 - val_acc: 0.6730\n",
      "Epoch 31/120\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 2.2701 - acc: 0.6891 - val_loss: 2.2337 - val_acc: 0.6680\n",
      "Epoch 32/120\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 2.1896 - acc: 0.6886 - val_loss: 2.1615 - val_acc: 0.6750\n",
      "Epoch 33/120\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 2.1284 - acc: 0.6906 - val_loss: 2.1086 - val_acc: 0.6710\n",
      "Epoch 34/120\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 2.0833 - acc: 0.6895 - val_loss: 2.0727 - val_acc: 0.6670\n",
      "Epoch 35/120\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 2.0501 - acc: 0.6896 - val_loss: 2.0423 - val_acc: 0.6670\n",
      "Epoch 36/120\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 2.0231 - acc: 0.6885 - val_loss: 2.0180 - val_acc: 0.6700\n",
      "Epoch 37/120\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.9992 - acc: 0.6911 - val_loss: 1.9941 - val_acc: 0.6660\n",
      "Epoch 38/120\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.9776 - acc: 0.6916 - val_loss: 1.9749 - val_acc: 0.6740\n",
      "Epoch 39/120\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.9579 - acc: 0.6919 - val_loss: 1.9543 - val_acc: 0.6720\n",
      "Epoch 40/120\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.9393 - acc: 0.6916 - val_loss: 1.9360 - val_acc: 0.6710\n",
      "Epoch 41/120\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.9218 - acc: 0.6934 - val_loss: 1.9190 - val_acc: 0.6740\n",
      "Epoch 42/120\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.9050 - acc: 0.6944 - val_loss: 1.9027 - val_acc: 0.6780\n",
      "Epoch 43/120\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.8897 - acc: 0.6937 - val_loss: 1.8891 - val_acc: 0.6770\n",
      "Epoch 44/120\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.8744 - acc: 0.6954 - val_loss: 1.8755 - val_acc: 0.6760\n",
      "Epoch 45/120\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 1.8596 - acc: 0.6921 - val_loss: 1.8613 - val_acc: 0.6740\n",
      "Epoch 46/120\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 1.8455 - acc: 0.6954 - val_loss: 1.8483 - val_acc: 0.6740\n",
      "Epoch 47/120\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.8319 - acc: 0.6969 - val_loss: 1.8315 - val_acc: 0.6790\n",
      "Epoch 48/120\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.8188 - acc: 0.6970 - val_loss: 1.8187 - val_acc: 0.6810\n",
      "Epoch 49/120\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.8059 - acc: 0.6974 - val_loss: 1.8085 - val_acc: 0.6820\n",
      "Epoch 50/120\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.7934 - acc: 0.6986 - val_loss: 1.7991 - val_acc: 0.6790\n",
      "Epoch 51/120\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 1.7815 - acc: 0.6996 - val_loss: 1.7835 - val_acc: 0.6780\n",
      "Epoch 52/120\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.7699 - acc: 0.7005 - val_loss: 1.7733 - val_acc: 0.6810\n",
      "Epoch 53/120\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.7585 - acc: 0.7017 - val_loss: 1.7603 - val_acc: 0.6800\n",
      "Epoch 54/120\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 1.7472 - acc: 0.7011 - val_loss: 1.7482 - val_acc: 0.6790\n",
      "Epoch 55/120\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.7361 - acc: 0.7017 - val_loss: 1.7392 - val_acc: 0.6800\n",
      "Epoch 56/120\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.7257 - acc: 0.7015 - val_loss: 1.7324 - val_acc: 0.6880\n",
      "Epoch 57/120\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 1.7150 - acc: 0.7031 - val_loss: 1.7233 - val_acc: 0.6830\n",
      "Epoch 58/120\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.7052 - acc: 0.7031 - val_loss: 1.7101 - val_acc: 0.6880\n",
      "Epoch 59/120\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.6947 - acc: 0.7032 - val_loss: 1.7024 - val_acc: 0.6860\n",
      "Epoch 60/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 50us/step - loss: 1.6849 - acc: 0.7047 - val_loss: 1.6921 - val_acc: 0.6900\n",
      "Epoch 61/120\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.6751 - acc: 0.7039 - val_loss: 1.6771 - val_acc: 0.6890\n",
      "Epoch 62/120\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 1.6652 - acc: 0.7053 - val_loss: 1.6730 - val_acc: 0.6930\n",
      "Epoch 63/120\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 1.6558 - acc: 0.7056 - val_loss: 1.6598 - val_acc: 0.6930\n",
      "Epoch 64/120\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 1.6463 - acc: 0.7065 - val_loss: 1.6517 - val_acc: 0.6940\n",
      "Epoch 65/120\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.6377 - acc: 0.7083 - val_loss: 1.6399 - val_acc: 0.6920\n",
      "Epoch 66/120\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.6284 - acc: 0.7067 - val_loss: 1.6328 - val_acc: 0.6990\n",
      "Epoch 67/120\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 1.6196 - acc: 0.7091 - val_loss: 1.6242 - val_acc: 0.6940\n",
      "Epoch 68/120\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.6111 - acc: 0.7095 - val_loss: 1.6150 - val_acc: 0.6900\n",
      "Epoch 69/120\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.6028 - acc: 0.7083 - val_loss: 1.6078 - val_acc: 0.6950\n",
      "Epoch 70/120\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.5940 - acc: 0.7092 - val_loss: 1.6050 - val_acc: 0.6950\n",
      "Epoch 71/120\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.5860 - acc: 0.7083 - val_loss: 1.5893 - val_acc: 0.6980\n",
      "Epoch 72/120\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.5775 - acc: 0.7099 - val_loss: 1.5860 - val_acc: 0.6980\n",
      "Epoch 73/120\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.5699 - acc: 0.7109 - val_loss: 1.5764 - val_acc: 0.7010\n",
      "Epoch 74/120\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.5618 - acc: 0.7092 - val_loss: 1.5693 - val_acc: 0.7020\n",
      "Epoch 75/120\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.5539 - acc: 0.7113 - val_loss: 1.5649 - val_acc: 0.6980\n",
      "Epoch 76/120\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.5461 - acc: 0.7114 - val_loss: 1.5513 - val_acc: 0.6990\n",
      "Epoch 77/120\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 1.5385 - acc: 0.7111- ETA: 0s - loss: 1.5354 - acc: 0.705 - ETA: 0s - loss: 1.5558 - acc: 0. - 0s 51us/step - loss: 1.5384 - acc: 0.7116 - val_loss: 1.5481 - val_acc: 0.6980\n",
      "Epoch 78/120\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 1.5309 - acc: 0.712 - 0s 44us/step - loss: 1.5311 - acc: 0.7122 - val_loss: 1.5354 - val_acc: 0.7000\n",
      "Epoch 79/120\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.5236 - acc: 0.7137 - val_loss: 1.5360 - val_acc: 0.7030\n",
      "Epoch 80/120\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.5164 - acc: 0.7129 - val_loss: 1.5237 - val_acc: 0.6990\n",
      "Epoch 81/120\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 1.5089 - acc: 0.7130 - val_loss: 1.5119 - val_acc: 0.7030\n",
      "Epoch 82/120\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 1.5015 - acc: 0.7155 - val_loss: 1.5047 - val_acc: 0.7060\n",
      "Epoch 83/120\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 1.4946 - acc: 0.7146 - val_loss: 1.4999 - val_acc: 0.7080\n",
      "Epoch 84/120\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 1.4871 - acc: 0.7150 - val_loss: 1.4955 - val_acc: 0.7060\n",
      "Epoch 85/120\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 1.4803 - acc: 0.7159 - val_loss: 1.4831 - val_acc: 0.7020\n",
      "Epoch 86/120\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 1.4731 - acc: 0.7143 - val_loss: 1.4771 - val_acc: 0.7020\n",
      "Epoch 87/120\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.4665 - acc: 0.7155 - val_loss: 1.4724 - val_acc: 0.7040\n",
      "Epoch 88/120\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 1.4599 - acc: 0.7159 - val_loss: 1.4652 - val_acc: 0.7070\n",
      "Epoch 89/120\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 1.4534 - acc: 0.7171 - val_loss: 1.4639 - val_acc: 0.7050\n",
      "Epoch 90/120\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 1.4462 - acc: 0.7164 - val_loss: 1.4546 - val_acc: 0.7020\n",
      "Epoch 91/120\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 1.4401 - acc: 0.7179 - val_loss: 1.4461 - val_acc: 0.7080\n",
      "Epoch 92/120\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 1.4334 - acc: 0.7180 - val_loss: 1.4382 - val_acc: 0.7020\n",
      "Epoch 93/120\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 1.4269 - acc: 0.7184 - val_loss: 1.4354 - val_acc: 0.7020\n",
      "Epoch 94/120\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 1.4233 - acc: 0.7166- ETA: 0s - loss: 1.4314 - acc: 0. - 0s 45us/step - loss: 1.4212 - acc: 0.7181 - val_loss: 1.4339 - val_acc: 0.7060\n",
      "Epoch 95/120\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 1.4152 - acc: 0.7180 - val_loss: 1.4246 - val_acc: 0.7030\n",
      "Epoch 96/120\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.4086 - acc: 0.7184 - val_loss: 1.4123 - val_acc: 0.7070\n",
      "Epoch 97/120\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.4019 - acc: 0.7198 - val_loss: 1.4108 - val_acc: 0.7130\n",
      "Epoch 98/120\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 1.3967 - acc: 0.7167 - val_loss: 1.4037 - val_acc: 0.7110\n",
      "Epoch 99/120\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.3904 - acc: 0.7191 - val_loss: 1.3962 - val_acc: 0.7090\n",
      "Epoch 100/120\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 1.3845 - acc: 0.7200 - val_loss: 1.3937 - val_acc: 0.7100\n",
      "Epoch 101/120\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.3790 - acc: 0.7206 - val_loss: 1.3828 - val_acc: 0.7100\n",
      "Epoch 102/120\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.3721 - acc: 0.7214 - val_loss: 1.3806 - val_acc: 0.7100\n",
      "Epoch 103/120\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 1.3668 - acc: 0.7211 - val_loss: 1.3737 - val_acc: 0.7100\n",
      "Epoch 104/120\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 1.3611 - acc: 0.7209 - val_loss: 1.3661 - val_acc: 0.7090\n",
      "Epoch 105/120\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.3558 - acc: 0.7232 - val_loss: 1.3636 - val_acc: 0.7120\n",
      "Epoch 106/120\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.3502 - acc: 0.7224 - val_loss: 1.3561 - val_acc: 0.7120\n",
      "Epoch 107/120\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 1.3443 - acc: 0.7228 - val_loss: 1.3541 - val_acc: 0.7070\n",
      "Epoch 108/120\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.3401 - acc: 0.7219 - val_loss: 1.3454 - val_acc: 0.7140\n",
      "Epoch 109/120\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 1.3338 - acc: 0.7236 - val_loss: 1.3432 - val_acc: 0.7100\n",
      "Epoch 110/120\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 1.3291 - acc: 0.7228 - val_loss: 1.3340 - val_acc: 0.7130\n",
      "Epoch 111/120\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.3240 - acc: 0.7245 - val_loss: 1.3303 - val_acc: 0.7130\n",
      "Epoch 112/120\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.3193 - acc: 0.7234 - val_loss: 1.3252 - val_acc: 0.7090\n",
      "Epoch 113/120\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 1.3139 - acc: 0.7248 - val_loss: 1.3223 - val_acc: 0.7160\n",
      "Epoch 114/120\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.3087 - acc: 0.7255 - val_loss: 1.3177 - val_acc: 0.7140\n",
      "Epoch 115/120\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 1.3038 - acc: 0.7265 - val_loss: 1.3122 - val_acc: 0.7140\n",
      "Epoch 116/120\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 1.2990 - acc: 0.726 - 0s 45us/step - loss: 1.2990 - acc: 0.7268 - val_loss: 1.3145 - val_acc: 0.7190\n",
      "Epoch 117/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.2952 - acc: 0.7261 - val_loss: 1.3004 - val_acc: 0.7130\n",
      "Epoch 118/120\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 1.2892 - acc: 0.7268 - val_loss: 1.2990 - val_acc: 0.7140\n",
      "Epoch 119/120\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 1.2868 - acc: 0.726 - 0s 42us/step - loss: 1.2852 - acc: 0.7255 - val_loss: 1.2924 - val_acc: 0.7160\n",
      "Epoch 120/120\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.2802 - acc: 0.7272 - val_loss: 1.2868 - val_acc: 0.7120\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu',kernel_regularizer=regularizers.l1(0.005), input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, kernel_regularizer=regularizers.l1(0.005), activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L1_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "L1_model_dict = L1_model.history\n",
    "plt.clf()\n",
    "\n",
    "acc_values = L1_model_dict['acc'] \n",
    "val_acc_values = L1_model_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L1')\n",
    "plt.plot(epochs, val_acc_values, 'g.', label='Validation acc L1')\n",
    "plt.title('Training & validation accuracy with L1 regularization')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how The training and validation accuracy don't diverge as much as before! Unfortunately, the validation accuracy doesn't reach rates much higher than 70%. It does seem like we can still improve the model by training much longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 1000 samples\n",
      "Epoch 1/1000\n",
      "8000/8000 [==============================] - 1s 112us/step - loss: 15.9876 - acc: 0.1570 - val_loss: 15.5615 - val_acc: 0.1370\n",
      "Epoch 2/1000\n",
      "8000/8000 [==============================] - 0s 40us/step - loss: 15.1791 - acc: 0.1749 - val_loss: 14.7689 - val_acc: 0.1560\n",
      "Epoch 3/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 14.3996 - acc: 0.2057 - val_loss: 14.0030 - val_acc: 0.1870\n",
      "Epoch 4/1000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 13.6454 - acc: 0.2309 - val_loss: 13.2613 - val_acc: 0.2270\n",
      "Epoch 5/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 12.9149 - acc: 0.2482 - val_loss: 12.5440 - val_acc: 0.2520\n",
      "Epoch 6/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 12.2071 - acc: 0.2706 - val_loss: 11.8491 - val_acc: 0.2510\n",
      "Epoch 7/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 11.5225 - acc: 0.2785 - val_loss: 11.1779 - val_acc: 0.2690\n",
      "Epoch 8/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 10.8614 - acc: 0.2920 - val_loss: 10.5301 - val_acc: 0.2930\n",
      "Epoch 9/1000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 10.2239 - acc: 0.3181 - val_loss: 9.9048 - val_acc: 0.3130\n",
      "Epoch 10/1000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 9.6093 - acc: 0.3410 - val_loss: 9.3023 - val_acc: 0.3410\n",
      "Epoch 11/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 9.0179 - acc: 0.3764 - val_loss: 8.7229 - val_acc: 0.3810\n",
      "Epoch 12/1000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 8.4498 - acc: 0.4111 - val_loss: 8.1684 - val_acc: 0.4100\n",
      "Epoch 13/1000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 7.9067 - acc: 0.4450 - val_loss: 7.6386 - val_acc: 0.4350\n",
      "Epoch 14/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 7.3875 - acc: 0.4784 - val_loss: 7.1330 - val_acc: 0.4870\n",
      "Epoch 15/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 6.8919 - acc: 0.5239 - val_loss: 6.6487 - val_acc: 0.5020\n",
      "Epoch 16/1000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 6.4203 - acc: 0.5479 - val_loss: 6.1898 - val_acc: 0.5290\n",
      "Epoch 17/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 5.9732 - acc: 0.5782 - val_loss: 5.7562 - val_acc: 0.5520\n",
      "Epoch 18/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 5.5511 - acc: 0.6036 - val_loss: 5.3474 - val_acc: 0.5620\n",
      "Epoch 19/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 5.1534 - acc: 0.6181 - val_loss: 4.9617 - val_acc: 0.5920\n",
      "Epoch 20/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 4.7806 - acc: 0.6341 - val_loss: 4.6022 - val_acc: 0.6120\n",
      "Epoch 21/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 4.4331 - acc: 0.6481 - val_loss: 4.2694 - val_acc: 0.6030\n",
      "Epoch 22/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 4.1113 - acc: 0.6501 - val_loss: 3.9595 - val_acc: 0.6390\n",
      "Epoch 23/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 3.8149 - acc: 0.6569 - val_loss: 3.6752 - val_acc: 0.6420\n",
      "Epoch 24/1000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 3.5436 - acc: 0.6624 - val_loss: 3.4177 - val_acc: 0.6470\n",
      "Epoch 25/1000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 3.2975 - acc: 0.6663 - val_loss: 3.1845 - val_acc: 0.6470\n",
      "Epoch 26/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 3.0761 - acc: 0.6653 - val_loss: 2.9745 - val_acc: 0.6500\n",
      "Epoch 27/1000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 2.8786 - acc: 0.6710 - val_loss: 2.7899 - val_acc: 0.6600\n",
      "Epoch 28/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 2.7054 - acc: 0.6700 - val_loss: 2.6298 - val_acc: 0.6600\n",
      "Epoch 29/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 2.5555 - acc: 0.6707 - val_loss: 2.4917 - val_acc: 0.6550\n",
      "Epoch 30/1000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 2.4283 - acc: 0.6703 - val_loss: 2.3753 - val_acc: 0.6590\n",
      "Epoch 31/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 2.3237 - acc: 0.6715 - val_loss: 2.2827 - val_acc: 0.6640\n",
      "Epoch 32/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 2.2409 - acc: 0.6724 - val_loss: 2.2099 - val_acc: 0.6510\n",
      "Epoch 33/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 2.1776 - acc: 0.6696 - val_loss: 2.1561 - val_acc: 0.6500\n",
      "Epoch 34/1000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 2.1310 - acc: 0.6706 - val_loss: 2.1200 - val_acc: 0.6620\n",
      "Epoch 35/1000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 2.0969 - acc: 0.6706 - val_loss: 2.0880 - val_acc: 0.6490\n",
      "Epoch 36/1000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 2.0676 - acc: 0.6713 - val_loss: 2.0588 - val_acc: 0.6510\n",
      "Epoch 37/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 2.0421 - acc: 0.6710 - val_loss: 2.0375 - val_acc: 0.6560\n",
      "Epoch 38/1000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 2.0196 - acc: 0.6699 - val_loss: 2.0157 - val_acc: 0.6530\n",
      "Epoch 39/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.9981 - acc: 0.6725 - val_loss: 1.9949 - val_acc: 0.6590\n",
      "Epoch 40/1000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 1.9778 - acc: 0.6740 - val_loss: 1.9735 - val_acc: 0.6520\n",
      "Epoch 41/1000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 1.9591 - acc: 0.6718 - val_loss: 1.9584 - val_acc: 0.6510\n",
      "Epoch 42/1000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 1.9416 - acc: 0.6737 - val_loss: 1.9380 - val_acc: 0.6580\n",
      "Epoch 43/1000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 1.9244 - acc: 0.6749 - val_loss: 1.9241 - val_acc: 0.6600\n",
      "Epoch 44/1000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 1.9082 - acc: 0.6734 - val_loss: 1.9103 - val_acc: 0.6580\n",
      "Epoch 45/1000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 1.8933 - acc: 0.6746 - val_loss: 1.8932 - val_acc: 0.6600\n",
      "Epoch 46/1000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 1.8782 - acc: 0.6773 - val_loss: 1.8801 - val_acc: 0.6540A: 0s - loss: 1.9061 - acc: 0.66 - ETA: 0s - loss: 1.8779 - acc: 0.68\n",
      "Epoch 47/1000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 1.8640 - acc: 0.6783 - val_loss: 1.8673 - val_acc: 0.6560\n",
      "Epoch 48/1000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 1.8504 - acc: 0.6770 - val_loss: 1.8524 - val_acc: 0.6570\n",
      "Epoch 49/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.8372 - acc: 0.6779 - val_loss: 1.8392 - val_acc: 0.6630\n",
      "Epoch 50/1000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 1.8237 - acc: 0.6789 - val_loss: 1.8255 - val_acc: 0.6590\n",
      "Epoch 51/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.8112 - acc: 0.6793 - val_loss: 1.8133 - val_acc: 0.6670\n",
      "Epoch 52/1000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 1.7990 - acc: 0.6808 - val_loss: 1.8032 - val_acc: 0.6600\n",
      "Epoch 53/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.7868 - acc: 0.6817 - val_loss: 1.7930 - val_acc: 0.6650\n",
      "Epoch 54/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.7756 - acc: 0.6817 - val_loss: 1.7802 - val_acc: 0.6700\n",
      "Epoch 55/1000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 1.7641 - acc: 0.6834 - val_loss: 1.7739 - val_acc: 0.6640\n",
      "Epoch 56/1000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 1.7532 - acc: 0.6833 - val_loss: 1.7561 - val_acc: 0.6690\n",
      "Epoch 57/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.7420 - acc: 0.6860 - val_loss: 1.7460 - val_acc: 0.6700\n",
      "Epoch 58/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.7314 - acc: 0.6866 - val_loss: 1.7358 - val_acc: 0.6660\n",
      "Epoch 59/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.7208 - acc: 0.6859 - val_loss: 1.7267 - val_acc: 0.6690\n",
      "Epoch 60/1000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 1.7106 - acc: 0.6863 - val_loss: 1.7184 - val_acc: 0.6700\n",
      "Epoch 61/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.7006 - acc: 0.6868 - val_loss: 1.7065 - val_acc: 0.6710\n",
      "Epoch 62/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.6903 - acc: 0.6894 - val_loss: 1.6942 - val_acc: 0.6650\n",
      "Epoch 63/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.6806 - acc: 0.6882 - val_loss: 1.6896 - val_acc: 0.6710\n",
      "Epoch 64/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.6712 - acc: 0.6903 - val_loss: 1.6800 - val_acc: 0.6680\n",
      "Epoch 65/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.6623 - acc: 0.6907 - val_loss: 1.6705 - val_acc: 0.6780\n",
      "Epoch 66/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.6537 - acc: 0.6894 - val_loss: 1.6574 - val_acc: 0.6750\n",
      "Epoch 67/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.6435 - acc: 0.6920 - val_loss: 1.6488 - val_acc: 0.6800\n",
      "Epoch 68/1000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 1.6348 - acc: 0.6927 - val_loss: 1.6415 - val_acc: 0.6750\n",
      "Epoch 69/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.6261 - acc: 0.6931 - val_loss: 1.6345 - val_acc: 0.6760\n",
      "Epoch 70/1000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 1.6175 - acc: 0.6926 - val_loss: 1.6255 - val_acc: 0.6830\n",
      "Epoch 71/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.6091 - acc: 0.6915 - val_loss: 1.6193 - val_acc: 0.6790\n",
      "Epoch 72/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.6004 - acc: 0.6956 - val_loss: 1.6111 - val_acc: 0.6810\n",
      "Epoch 73/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.5922 - acc: 0.6931 - val_loss: 1.6057 - val_acc: 0.6860\n",
      "Epoch 74/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.5844 - acc: 0.6983 - val_loss: 1.5912 - val_acc: 0.6800\n",
      "Epoch 75/1000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 1.5756 - acc: 0.6951 - val_loss: 1.5873 - val_acc: 0.6770\n",
      "Epoch 76/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.5682 - acc: 0.6970 - val_loss: 1.5801 - val_acc: 0.6850\n",
      "Epoch 77/1000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 1.5604 - acc: 0.6960 - val_loss: 1.5668 - val_acc: 0.6840\n",
      "Epoch 78/1000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 1.5525 - acc: 0.6969 - val_loss: 1.5598 - val_acc: 0.6840A: 0s - loss: 1.5455 - acc: 0.\n",
      "Epoch 79/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.5447 - acc: 0.6984 - val_loss: 1.5523 - val_acc: 0.6860\n",
      "Epoch 80/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.5369 - acc: 0.6985 - val_loss: 1.5475 - val_acc: 0.6850\n",
      "Epoch 81/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.5296 - acc: 0.6995 - val_loss: 1.5384 - val_acc: 0.6900\n",
      "Epoch 82/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.5224 - acc: 0.6985 - val_loss: 1.5332 - val_acc: 0.6850\n",
      "Epoch 83/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.5147 - acc: 0.7002 - val_loss: 1.5263 - val_acc: 0.6870\n",
      "Epoch 84/1000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 1.5075 - acc: 0.7019 - val_loss: 1.5216 - val_acc: 0.6920\n",
      "Epoch 85/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.5003 - acc: 0.7007 - val_loss: 1.5105 - val_acc: 0.6840\n",
      "Epoch 86/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.4930 - acc: 0.7026 - val_loss: 1.5059 - val_acc: 0.6890\n",
      "Epoch 87/1000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 1.4861 - acc: 0.7025 - val_loss: 1.4985 - val_acc: 0.6800\n",
      "Epoch 88/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.4791 - acc: 0.7025 - val_loss: 1.4866 - val_acc: 0.6870\n",
      "Epoch 89/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.4720 - acc: 0.7035 - val_loss: 1.4840 - val_acc: 0.6950\n",
      "Epoch 90/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.4653 - acc: 0.7043 - val_loss: 1.4772 - val_acc: 0.6860\n",
      "Epoch 91/1000\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 1.4589 - acc: 0.7036 - val_loss: 1.4686 - val_acc: 0.6890\n",
      "Epoch 92/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.4520 - acc: 0.7044 - val_loss: 1.4617 - val_acc: 0.6950\n",
      "Epoch 93/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.4461 - acc: 0.7049 - val_loss: 1.4555 - val_acc: 0.6900\n",
      "Epoch 94/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.4391 - acc: 0.7051 - val_loss: 1.4505 - val_acc: 0.6930\n",
      "Epoch 95/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.4327 - acc: 0.7067 - val_loss: 1.4502 - val_acc: 0.6960A: 0s - loss: 1.4325 - acc: 0.70\n",
      "Epoch 96/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.4266 - acc: 0.7050 - val_loss: 1.4390 - val_acc: 0.6990\n",
      "Epoch 97/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.4203 - acc: 0.7057 - val_loss: 1.4292 - val_acc: 0.6990\n",
      "Epoch 98/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.4139 - acc: 0.7075 - val_loss: 1.4244 - val_acc: 0.6970\n",
      "Epoch 99/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.4079 - acc: 0.7062 - val_loss: 1.4202 - val_acc: 0.7020\n",
      "Epoch 100/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.4016 - acc: 0.7090 - val_loss: 1.4137 - val_acc: 0.7000\n",
      "Epoch 101/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.3966 - acc: 0.7073 - val_loss: 1.4106 - val_acc: 0.6970\n",
      "Epoch 102/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.3900 - acc: 0.7099 - val_loss: 1.4019 - val_acc: 0.7000\n",
      "Epoch 103/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.3844 - acc: 0.7089 - val_loss: 1.3958 - val_acc: 0.7020\n",
      "Epoch 104/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.3783 - acc: 0.7084 - val_loss: 1.3907 - val_acc: 0.7020\n",
      "Epoch 105/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.3728 - acc: 0.7092 - val_loss: 1.3902 - val_acc: 0.7040\n",
      "Epoch 106/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.3673 - acc: 0.7120 - val_loss: 1.3806 - val_acc: 0.7000\n",
      "Epoch 107/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.3612 - acc: 0.7094 - val_loss: 1.3775 - val_acc: 0.6960\n",
      "Epoch 108/1000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 1.3560 - acc: 0.7109 - val_loss: 1.3713 - val_acc: 0.7010\n",
      "Epoch 109/1000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 1.3532 - acc: 0.7091- ETA: 0s - loss: 1.3605 - acc: 0. - 0s 44us/step - loss: 1.3508 - acc: 0.7114 - val_loss: 1.3645 - val_acc: 0.6930\n",
      "Epoch 110/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.3447 - acc: 0.7117 - val_loss: 1.3574 - val_acc: 0.7050\n",
      "Epoch 111/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.3397 - acc: 0.7134 - val_loss: 1.3551 - val_acc: 0.7050\n",
      "Epoch 112/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.3341 - acc: 0.7113 - val_loss: 1.3472 - val_acc: 0.6990\n",
      "Epoch 113/1000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 1.3263 - acc: 0.712 - 0s 43us/step - loss: 1.3286 - acc: 0.7116 - val_loss: 1.3529 - val_acc: 0.7000\n",
      "Epoch 114/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.3242 - acc: 0.7145 - val_loss: 1.3397 - val_acc: 0.6990\n",
      "Epoch 115/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.3185 - acc: 0.7127 - val_loss: 1.3324 - val_acc: 0.7000\n",
      "Epoch 116/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.3134 - acc: 0.7139 - val_loss: 1.3276 - val_acc: 0.7030\n",
      "Epoch 117/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.3083 - acc: 0.7133 - val_loss: 1.3207 - val_acc: 0.7060\n",
      "Epoch 118/1000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 1.3030 - acc: 0.7143 - val_loss: 1.3170 - val_acc: 0.7050\n",
      "Epoch 119/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.2985 - acc: 0.7150 - val_loss: 1.3147 - val_acc: 0.6990\n",
      "Epoch 120/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.2937 - acc: 0.7155 - val_loss: 1.3074 - val_acc: 0.7090\n",
      "Epoch 121/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.2896 - acc: 0.7149 - val_loss: 1.3028 - val_acc: 0.7060\n",
      "Epoch 122/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.2841 - acc: 0.7164 - val_loss: 1.2969 - val_acc: 0.7050\n",
      "Epoch 123/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.2793 - acc: 0.7160 - val_loss: 1.2973 - val_acc: 0.7020\n",
      "Epoch 124/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.2753 - acc: 0.7176 - val_loss: 1.2929 - val_acc: 0.6970\n",
      "Epoch 125/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.2703 - acc: 0.7174 - val_loss: 1.2895 - val_acc: 0.7040\n",
      "Epoch 126/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.2662 - acc: 0.7180 - val_loss: 1.2803 - val_acc: 0.7080\n",
      "Epoch 127/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.2608 - acc: 0.7170 - val_loss: 1.2756 - val_acc: 0.7080\n",
      "Epoch 128/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.2571 - acc: 0.7167 - val_loss: 1.2751 - val_acc: 0.7080\n",
      "Epoch 129/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.2526 - acc: 0.7191 - val_loss: 1.2671 - val_acc: 0.7090\n",
      "Epoch 130/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.2483 - acc: 0.7196 - val_loss: 1.2630 - val_acc: 0.7040\n",
      "Epoch 131/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.2441 - acc: 0.7200 - val_loss: 1.2602 - val_acc: 0.7040\n",
      "Epoch 132/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.2404 - acc: 0.7198 - val_loss: 1.2563 - val_acc: 0.7050\n",
      "Epoch 133/1000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 1.2360 - acc: 0.7208 - val_loss: 1.2561 - val_acc: 0.6960\n",
      "Epoch 134/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.2318 - acc: 0.7208 - val_loss: 1.2459 - val_acc: 0.7140\n",
      "Epoch 135/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.2283 - acc: 0.7215 - val_loss: 1.2491 - val_acc: 0.7070\n",
      "Epoch 136/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.2245 - acc: 0.7224 - val_loss: 1.2399 - val_acc: 0.7110\n",
      "Epoch 137/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.2197 - acc: 0.7231 - val_loss: 1.2368 - val_acc: 0.7090\n",
      "Epoch 138/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.2161 - acc: 0.7225 - val_loss: 1.2333 - val_acc: 0.7100\n",
      "Epoch 139/1000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 1.2127 - acc: 0.7211 - val_loss: 1.2271 - val_acc: 0.7100\n",
      "Epoch 140/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.2085 - acc: 0.7249 - val_loss: 1.2266 - val_acc: 0.7090\n",
      "Epoch 141/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.2050 - acc: 0.7248 - val_loss: 1.2212 - val_acc: 0.7120\n",
      "Epoch 142/1000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 1.2016 - acc: 0.7231 - val_loss: 1.2158 - val_acc: 0.7140\n",
      "Epoch 143/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.1979 - acc: 0.7242 - val_loss: 1.2148 - val_acc: 0.7150\n",
      "Epoch 144/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.1942 - acc: 0.7253 - val_loss: 1.2102 - val_acc: 0.7090\n",
      "Epoch 145/1000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 1.1910 - acc: 0.7242 - val_loss: 1.2084 - val_acc: 0.7090\n",
      "Epoch 146/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.1874 - acc: 0.7245 - val_loss: 1.2047 - val_acc: 0.7110\n",
      "Epoch 147/1000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 1.1842 - acc: 0.7258 - val_loss: 1.2024 - val_acc: 0.7070\n",
      "Epoch 148/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.1806 - acc: 0.7260 - val_loss: 1.1996 - val_acc: 0.7100\n",
      "Epoch 149/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.1776 - acc: 0.7275 - val_loss: 1.1933 - val_acc: 0.7100\n",
      "Epoch 150/1000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 1.1740 - acc: 0.7292 - val_loss: 1.1907 - val_acc: 0.7140\n",
      "Epoch 151/1000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 1.1709 - acc: 0.7280 - val_loss: 1.1883 - val_acc: 0.7130\n",
      "Epoch 152/1000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 1.1681 - acc: 0.7271 - val_loss: 1.1841 - val_acc: 0.7090\n",
      "Epoch 153/1000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 1.1647 - acc: 0.7271 - val_loss: 1.1893 - val_acc: 0.7100\n",
      "Epoch 154/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.1622 - acc: 0.7276 - val_loss: 1.1829 - val_acc: 0.7140\n",
      "Epoch 155/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.1596 - acc: 0.7274 - val_loss: 1.1786 - val_acc: 0.7110\n",
      "Epoch 156/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.1562 - acc: 0.7309 - val_loss: 1.1784 - val_acc: 0.7140\n",
      "Epoch 157/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.1532 - acc: 0.7324 - val_loss: 1.1717 - val_acc: 0.7170\n",
      "Epoch 158/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.1500 - acc: 0.7308 - val_loss: 1.1683 - val_acc: 0.7120\n",
      "Epoch 159/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.1478 - acc: 0.7315 - val_loss: 1.1668 - val_acc: 0.7110\n",
      "Epoch 160/1000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 1.1470 - acc: 0.730 - 0s 43us/step - loss: 1.1451 - acc: 0.7315 - val_loss: 1.1620 - val_acc: 0.7190\n",
      "Epoch 161/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.1421 - acc: 0.7311 - val_loss: 1.1590 - val_acc: 0.7170\n",
      "Epoch 162/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.1393 - acc: 0.7314 - val_loss: 1.1570 - val_acc: 0.7160\n",
      "Epoch 163/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.1368 - acc: 0.7331 - val_loss: 1.1550 - val_acc: 0.7190\n",
      "Epoch 164/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.1341 - acc: 0.7343 - val_loss: 1.1554 - val_acc: 0.7200\n",
      "Epoch 165/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.1324 - acc: 0.7332 - val_loss: 1.1487 - val_acc: 0.7200\n",
      "Epoch 166/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.1292 - acc: 0.7329 - val_loss: 1.1471 - val_acc: 0.7190\n",
      "Epoch 167/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.1266 - acc: 0.7334 - val_loss: 1.1459 - val_acc: 0.7170\n",
      "Epoch 168/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.1239 - acc: 0.7338 - val_loss: 1.1437 - val_acc: 0.7170\n",
      "Epoch 169/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.1219 - acc: 0.7348 - val_loss: 1.1411 - val_acc: 0.7200\n",
      "Epoch 170/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.1198 - acc: 0.7341 - val_loss: 1.1364 - val_acc: 0.7190\n",
      "Epoch 171/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.1175 - acc: 0.7348 - val_loss: 1.1359 - val_acc: 0.7220\n",
      "Epoch 172/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.1151 - acc: 0.7359 - val_loss: 1.1344 - val_acc: 0.7240\n",
      "Epoch 173/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.1127 - acc: 0.7361 - val_loss: 1.1321 - val_acc: 0.7210\n",
      "Epoch 174/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.1099 - acc: 0.7348 - val_loss: 1.1332 - val_acc: 0.7240\n",
      "Epoch 175/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.1086 - acc: 0.7345 - val_loss: 1.1374 - val_acc: 0.7110\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 176/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.1067 - acc: 0.7373 - val_loss: 1.1269 - val_acc: 0.7220\n",
      "Epoch 177/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.1038 - acc: 0.7361 - val_loss: 1.1277 - val_acc: 0.7250\n",
      "Epoch 178/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.1024 - acc: 0.7361 - val_loss: 1.1223 - val_acc: 0.7290\n",
      "Epoch 179/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.1002 - acc: 0.7368 - val_loss: 1.1207 - val_acc: 0.7240\n",
      "Epoch 180/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.0982 - acc: 0.7360 - val_loss: 1.1182 - val_acc: 0.7200\n",
      "Epoch 181/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.0964 - acc: 0.7371 - val_loss: 1.1153 - val_acc: 0.7240\n",
      "Epoch 182/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.0944 - acc: 0.7373 - val_loss: 1.1145 - val_acc: 0.7280\n",
      "Epoch 183/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.0923 - acc: 0.7371 - val_loss: 1.1106 - val_acc: 0.7250\n",
      "Epoch 184/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.0907 - acc: 0.7386 - val_loss: 1.1092 - val_acc: 0.7260\n",
      "Epoch 185/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.0881 - acc: 0.7392 - val_loss: 1.1138 - val_acc: 0.7240\n",
      "Epoch 186/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.0872 - acc: 0.7378 - val_loss: 1.1059 - val_acc: 0.7250\n",
      "Epoch 187/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.0852 - acc: 0.7398 - val_loss: 1.1042 - val_acc: 0.7260\n",
      "Epoch 188/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.0831 - acc: 0.7394 - val_loss: 1.1025 - val_acc: 0.7280\n",
      "Epoch 189/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.0816 - acc: 0.7398 - val_loss: 1.1035 - val_acc: 0.7260\n",
      "Epoch 190/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.0800 - acc: 0.7386 - val_loss: 1.1024 - val_acc: 0.7260\n",
      "Epoch 191/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.0782 - acc: 0.7386 - val_loss: 1.1025 - val_acc: 0.7230\n",
      "Epoch 192/1000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 1.0768 - acc: 0.7405 - val_loss: 1.1001 - val_acc: 0.7260\n",
      "Epoch 193/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.0752 - acc: 0.7400 - val_loss: 1.1010 - val_acc: 0.7240\n",
      "Epoch 194/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.0741 - acc: 0.7392 - val_loss: 1.1029 - val_acc: 0.7240\n",
      "Epoch 195/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.0723 - acc: 0.7408 - val_loss: 1.0931 - val_acc: 0.7270\n",
      "Epoch 196/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.0703 - acc: 0.7422 - val_loss: 1.0924 - val_acc: 0.7260\n",
      "Epoch 197/1000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 1.0688 - acc: 0.7401 - val_loss: 1.0895 - val_acc: 0.7280\n",
      "Epoch 198/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.0670 - acc: 0.7421 - val_loss: 1.0911 - val_acc: 0.7310: 0s - loss: 1.0682 - acc: 0.745 - ETA: 0s - loss: 1.0674 - acc: 0.743\n",
      "Epoch 199/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.0656 - acc: 0.7420 - val_loss: 1.0857 - val_acc: 0.7320\n",
      "Epoch 200/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.0643 - acc: 0.7418 - val_loss: 1.0843 - val_acc: 0.7280\n",
      "Epoch 201/1000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 1.0621 - acc: 0.7431 - val_loss: 1.0839 - val_acc: 0.7310\n",
      "Epoch 202/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.0605 - acc: 0.7429 - val_loss: 1.0851 - val_acc: 0.7350\n",
      "Epoch 203/1000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 1.0597 - acc: 0.7445 - val_loss: 1.0831 - val_acc: 0.7240\n",
      "Epoch 204/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.0577 - acc: 0.7436 - val_loss: 1.0832 - val_acc: 0.7230\n",
      "Epoch 205/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.0564 - acc: 0.7428 - val_loss: 1.0800 - val_acc: 0.7270\n",
      "Epoch 206/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.0549 - acc: 0.7440 - val_loss: 1.0775 - val_acc: 0.7270\n",
      "Epoch 207/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.0536 - acc: 0.7450 - val_loss: 1.0802 - val_acc: 0.7370\n",
      "Epoch 208/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.0522 - acc: 0.7436 - val_loss: 1.0784 - val_acc: 0.7230\n",
      "Epoch 209/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.0509 - acc: 0.7433 - val_loss: 1.0720 - val_acc: 0.7310\n",
      "Epoch 210/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.0493 - acc: 0.7411 - val_loss: 1.0725 - val_acc: 0.7320\n",
      "Epoch 211/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.0477 - acc: 0.7431 - val_loss: 1.0766 - val_acc: 0.7270\n",
      "Epoch 212/1000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 1.0467 - acc: 0.7460 - val_loss: 1.0720 - val_acc: 0.7280\n",
      "Epoch 213/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.0457 - acc: 0.7441 - val_loss: 1.0679 - val_acc: 0.7300\n",
      "Epoch 214/1000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 1.0441 - acc: 0.7439 - val_loss: 1.0695 - val_acc: 0.7300\n",
      "Epoch 215/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.0426 - acc: 0.7434 - val_loss: 1.0698 - val_acc: 0.7260\n",
      "Epoch 216/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.0417 - acc: 0.7464 - val_loss: 1.0662 - val_acc: 0.7270\n",
      "Epoch 217/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.0397 - acc: 0.7455 - val_loss: 1.0671 - val_acc: 0.7320\n",
      "Epoch 218/1000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 1.0389 - acc: 0.7461 - val_loss: 1.0654 - val_acc: 0.7280\n",
      "Epoch 219/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.0377 - acc: 0.7464 - val_loss: 1.0643 - val_acc: 0.7300\n",
      "Epoch 220/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.0367 - acc: 0.7442 - val_loss: 1.0598 - val_acc: 0.7390\n",
      "Epoch 221/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.0352 - acc: 0.7454 - val_loss: 1.0587 - val_acc: 0.7320\n",
      "Epoch 222/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.0342 - acc: 0.7482 - val_loss: 1.0598 - val_acc: 0.7330\n",
      "Epoch 223/1000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 1.0326 - acc: 0.7481 - val_loss: 1.0573 - val_acc: 0.7360\n",
      "Epoch 224/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.0320 - acc: 0.7464 - val_loss: 1.0604 - val_acc: 0.7350\n",
      "Epoch 225/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.0309 - acc: 0.7469 - val_loss: 1.0561 - val_acc: 0.7330\n",
      "Epoch 226/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.0290 - acc: 0.7476 - val_loss: 1.0539 - val_acc: 0.7300\n",
      "Epoch 227/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.0288 - acc: 0.7464 - val_loss: 1.0529 - val_acc: 0.7350\n",
      "Epoch 228/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.0267 - acc: 0.7476 - val_loss: 1.0626 - val_acc: 0.7310\n",
      "Epoch 229/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.0263 - acc: 0.7479 - val_loss: 1.0509 - val_acc: 0.7350\n",
      "Epoch 230/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.0247 - acc: 0.7478 - val_loss: 1.0490 - val_acc: 0.7340\n",
      "Epoch 231/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.0232 - acc: 0.7494 - val_loss: 1.0485 - val_acc: 0.7370\n",
      "Epoch 232/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.0223 - acc: 0.7480 - val_loss: 1.0467 - val_acc: 0.7320: 0s - loss: 1.0241 - acc: 0.7\n",
      "Epoch 233/1000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 1.0205 - acc: 0.7498 - val_loss: 1.0439 - val_acc: 0.7390\n",
      "Epoch 234/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.0195 - acc: 0.7484 - val_loss: 1.0467 - val_acc: 0.7380\n",
      "Epoch 235/1000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 1.0193 - acc: 0.7501 - val_loss: 1.0478 - val_acc: 0.7310\n",
      "Epoch 236/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.0183 - acc: 0.7475 - val_loss: 1.0466 - val_acc: 0.7300\n",
      "Epoch 237/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.0169 - acc: 0.7466 - val_loss: 1.0456 - val_acc: 0.7380\n",
      "Epoch 238/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.0158 - acc: 0.7524 - val_loss: 1.0477 - val_acc: 0.7270\n",
      "Epoch 239/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.0145 - acc: 0.7511 - val_loss: 1.0433 - val_acc: 0.7370\n",
      "Epoch 240/1000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 1.0133 - acc: 0.7496 - val_loss: 1.0400 - val_acc: 0.7370\n",
      "Epoch 241/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.0126 - acc: 0.7500 - val_loss: 1.0405 - val_acc: 0.7390\n",
      "Epoch 242/1000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 1.0113 - acc: 0.7530 - val_loss: 1.0370 - val_acc: 0.7390\n",
      "Epoch 243/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.0107 - acc: 0.7512 - val_loss: 1.0373 - val_acc: 0.7350\n",
      "Epoch 244/1000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 1.0091 - acc: 0.7510 - val_loss: 1.0373 - val_acc: 0.7420\n",
      "Epoch 245/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.0081 - acc: 0.7529 - val_loss: 1.0366 - val_acc: 0.7340\n",
      "Epoch 246/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.0075 - acc: 0.7514 - val_loss: 1.0373 - val_acc: 0.7310\n",
      "Epoch 247/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.0066 - acc: 0.7520 - val_loss: 1.0325 - val_acc: 0.7360\n",
      "Epoch 248/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.0059 - acc: 0.7504 - val_loss: 1.0328 - val_acc: 0.7360\n",
      "Epoch 249/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 1.0042 - acc: 0.7495 - val_loss: 1.0367 - val_acc: 0.7380\n",
      "Epoch 250/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.0040 - acc: 0.7516 - val_loss: 1.0309 - val_acc: 0.7400\n",
      "Epoch 251/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.0026 - acc: 0.7516 - val_loss: 1.0325 - val_acc: 0.7390\n",
      "Epoch 252/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.0016 - acc: 0.7512 - val_loss: 1.0336 - val_acc: 0.7430\n",
      "Epoch 253/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 1.0006 - acc: 0.7531 - val_loss: 1.0275 - val_acc: 0.7410\n",
      "Epoch 254/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.9996 - acc: 0.7531 - val_loss: 1.0322 - val_acc: 0.7360\n",
      "Epoch 255/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.9992 - acc: 0.7506 - val_loss: 1.0303 - val_acc: 0.7380\n",
      "Epoch 256/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.9977 - acc: 0.7531 - val_loss: 1.0390 - val_acc: 0.7280\n",
      "Epoch 257/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.9974 - acc: 0.7522 - val_loss: 1.0235 - val_acc: 0.7380\n",
      "Epoch 258/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.9963 - acc: 0.7534 - val_loss: 1.0329 - val_acc: 0.7310\n",
      "Epoch 259/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.9956 - acc: 0.7529 - val_loss: 1.0236 - val_acc: 0.7450\n",
      "Epoch 260/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.9940 - acc: 0.7535 - val_loss: 1.0268 - val_acc: 0.7350\n",
      "Epoch 261/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.9937 - acc: 0.7552 - val_loss: 1.0232 - val_acc: 0.7380\n",
      "Epoch 262/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.9921 - acc: 0.7516 - val_loss: 1.0260 - val_acc: 0.7390\n",
      "Epoch 263/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.9925 - acc: 0.7550 - val_loss: 1.0243 - val_acc: 0.7370\n",
      "Epoch 264/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.9913 - acc: 0.7530 - val_loss: 1.0207 - val_acc: 0.7400\n",
      "Epoch 265/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.9901 - acc: 0.7539 - val_loss: 1.0229 - val_acc: 0.7340\n",
      "Epoch 266/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.9890 - acc: 0.7532 - val_loss: 1.0181 - val_acc: 0.7410\n",
      "Epoch 267/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.9888 - acc: 0.7537 - val_loss: 1.0264 - val_acc: 0.7390\n",
      "Epoch 268/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.9877 - acc: 0.7554 - val_loss: 1.0168 - val_acc: 0.7440\n",
      "Epoch 269/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.9871 - acc: 0.7554 - val_loss: 1.0149 - val_acc: 0.7420\n",
      "Epoch 270/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.9854 - acc: 0.7560 - val_loss: 1.0175 - val_acc: 0.7380\n",
      "Epoch 271/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.9857 - acc: 0.7554 - val_loss: 1.0133 - val_acc: 0.7390\n",
      "Epoch 272/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.9842 - acc: 0.7549 - val_loss: 1.0203 - val_acc: 0.7420\n",
      "Epoch 273/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.9838 - acc: 0.7534 - val_loss: 1.0151 - val_acc: 0.7460\n",
      "Epoch 274/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.9826 - acc: 0.7551 - val_loss: 1.0157 - val_acc: 0.7370\n",
      "Epoch 275/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.9817 - acc: 0.7571 - val_loss: 1.0135 - val_acc: 0.7370\n",
      "Epoch 276/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.9817 - acc: 0.7569 - val_loss: 1.0163 - val_acc: 0.7320\n",
      "Epoch 277/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.9809 - acc: 0.7575 - val_loss: 1.0146 - val_acc: 0.7410\n",
      "Epoch 278/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.9803 - acc: 0.7570 - val_loss: 1.0128 - val_acc: 0.7400\n",
      "Epoch 279/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.9792 - acc: 0.7570 - val_loss: 1.0118 - val_acc: 0.7370\n",
      "Epoch 280/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.9781 - acc: 0.7554 - val_loss: 1.0098 - val_acc: 0.7410\n",
      "Epoch 281/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.9777 - acc: 0.7572 - val_loss: 1.0080 - val_acc: 0.7390\n",
      "Epoch 282/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.9769 - acc: 0.7581 - val_loss: 1.0047 - val_acc: 0.7450\n",
      "Epoch 283/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.9758 - acc: 0.7578 - val_loss: 1.0062 - val_acc: 0.7410\n",
      "Epoch 284/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.9761 - acc: 0.7586 - val_loss: 1.0105 - val_acc: 0.7330\n",
      "Epoch 285/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.9752 - acc: 0.7567 - val_loss: 1.0066 - val_acc: 0.7400\n",
      "Epoch 286/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.9735 - acc: 0.7564 - val_loss: 1.0149 - val_acc: 0.7320\n",
      "Epoch 287/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.9734 - acc: 0.7567 - val_loss: 1.0077 - val_acc: 0.7440\n",
      "Epoch 288/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.9723 - acc: 0.7586 - val_loss: 1.0065 - val_acc: 0.7430\n",
      "Epoch 289/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.9720 - acc: 0.7581 - val_loss: 1.0064 - val_acc: 0.7400\n",
      "Epoch 290/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.9716 - acc: 0.7586 - val_loss: 1.0075 - val_acc: 0.7390\n",
      "Epoch 291/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.9711 - acc: 0.7572 - val_loss: 1.0036 - val_acc: 0.7380\n",
      "Epoch 292/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.9697 - acc: 0.7580 - val_loss: 1.0023 - val_acc: 0.7450\n",
      "Epoch 293/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.9695 - acc: 0.7588 - val_loss: 1.0035 - val_acc: 0.7480\n",
      "Epoch 294/1000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.9685 - acc: 0.7560 - val_loss: 1.0023 - val_acc: 0.7410\n",
      "Epoch 295/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.9675 - acc: 0.7586 - val_loss: 0.9988 - val_acc: 0.7420\n",
      "Epoch 296/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.9673 - acc: 0.7574 - val_loss: 1.0089 - val_acc: 0.7360\n",
      "Epoch 297/1000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.9666 - acc: 0.7599 - val_loss: 1.0079 - val_acc: 0.7410\n",
      "Epoch 298/1000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.9664 - acc: 0.7569 - val_loss: 1.0012 - val_acc: 0.7410\n",
      "Epoch 299/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.9654 - acc: 0.7599 - val_loss: 1.0127 - val_acc: 0.7200\n",
      "Epoch 300/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.9647 - acc: 0.7582 - val_loss: 1.0192 - val_acc: 0.7330\n",
      "Epoch 301/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.9639 - acc: 0.7595 - val_loss: 1.0033 - val_acc: 0.7370\n",
      "Epoch 302/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.9632 - acc: 0.7562 - val_loss: 1.0076 - val_acc: 0.7410\n",
      "Epoch 303/1000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.9629 - acc: 0.7594 - val_loss: 0.9983 - val_acc: 0.7430\n",
      "Epoch 304/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.9627 - acc: 0.7590 - val_loss: 0.9992 - val_acc: 0.7430\n",
      "Epoch 305/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.9612 - acc: 0.7596 - val_loss: 0.9974 - val_acc: 0.7380\n",
      "Epoch 306/1000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.9606 - acc: 0.7600 - val_loss: 1.0022 - val_acc: 0.7290\n",
      "Epoch 307/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.9607 - acc: 0.7614 - val_loss: 0.9964 - val_acc: 0.7380\n",
      "Epoch 308/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.9600 - acc: 0.7599 - val_loss: 0.9958 - val_acc: 0.7400\n",
      "Epoch 309/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.9587 - acc: 0.7605 - val_loss: 0.9989 - val_acc: 0.7300\n",
      "Epoch 310/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.9591 - acc: 0.7610 - val_loss: 0.9942 - val_acc: 0.7460\n",
      "Epoch 311/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.9581 - acc: 0.7626 - val_loss: 0.9921 - val_acc: 0.7390\n",
      "Epoch 312/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.9574 - acc: 0.7595 - val_loss: 1.0007 - val_acc: 0.7390\n",
      "Epoch 313/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.9575 - acc: 0.7605 - val_loss: 0.9949 - val_acc: 0.7400\n",
      "Epoch 314/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.9564 - acc: 0.7624 - val_loss: 0.9958 - val_acc: 0.7370\n",
      "Epoch 315/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.9560 - acc: 0.7618 - val_loss: 0.9907 - val_acc: 0.7380\n",
      "Epoch 316/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.9551 - acc: 0.7619 - val_loss: 1.0018 - val_acc: 0.7430\n",
      "Epoch 317/1000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.9554 - acc: 0.7615 - val_loss: 0.9926 - val_acc: 0.7360\n",
      "Epoch 318/1000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.9540 - acc: 0.7612 - val_loss: 0.9892 - val_acc: 0.7430\n",
      "Epoch 319/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.9546 - acc: 0.7601 - val_loss: 0.9911 - val_acc: 0.7440\n",
      "Epoch 320/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.9531 - acc: 0.7615 - val_loss: 0.9870 - val_acc: 0.7450\n",
      "Epoch 321/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.9521 - acc: 0.7634 - val_loss: 0.9914 - val_acc: 0.7400\n",
      "Epoch 322/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.9518 - acc: 0.7629 - val_loss: 0.9917 - val_acc: 0.7380\n",
      "Epoch 323/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.9513 - acc: 0.7615 - val_loss: 0.9904 - val_acc: 0.7390\n",
      "Epoch 324/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.9517 - acc: 0.7630 - val_loss: 0.9884 - val_acc: 0.7350\n",
      "Epoch 325/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.9498 - acc: 0.7620 - val_loss: 0.9846 - val_acc: 0.7470\n",
      "Epoch 326/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.9495 - acc: 0.7632 - val_loss: 0.9908 - val_acc: 0.7410\n",
      "Epoch 327/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.9496 - acc: 0.7641 - val_loss: 0.9905 - val_acc: 0.7310\n",
      "Epoch 328/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.9490 - acc: 0.7611 - val_loss: 0.9902 - val_acc: 0.7390\n",
      "Epoch 329/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.9480 - acc: 0.7635 - val_loss: 0.9887 - val_acc: 0.7370\n",
      "Epoch 330/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.9480 - acc: 0.7654 - val_loss: 0.9893 - val_acc: 0.7330\n",
      "Epoch 331/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.9471 - acc: 0.7626 - val_loss: 0.9848 - val_acc: 0.7430\n",
      "Epoch 332/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.9473 - acc: 0.7649 - val_loss: 0.9855 - val_acc: 0.7420\n",
      "Epoch 333/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.9461 - acc: 0.7632 - val_loss: 0.9870 - val_acc: 0.7410\n",
      "Epoch 334/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.9454 - acc: 0.7632 - val_loss: 0.9886 - val_acc: 0.7430\n",
      "Epoch 335/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.9457 - acc: 0.7646 - val_loss: 0.9829 - val_acc: 0.7410\n",
      "Epoch 336/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.9453 - acc: 0.7624 - val_loss: 0.9844 - val_acc: 0.7390\n",
      "Epoch 337/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.9440 - acc: 0.7641 - val_loss: 0.9894 - val_acc: 0.7320A: 0s - loss: 0.9396 - acc: 0.76\n",
      "Epoch 338/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.9447 - acc: 0.7644 - val_loss: 0.9827 - val_acc: 0.7380\n",
      "Epoch 339/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.9422 - acc: 0.7641 - val_loss: 0.9805 - val_acc: 0.7490\n",
      "Epoch 340/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.9425 - acc: 0.7632 - val_loss: 0.9924 - val_acc: 0.7350\n",
      "Epoch 341/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.9433 - acc: 0.7620 - val_loss: 0.9815 - val_acc: 0.7420\n",
      "Epoch 342/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.9420 - acc: 0.7644 - val_loss: 0.9800 - val_acc: 0.7380\n",
      "Epoch 343/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.9417 - acc: 0.7654 - val_loss: 0.9952 - val_acc: 0.7210\n",
      "Epoch 344/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.9408 - acc: 0.7639 - val_loss: 0.9802 - val_acc: 0.7390\n",
      "Epoch 345/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.9395 - acc: 0.7661 - val_loss: 0.9820 - val_acc: 0.7370\n",
      "Epoch 346/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.9398 - acc: 0.7656 - val_loss: 0.9793 - val_acc: 0.7400\n",
      "Epoch 347/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.9391 - acc: 0.7650 - val_loss: 0.9853 - val_acc: 0.7470\n",
      "Epoch 348/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.9391 - acc: 0.7635 - val_loss: 0.9854 - val_acc: 0.7390\n",
      "Epoch 349/1000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 0.9386 - acc: 0.7650 - val_loss: 0.9781 - val_acc: 0.7400\n",
      "Epoch 350/1000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.9371 - acc: 0.7659 - val_loss: 0.9790 - val_acc: 0.7440\n",
      "Epoch 351/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.9380 - acc: 0.7640 - val_loss: 0.9782 - val_acc: 0.7450\n",
      "Epoch 352/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.9361 - acc: 0.7675 - val_loss: 0.9784 - val_acc: 0.7460\n",
      "Epoch 353/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.9362 - acc: 0.7652 - val_loss: 0.9773 - val_acc: 0.7400\n",
      "Epoch 354/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.9363 - acc: 0.7650 - val_loss: 0.9758 - val_acc: 0.7430\n",
      "Epoch 355/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.9356 - acc: 0.7660 - val_loss: 0.9784 - val_acc: 0.7440\n",
      "Epoch 356/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.9346 - acc: 0.7650 - val_loss: 0.9764 - val_acc: 0.7450\n",
      "Epoch 357/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.9353 - acc: 0.7669 - val_loss: 0.9762 - val_acc: 0.7390\n",
      "Epoch 358/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.9337 - acc: 0.7661 - val_loss: 0.9809 - val_acc: 0.7300\n",
      "Epoch 359/1000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.9335 - acc: 0.7660 - val_loss: 0.9762 - val_acc: 0.7390\n",
      "Epoch 360/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.9336 - acc: 0.7655 - val_loss: 0.9796 - val_acc: 0.7410\n",
      "Epoch 361/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.9327 - acc: 0.7665 - val_loss: 0.9788 - val_acc: 0.7380\n",
      "Epoch 362/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.9323 - acc: 0.7665 - val_loss: 0.9744 - val_acc: 0.7360A: 0s - loss: 0.9372 - acc: 0.76\n",
      "Epoch 363/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.9312 - acc: 0.7682 - val_loss: 0.9731 - val_acc: 0.7410\n",
      "Epoch 364/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.9310 - acc: 0.7676 - val_loss: 0.9707 - val_acc: 0.7440\n",
      "Epoch 365/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.9304 - acc: 0.7672 - val_loss: 0.9731 - val_acc: 0.7410\n",
      "Epoch 366/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.9305 - acc: 0.7671 - val_loss: 0.9711 - val_acc: 0.7420\n",
      "Epoch 367/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.9301 - acc: 0.7671 - val_loss: 0.9778 - val_acc: 0.7330\n",
      "Epoch 368/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.9295 - acc: 0.7675 - val_loss: 0.9744 - val_acc: 0.7390\n",
      "Epoch 369/1000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.9287 - acc: 0.7686 - val_loss: 0.9755 - val_acc: 0.7390\n",
      "Epoch 370/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.9285 - acc: 0.7671 - val_loss: 0.9747 - val_acc: 0.7360\n",
      "Epoch 371/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.9284 - acc: 0.7680 - val_loss: 0.9720 - val_acc: 0.7430\n",
      "Epoch 372/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.9275 - acc: 0.7666 - val_loss: 0.9695 - val_acc: 0.7420\n",
      "Epoch 373/1000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.9272 - acc: 0.7680 - val_loss: 0.9723 - val_acc: 0.7380\n",
      "Epoch 374/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.9274 - acc: 0.7669 - val_loss: 0.9723 - val_acc: 0.7390\n",
      "Epoch 375/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.9267 - acc: 0.7664 - val_loss: 0.9744 - val_acc: 0.7350\n",
      "Epoch 376/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.9255 - acc: 0.7649 - val_loss: 0.9716 - val_acc: 0.7390\n",
      "Epoch 377/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.9262 - acc: 0.7676 - val_loss: 0.9758 - val_acc: 0.7400\n",
      "Epoch 378/1000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 0.9245 - acc: 0.7687 - val_loss: 0.9791 - val_acc: 0.7250\n",
      "Epoch 379/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.9253 - acc: 0.7679 - val_loss: 0.9666 - val_acc: 0.7440\n",
      "Epoch 380/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.9241 - acc: 0.7671 - val_loss: 0.9694 - val_acc: 0.7420\n",
      "Epoch 381/1000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.9239 - acc: 0.7684 - val_loss: 0.9726 - val_acc: 0.7330\n",
      "Epoch 382/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.9236 - acc: 0.7706 - val_loss: 0.9720 - val_acc: 0.7400\n",
      "Epoch 383/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.9233 - acc: 0.7685 - val_loss: 0.9851 - val_acc: 0.7320\n",
      "Epoch 384/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.9232 - acc: 0.7692 - val_loss: 0.9675 - val_acc: 0.7440\n",
      "Epoch 385/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.9223 - acc: 0.7682 - val_loss: 0.9643 - val_acc: 0.7420\n",
      "Epoch 386/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.9210 - acc: 0.7705 - val_loss: 0.9738 - val_acc: 0.7410\n",
      "Epoch 387/1000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.9208 - acc: 0.7704 - val_loss: 0.9812 - val_acc: 0.7260\n",
      "Epoch 388/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.9209 - acc: 0.7656 - val_loss: 0.9679 - val_acc: 0.7380\n",
      "Epoch 389/1000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 0.9224 - acc: 0.768 - 0s 46us/step - loss: 0.9193 - acc: 0.7710 - val_loss: 0.9728 - val_acc: 0.7430\n",
      "Epoch 390/1000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.9209 - acc: 0.7698 - val_loss: 0.9671 - val_acc: 0.7420\n",
      "Epoch 391/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.9199 - acc: 0.7706 - val_loss: 0.9686 - val_acc: 0.7400\n",
      "Epoch 392/1000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.9197 - acc: 0.7702 - val_loss: 0.9722 - val_acc: 0.7320\n",
      "Epoch 393/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.9187 - acc: 0.7679 - val_loss: 0.9630 - val_acc: 0.7430\n",
      "Epoch 394/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.9175 - acc: 0.7690 - val_loss: 0.9641 - val_acc: 0.7410\n",
      "Epoch 395/1000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 0.9129 - acc: 0.7707- ETA: 0s - loss: 0.9356 - acc: 0 - 0s 43us/step - loss: 0.9174 - acc: 0.7695 - val_loss: 0.9647 - val_acc: 0.7390\n",
      "Epoch 396/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.9182 - acc: 0.7710 - val_loss: 0.9657 - val_acc: 0.7330\n",
      "Epoch 397/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.9168 - acc: 0.7722 - val_loss: 0.9642 - val_acc: 0.7360\n",
      "Epoch 398/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.9159 - acc: 0.7700 - val_loss: 0.9619 - val_acc: 0.7400\n",
      "Epoch 399/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.9153 - acc: 0.7704 - val_loss: 0.9681 - val_acc: 0.7380\n",
      "Epoch 400/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.9151 - acc: 0.7695 - val_loss: 0.9603 - val_acc: 0.7450\n",
      "Epoch 401/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.9154 - acc: 0.7702 - val_loss: 0.9606 - val_acc: 0.7450\n",
      "Epoch 402/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.9149 - acc: 0.7695 - val_loss: 0.9606 - val_acc: 0.7370\n",
      "Epoch 403/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.9145 - acc: 0.7675 - val_loss: 0.9609 - val_acc: 0.7370\n",
      "Epoch 404/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.9141 - acc: 0.7691 - val_loss: 0.9596 - val_acc: 0.7410\n",
      "Epoch 405/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.9132 - acc: 0.7711 - val_loss: 0.9637 - val_acc: 0.7450\n",
      "Epoch 406/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.9138 - acc: 0.7689 - val_loss: 0.9610 - val_acc: 0.7420\n",
      "Epoch 407/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.9124 - acc: 0.7712 - val_loss: 0.9641 - val_acc: 0.7360\n",
      "Epoch 408/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.9119 - acc: 0.7709 - val_loss: 0.9565 - val_acc: 0.7430\n",
      "Epoch 409/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.9109 - acc: 0.7714 - val_loss: 0.9552 - val_acc: 0.7390\n",
      "Epoch 410/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.9119 - acc: 0.7701 - val_loss: 0.9727 - val_acc: 0.7260\n",
      "Epoch 411/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.9115 - acc: 0.7705 - val_loss: 0.9603 - val_acc: 0.7390\n",
      "Epoch 412/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.9120 - acc: 0.7695 - val_loss: 0.9620 - val_acc: 0.7440\n",
      "Epoch 413/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.9107 - acc: 0.7695 - val_loss: 0.9735 - val_acc: 0.7390\n",
      "Epoch 414/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.9107 - acc: 0.7704 - val_loss: 0.9567 - val_acc: 0.7460\n",
      "Epoch 415/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.9095 - acc: 0.7694 - val_loss: 0.9612 - val_acc: 0.7450\n",
      "Epoch 416/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.9093 - acc: 0.7699 - val_loss: 0.9593 - val_acc: 0.7420\n",
      "Epoch 417/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.9081 - acc: 0.7712 - val_loss: 0.9594 - val_acc: 0.7440\n",
      "Epoch 418/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.9081 - acc: 0.7696 - val_loss: 0.9548 - val_acc: 0.7430\n",
      "Epoch 419/1000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.9079 - acc: 0.7708 - val_loss: 0.9536 - val_acc: 0.7390\n",
      "Epoch 420/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.9075 - acc: 0.7691 - val_loss: 0.9597 - val_acc: 0.7420\n",
      "Epoch 421/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.9086 - acc: 0.7717 - val_loss: 0.9565 - val_acc: 0.7330\n",
      "Epoch 422/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.9068 - acc: 0.7735 - val_loss: 0.9618 - val_acc: 0.7360\n",
      "Epoch 423/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.9063 - acc: 0.7712 - val_loss: 0.9591 - val_acc: 0.7490\n",
      "Epoch 424/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.9060 - acc: 0.7720 - val_loss: 0.9586 - val_acc: 0.7350\n",
      "Epoch 425/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.9061 - acc: 0.7738 - val_loss: 0.9572 - val_acc: 0.7360\n",
      "Epoch 426/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.9058 - acc: 0.7731 - val_loss: 0.9539 - val_acc: 0.7400\n",
      "Epoch 427/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.9047 - acc: 0.7740 - val_loss: 0.9539 - val_acc: 0.7430\n",
      "Epoch 428/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.9060 - acc: 0.7704 - val_loss: 0.9513 - val_acc: 0.7420\n",
      "Epoch 429/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.9047 - acc: 0.7732 - val_loss: 0.9495 - val_acc: 0.7490\n",
      "Epoch 430/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.9042 - acc: 0.7726 - val_loss: 0.9643 - val_acc: 0.7400\n",
      "Epoch 431/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.9039 - acc: 0.7759 - val_loss: 0.9522 - val_acc: 0.7450\n",
      "Epoch 432/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.9032 - acc: 0.7754 - val_loss: 0.9629 - val_acc: 0.7430\n",
      "Epoch 433/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.9039 - acc: 0.7708 - val_loss: 0.9826 - val_acc: 0.7240\n",
      "Epoch 434/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.9038 - acc: 0.7716 - val_loss: 0.9511 - val_acc: 0.7420\n",
      "Epoch 435/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.9038 - acc: 0.7735 - val_loss: 0.9588 - val_acc: 0.7370\n",
      "Epoch 436/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.9028 - acc: 0.7741 - val_loss: 0.9612 - val_acc: 0.7400\n",
      "Epoch 437/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.9024 - acc: 0.7721 - val_loss: 0.9776 - val_acc: 0.7380\n",
      "Epoch 438/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.9034 - acc: 0.7726 - val_loss: 0.9949 - val_acc: 0.7160\n",
      "Epoch 439/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.9023 - acc: 0.7734 - val_loss: 0.9508 - val_acc: 0.7380\n",
      "Epoch 440/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.9017 - acc: 0.7712 - val_loss: 0.9512 - val_acc: 0.7440\n",
      "Epoch 441/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8998 - acc: 0.7750 - val_loss: 0.9560 - val_acc: 0.7440\n",
      "Epoch 442/1000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 0.9011 - acc: 0.7726 - val_loss: 0.9624 - val_acc: 0.7450\n",
      "Epoch 443/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.9016 - acc: 0.7752 - val_loss: 0.9576 - val_acc: 0.7300\n",
      "Epoch 444/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8993 - acc: 0.7744 - val_loss: 0.9564 - val_acc: 0.7360\n",
      "Epoch 445/1000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.8998 - acc: 0.7724 - val_loss: 0.9471 - val_acc: 0.7490\n",
      "Epoch 446/1000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.8988 - acc: 0.7738 - val_loss: 0.9552 - val_acc: 0.7330: 0s - loss: 0.9042 - acc: 0.76\n",
      "Epoch 447/1000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.8978 - acc: 0.7756 - val_loss: 0.9455 - val_acc: 0.7410\n",
      "Epoch 448/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8979 - acc: 0.7760 - val_loss: 0.9609 - val_acc: 0.7500\n",
      "Epoch 449/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8994 - acc: 0.7730 - val_loss: 0.9589 - val_acc: 0.7310\n",
      "Epoch 450/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8972 - acc: 0.7754 - val_loss: 0.9499 - val_acc: 0.7390\n",
      "Epoch 451/1000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.8965 - acc: 0.7747 - val_loss: 0.9553 - val_acc: 0.7380\n",
      "Epoch 452/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8974 - acc: 0.7755 - val_loss: 0.9483 - val_acc: 0.7440\n",
      "Epoch 453/1000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.8955 - acc: 0.7750 - val_loss: 0.9541 - val_acc: 0.7420\n",
      "Epoch 454/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8967 - acc: 0.7762 - val_loss: 0.9487 - val_acc: 0.7450\n",
      "Epoch 455/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8967 - acc: 0.7732 - val_loss: 0.9449 - val_acc: 0.7480\n",
      "Epoch 456/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8948 - acc: 0.7747 - val_loss: 0.9448 - val_acc: 0.7450\n",
      "Epoch 457/1000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.8946 - acc: 0.7730 - val_loss: 0.9508 - val_acc: 0.7370\n",
      "Epoch 458/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8958 - acc: 0.7760 - val_loss: 0.9458 - val_acc: 0.7450\n",
      "Epoch 459/1000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.8947 - acc: 0.7780 - val_loss: 0.9472 - val_acc: 0.7400\n",
      "Epoch 460/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8940 - acc: 0.7777 - val_loss: 0.9465 - val_acc: 0.7450\n",
      "Epoch 461/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8933 - acc: 0.7761 - val_loss: 0.9496 - val_acc: 0.7390\n",
      "Epoch 462/1000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.8931 - acc: 0.7779 - val_loss: 0.9537 - val_acc: 0.7380\n",
      "Epoch 463/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8937 - acc: 0.7772 - val_loss: 0.9514 - val_acc: 0.7420\n",
      "Epoch 464/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8940 - acc: 0.7768 - val_loss: 0.9431 - val_acc: 0.7480\n",
      "Epoch 465/1000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.8930 - acc: 0.7790 - val_loss: 0.9437 - val_acc: 0.7490\n",
      "Epoch 466/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8908 - acc: 0.7776 - val_loss: 0.9521 - val_acc: 0.7430\n",
      "Epoch 467/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8921 - acc: 0.7785 - val_loss: 0.9573 - val_acc: 0.7360\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 468/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8926 - acc: 0.7790 - val_loss: 0.9493 - val_acc: 0.7400\n",
      "Epoch 469/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8912 - acc: 0.7768 - val_loss: 0.9527 - val_acc: 0.7380\n",
      "Epoch 470/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8898 - acc: 0.7765 - val_loss: 0.9444 - val_acc: 0.7410\n",
      "Epoch 471/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8919 - acc: 0.7758 - val_loss: 0.9472 - val_acc: 0.7450\n",
      "Epoch 472/1000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.8903 - acc: 0.7766 - val_loss: 0.9468 - val_acc: 0.7420\n",
      "Epoch 473/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8903 - acc: 0.7789 - val_loss: 0.9498 - val_acc: 0.7400\n",
      "Epoch 474/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8901 - acc: 0.7779 - val_loss: 0.9408 - val_acc: 0.7480\n",
      "Epoch 475/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8879 - acc: 0.7800 - val_loss: 0.9426 - val_acc: 0.7450\n",
      "Epoch 476/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8888 - acc: 0.7788 - val_loss: 0.9431 - val_acc: 0.7510\n",
      "Epoch 477/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8887 - acc: 0.7798 - val_loss: 0.9421 - val_acc: 0.7460\n",
      "Epoch 478/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8898 - acc: 0.7782 - val_loss: 0.9537 - val_acc: 0.7340\n",
      "Epoch 479/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8884 - acc: 0.7775 - val_loss: 0.9449 - val_acc: 0.7380\n",
      "Epoch 480/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8885 - acc: 0.7769 - val_loss: 0.9434 - val_acc: 0.7360\n",
      "Epoch 481/1000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 0.8876 - acc: 0.7781 - val_loss: 0.9404 - val_acc: 0.7460\n",
      "Epoch 482/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8881 - acc: 0.7798 - val_loss: 0.9503 - val_acc: 0.7340\n",
      "Epoch 483/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8863 - acc: 0.7802 - val_loss: 0.9444 - val_acc: 0.7490\n",
      "Epoch 484/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8878 - acc: 0.7791 - val_loss: 0.9466 - val_acc: 0.7400: 0s - loss: 0.8806 - acc: 0.7\n",
      "Epoch 485/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8859 - acc: 0.7780 - val_loss: 0.9399 - val_acc: 0.7380\n",
      "Epoch 486/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8854 - acc: 0.7788 - val_loss: 0.9688 - val_acc: 0.7430\n",
      "Epoch 487/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8878 - acc: 0.7790 - val_loss: 0.9382 - val_acc: 0.7470\n",
      "Epoch 488/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8860 - acc: 0.7809 - val_loss: 0.9375 - val_acc: 0.7500\n",
      "Epoch 489/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8861 - acc: 0.7805 - val_loss: 0.9550 - val_acc: 0.7300\n",
      "Epoch 490/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8843 - acc: 0.7784 - val_loss: 0.9386 - val_acc: 0.7470\n",
      "Epoch 491/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8840 - acc: 0.7807 - val_loss: 0.9430 - val_acc: 0.7450\n",
      "Epoch 492/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8843 - acc: 0.7814 - val_loss: 0.9549 - val_acc: 0.7480\n",
      "Epoch 493/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8859 - acc: 0.7791 - val_loss: 0.9387 - val_acc: 0.7500\n",
      "Epoch 494/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8832 - acc: 0.7812 - val_loss: 0.9406 - val_acc: 0.7550\n",
      "Epoch 495/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8827 - acc: 0.7785 - val_loss: 0.9413 - val_acc: 0.7470\n",
      "Epoch 496/1000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 0.8849 - acc: 0.7798- ETA: 0s - loss: 0.8715 - acc: 0 - 0s 42us/step - loss: 0.8827 - acc: 0.7802 - val_loss: 0.9370 - val_acc: 0.7500\n",
      "Epoch 497/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8821 - acc: 0.7798 - val_loss: 0.9370 - val_acc: 0.7390\n",
      "Epoch 498/1000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.8817 - acc: 0.7781 - val_loss: 0.9354 - val_acc: 0.7420\n",
      "Epoch 499/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8814 - acc: 0.7810 - val_loss: 0.9461 - val_acc: 0.7470\n",
      "Epoch 500/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8831 - acc: 0.7777 - val_loss: 0.9427 - val_acc: 0.7340\n",
      "Epoch 501/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8832 - acc: 0.7786 - val_loss: 0.9379 - val_acc: 0.7410\n",
      "Epoch 502/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8822 - acc: 0.7815 - val_loss: 0.9356 - val_acc: 0.7510\n",
      "Epoch 503/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8818 - acc: 0.7815 - val_loss: 0.9348 - val_acc: 0.7490\n",
      "Epoch 504/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8816 - acc: 0.7798 - val_loss: 0.9377 - val_acc: 0.7500\n",
      "Epoch 505/1000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.8796 - acc: 0.7801 - val_loss: 0.9472 - val_acc: 0.7400\n",
      "Epoch 506/1000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 0.8810 - acc: 0.7800 - val_loss: 0.9357 - val_acc: 0.7440\n",
      "Epoch 507/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8793 - acc: 0.7805 - val_loss: 0.9338 - val_acc: 0.7430\n",
      "Epoch 508/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8822 - acc: 0.7807 - val_loss: 0.9404 - val_acc: 0.7480\n",
      "Epoch 509/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8793 - acc: 0.7812 - val_loss: 0.9388 - val_acc: 0.7440\n",
      "Epoch 510/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8789 - acc: 0.7823 - val_loss: 0.9461 - val_acc: 0.7320\n",
      "Epoch 511/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8776 - acc: 0.7792 - val_loss: 0.9415 - val_acc: 0.7470\n",
      "Epoch 512/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8791 - acc: 0.7798 - val_loss: 0.9380 - val_acc: 0.7400\n",
      "Epoch 513/1000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.8782 - acc: 0.7821 - val_loss: 0.9325 - val_acc: 0.7460\n",
      "Epoch 514/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8765 - acc: 0.7845 - val_loss: 0.9346 - val_acc: 0.7470\n",
      "Epoch 515/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8783 - acc: 0.7826 - val_loss: 0.9445 - val_acc: 0.7470\n",
      "Epoch 516/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8792 - acc: 0.7830 - val_loss: 0.9362 - val_acc: 0.7520\n",
      "Epoch 517/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8779 - acc: 0.7821 - val_loss: 0.9349 - val_acc: 0.7440\n",
      "Epoch 518/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8785 - acc: 0.7823 - val_loss: 0.9354 - val_acc: 0.7520\n",
      "Epoch 519/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8772 - acc: 0.7815 - val_loss: 0.9523 - val_acc: 0.7380\n",
      "Epoch 520/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8771 - acc: 0.7809 - val_loss: 0.9382 - val_acc: 0.7460\n",
      "Epoch 521/1000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.8761 - acc: 0.7812 - val_loss: 0.9304 - val_acc: 0.7460\n",
      "Epoch 522/1000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.8763 - acc: 0.7825 - val_loss: 0.9485 - val_acc: 0.7390\n",
      "Epoch 523/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8775 - acc: 0.7818 - val_loss: 0.9374 - val_acc: 0.7450\n",
      "Epoch 524/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8764 - acc: 0.7840 - val_loss: 0.9353 - val_acc: 0.7460\n",
      "Epoch 525/1000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 0.8758 - acc: 0.7845 - val_loss: 0.9329 - val_acc: 0.7460\n",
      "Epoch 526/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8748 - acc: 0.7843 - val_loss: 0.9339 - val_acc: 0.7440\n",
      "Epoch 527/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8764 - acc: 0.7840 - val_loss: 0.9468 - val_acc: 0.7430\n",
      "Epoch 528/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8745 - acc: 0.7839 - val_loss: 0.9362 - val_acc: 0.7420\n",
      "Epoch 529/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8741 - acc: 0.7841 - val_loss: 0.9367 - val_acc: 0.7400\n",
      "Epoch 530/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8744 - acc: 0.7844 - val_loss: 0.9348 - val_acc: 0.7500\n",
      "Epoch 531/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8737 - acc: 0.7830 - val_loss: 0.9279 - val_acc: 0.7510\n",
      "Epoch 532/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8756 - acc: 0.7827 - val_loss: 0.9451 - val_acc: 0.7360\n",
      "Epoch 533/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8767 - acc: 0.7824 - val_loss: 0.9516 - val_acc: 0.7370\n",
      "Epoch 534/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8743 - acc: 0.7833 - val_loss: 0.9444 - val_acc: 0.7420\n",
      "Epoch 535/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8751 - acc: 0.7824 - val_loss: 0.9338 - val_acc: 0.7480\n",
      "Epoch 536/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8727 - acc: 0.7864 - val_loss: 0.9351 - val_acc: 0.7450\n",
      "Epoch 537/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8730 - acc: 0.7819 - val_loss: 0.9361 - val_acc: 0.7390\n",
      "Epoch 538/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8734 - acc: 0.7865 - val_loss: 0.9396 - val_acc: 0.7410\n",
      "Epoch 539/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8728 - acc: 0.7814 - val_loss: 0.9474 - val_acc: 0.7360\n",
      "Epoch 540/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8727 - acc: 0.7857 - val_loss: 0.9317 - val_acc: 0.7500\n",
      "Epoch 541/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8709 - acc: 0.7840 - val_loss: 0.9329 - val_acc: 0.7450\n",
      "Epoch 542/1000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.8730 - acc: 0.7860 - val_loss: 0.9445 - val_acc: 0.7360\n",
      "Epoch 543/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8716 - acc: 0.7854 - val_loss: 0.9389 - val_acc: 0.7470\n",
      "Epoch 544/1000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.8727 - acc: 0.7857 - val_loss: 0.9349 - val_acc: 0.7440\n",
      "Epoch 545/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8723 - acc: 0.7860 - val_loss: 0.9482 - val_acc: 0.7360\n",
      "Epoch 546/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8706 - acc: 0.7854 - val_loss: 0.9542 - val_acc: 0.7340\n",
      "Epoch 547/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8719 - acc: 0.7853 - val_loss: 0.9294 - val_acc: 0.7500\n",
      "Epoch 548/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8700 - acc: 0.7845 - val_loss: 0.9463 - val_acc: 0.7390\n",
      "Epoch 549/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8730 - acc: 0.7844 - val_loss: 0.9448 - val_acc: 0.7390\n",
      "Epoch 550/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8698 - acc: 0.7846 - val_loss: 0.9313 - val_acc: 0.7450\n",
      "Epoch 551/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8711 - acc: 0.7866 - val_loss: 0.9355 - val_acc: 0.7410\n",
      "Epoch 552/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8704 - acc: 0.7850 - val_loss: 0.9362 - val_acc: 0.7420\n",
      "Epoch 553/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8694 - acc: 0.7846 - val_loss: 0.9453 - val_acc: 0.7380\n",
      "Epoch 554/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8726 - acc: 0.7836 - val_loss: 0.9353 - val_acc: 0.7470\n",
      "Epoch 555/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8686 - acc: 0.7871 - val_loss: 0.9454 - val_acc: 0.7460\n",
      "Epoch 556/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8687 - acc: 0.7871 - val_loss: 0.9465 - val_acc: 0.7330\n",
      "Epoch 557/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8704 - acc: 0.7869 - val_loss: 0.9359 - val_acc: 0.7400\n",
      "Epoch 558/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8683 - acc: 0.7840 - val_loss: 0.9390 - val_acc: 0.7480\n",
      "Epoch 559/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8690 - acc: 0.7871 - val_loss: 0.9370 - val_acc: 0.7450\n",
      "Epoch 560/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8694 - acc: 0.7849 - val_loss: 0.9313 - val_acc: 0.7400\n",
      "Epoch 561/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8688 - acc: 0.7859 - val_loss: 0.9451 - val_acc: 0.7500\n",
      "Epoch 562/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8684 - acc: 0.7886 - val_loss: 0.9281 - val_acc: 0.7500\n",
      "Epoch 563/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8690 - acc: 0.7896 - val_loss: 0.9398 - val_acc: 0.7390\n",
      "Epoch 564/1000\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 0.8684 - acc: 0.7880 - val_loss: 0.9394 - val_acc: 0.7370\n",
      "Epoch 565/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8675 - acc: 0.7844 - val_loss: 0.9396 - val_acc: 0.7450\n",
      "Epoch 566/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8693 - acc: 0.7857 - val_loss: 0.9330 - val_acc: 0.7480\n",
      "Epoch 567/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8676 - acc: 0.7873 - val_loss: 0.9532 - val_acc: 0.7310\n",
      "Epoch 568/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8677 - acc: 0.7874 - val_loss: 0.9329 - val_acc: 0.7460\n",
      "Epoch 569/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8677 - acc: 0.7869 - val_loss: 0.9523 - val_acc: 0.7350\n",
      "Epoch 570/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8679 - acc: 0.7879 - val_loss: 0.9353 - val_acc: 0.7510\n",
      "Epoch 571/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8660 - acc: 0.7879 - val_loss: 0.9410 - val_acc: 0.7490\n",
      "Epoch 572/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8657 - acc: 0.7876 - val_loss: 0.9359 - val_acc: 0.7370\n",
      "Epoch 573/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8674 - acc: 0.7875 - val_loss: 0.9370 - val_acc: 0.7440\n",
      "Epoch 574/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8669 - acc: 0.7891 - val_loss: 0.9334 - val_acc: 0.7510\n",
      "Epoch 575/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8655 - acc: 0.7874 - val_loss: 0.9809 - val_acc: 0.7290\n",
      "Epoch 576/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8687 - acc: 0.7848 - val_loss: 0.9505 - val_acc: 0.7480\n",
      "Epoch 577/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8652 - acc: 0.7901 - val_loss: 0.9392 - val_acc: 0.7360\n",
      "Epoch 578/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8674 - acc: 0.7865 - val_loss: 0.9297 - val_acc: 0.7490\n",
      "Epoch 579/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8650 - acc: 0.7886 - val_loss: 0.9420 - val_acc: 0.7470\n",
      "Epoch 580/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8658 - acc: 0.7893 - val_loss: 0.9302 - val_acc: 0.7570\n",
      "Epoch 581/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8648 - acc: 0.7896 - val_loss: 0.9330 - val_acc: 0.7500\n",
      "Epoch 582/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8653 - acc: 0.7874 - val_loss: 0.9319 - val_acc: 0.7340\n",
      "Epoch 583/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8648 - acc: 0.7884 - val_loss: 0.9407 - val_acc: 0.7420\n",
      "Epoch 584/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8668 - acc: 0.7874 - val_loss: 0.9339 - val_acc: 0.7480\n",
      "Epoch 585/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8636 - acc: 0.7915 - val_loss: 0.9371 - val_acc: 0.7450\n",
      "Epoch 586/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8649 - acc: 0.7875 - val_loss: 0.9304 - val_acc: 0.7490\n",
      "Epoch 587/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8646 - acc: 0.7859 - val_loss: 0.9353 - val_acc: 0.7350\n",
      "Epoch 588/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8632 - acc: 0.7875 - val_loss: 0.9347 - val_acc: 0.7420\n",
      "Epoch 589/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8631 - acc: 0.7891 - val_loss: 0.9553 - val_acc: 0.7370\n",
      "Epoch 590/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8638 - acc: 0.7889 - val_loss: 0.9289 - val_acc: 0.7460\n",
      "Epoch 591/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8631 - acc: 0.7896 - val_loss: 0.9349 - val_acc: 0.7490\n",
      "Epoch 592/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8648 - acc: 0.7864 - val_loss: 0.9305 - val_acc: 0.7480\n",
      "Epoch 593/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8636 - acc: 0.7895 - val_loss: 0.9235 - val_acc: 0.7500\n",
      "Epoch 594/1000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.8625 - acc: 0.7869 - val_loss: 0.9341 - val_acc: 0.7460\n",
      "Epoch 595/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8628 - acc: 0.7899 - val_loss: 0.9291 - val_acc: 0.7490\n",
      "Epoch 596/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8627 - acc: 0.7924 - val_loss: 0.9411 - val_acc: 0.7420\n",
      "Epoch 597/1000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.8624 - acc: 0.7886 - val_loss: 0.9422 - val_acc: 0.7420\n",
      "Epoch 598/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8618 - acc: 0.7906 - val_loss: 0.9376 - val_acc: 0.7400\n",
      "Epoch 599/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8609 - acc: 0.7926 - val_loss: 0.9372 - val_acc: 0.7460\n",
      "Epoch 600/1000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 0.8631 - acc: 0.7904 - val_loss: 0.9327 - val_acc: 0.7450\n",
      "Epoch 601/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8603 - acc: 0.7927 - val_loss: 0.9277 - val_acc: 0.7430\n",
      "Epoch 602/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8636 - acc: 0.7901 - val_loss: 0.9506 - val_acc: 0.7480\n",
      "Epoch 603/1000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.8614 - acc: 0.7909 - val_loss: 0.9483 - val_acc: 0.7490\n",
      "Epoch 604/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8635 - acc: 0.7886 - val_loss: 0.9333 - val_acc: 0.7510\n",
      "Epoch 605/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8606 - acc: 0.7923 - val_loss: 0.9299 - val_acc: 0.7460\n",
      "Epoch 606/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8595 - acc: 0.7905 - val_loss: 0.9286 - val_acc: 0.7490\n",
      "Epoch 607/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8605 - acc: 0.7930 - val_loss: 0.9453 - val_acc: 0.7400\n",
      "Epoch 608/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8611 - acc: 0.7927 - val_loss: 0.9326 - val_acc: 0.7410\n",
      "Epoch 609/1000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.8585 - acc: 0.7914 - val_loss: 0.9314 - val_acc: 0.7440\n",
      "Epoch 610/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8620 - acc: 0.7897 - val_loss: 0.9322 - val_acc: 0.7460A: 0s - loss: 0.8599 - acc: 0.\n",
      "Epoch 611/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8600 - acc: 0.7900 - val_loss: 0.9339 - val_acc: 0.7420\n",
      "Epoch 612/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8591 - acc: 0.7920 - val_loss: 0.9265 - val_acc: 0.7460\n",
      "Epoch 613/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8599 - acc: 0.7915 - val_loss: 0.9364 - val_acc: 0.7540\n",
      "Epoch 614/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8608 - acc: 0.7900 - val_loss: 0.9316 - val_acc: 0.7510\n",
      "Epoch 615/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8586 - acc: 0.7936 - val_loss: 0.9282 - val_acc: 0.7460\n",
      "Epoch 616/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8607 - acc: 0.7903 - val_loss: 0.9576 - val_acc: 0.7400\n",
      "Epoch 617/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8605 - acc: 0.7938 - val_loss: 0.9475 - val_acc: 0.7410\n",
      "Epoch 618/1000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.8607 - acc: 0.7904 - val_loss: 0.9242 - val_acc: 0.7520\n",
      "Epoch 619/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8615 - acc: 0.7929 - val_loss: 0.9432 - val_acc: 0.7420\n",
      "Epoch 620/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8601 - acc: 0.7917 - val_loss: 0.9297 - val_acc: 0.7510\n",
      "Epoch 621/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8594 - acc: 0.7921 - val_loss: 0.9658 - val_acc: 0.7370\n",
      "Epoch 622/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8595 - acc: 0.7890 - val_loss: 0.9269 - val_acc: 0.7500\n",
      "Epoch 623/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8597 - acc: 0.7913 - val_loss: 0.9379 - val_acc: 0.7420\n",
      "Epoch 624/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8572 - acc: 0.7905 - val_loss: 0.9499 - val_acc: 0.7250\n",
      "Epoch 625/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8584 - acc: 0.7936 - val_loss: 0.9386 - val_acc: 0.7420\n",
      "Epoch 626/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8580 - acc: 0.7929 - val_loss: 0.9234 - val_acc: 0.7450\n",
      "Epoch 627/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8570 - acc: 0.7927 - val_loss: 0.9272 - val_acc: 0.7460\n",
      "Epoch 628/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8603 - acc: 0.7923 - val_loss: 0.9520 - val_acc: 0.7360\n",
      "Epoch 629/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8581 - acc: 0.7943 - val_loss: 0.9356 - val_acc: 0.7480\n",
      "Epoch 630/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8584 - acc: 0.7957 - val_loss: 0.9539 - val_acc: 0.7390\n",
      "Epoch 631/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8584 - acc: 0.7935 - val_loss: 0.9266 - val_acc: 0.7510\n",
      "Epoch 632/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8572 - acc: 0.7926 - val_loss: 0.9458 - val_acc: 0.7440\n",
      "Epoch 633/1000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.8572 - acc: 0.7938 - val_loss: 0.9296 - val_acc: 0.7460\n",
      "Epoch 634/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8564 - acc: 0.7945 - val_loss: 0.9238 - val_acc: 0.7500\n",
      "Epoch 635/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8568 - acc: 0.7949 - val_loss: 0.9350 - val_acc: 0.7420\n",
      "Epoch 636/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8565 - acc: 0.7935 - val_loss: 0.9538 - val_acc: 0.7500\n",
      "Epoch 637/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8583 - acc: 0.7904 - val_loss: 0.9293 - val_acc: 0.7420\n",
      "Epoch 638/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8581 - acc: 0.7924 - val_loss: 0.9383 - val_acc: 0.7420\n",
      "Epoch 639/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8567 - acc: 0.7951 - val_loss: 0.9401 - val_acc: 0.7400\n",
      "Epoch 640/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8569 - acc: 0.7956 - val_loss: 0.9958 - val_acc: 0.7180\n",
      "Epoch 641/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8572 - acc: 0.7925 - val_loss: 0.9342 - val_acc: 0.7410\n",
      "Epoch 642/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8566 - acc: 0.7934 - val_loss: 0.9265 - val_acc: 0.7460\n",
      "Epoch 643/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8552 - acc: 0.7936 - val_loss: 0.9256 - val_acc: 0.7480\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 644/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8563 - acc: 0.7946 - val_loss: 0.9328 - val_acc: 0.7410\n",
      "Epoch 645/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8559 - acc: 0.7950 - val_loss: 0.9321 - val_acc: 0.7500\n",
      "Epoch 646/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8559 - acc: 0.7925 - val_loss: 0.9269 - val_acc: 0.7510\n",
      "Epoch 647/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8552 - acc: 0.7939 - val_loss: 0.9251 - val_acc: 0.7530\n",
      "Epoch 648/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8544 - acc: 0.7939 - val_loss: 0.9351 - val_acc: 0.7430\n",
      "Epoch 649/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8537 - acc: 0.7959 - val_loss: 0.9218 - val_acc: 0.7480\n",
      "Epoch 650/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8565 - acc: 0.7924 - val_loss: 0.9218 - val_acc: 0.7520\n",
      "Epoch 651/1000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.8552 - acc: 0.7941 - val_loss: 0.9288 - val_acc: 0.7440\n",
      "Epoch 652/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8561 - acc: 0.7947 - val_loss: 0.9411 - val_acc: 0.7530\n",
      "Epoch 653/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8543 - acc: 0.7965 - val_loss: 0.9220 - val_acc: 0.7440\n",
      "Epoch 654/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8528 - acc: 0.7971 - val_loss: 0.9219 - val_acc: 0.7520\n",
      "Epoch 655/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8540 - acc: 0.7971 - val_loss: 0.9598 - val_acc: 0.7350\n",
      "Epoch 656/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8556 - acc: 0.7950 - val_loss: 0.9389 - val_acc: 0.7430\n",
      "Epoch 657/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8558 - acc: 0.7954 - val_loss: 0.9288 - val_acc: 0.7460\n",
      "Epoch 658/1000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.8525 - acc: 0.7951 - val_loss: 0.9278 - val_acc: 0.7490\n",
      "Epoch 659/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8531 - acc: 0.7950 - val_loss: 0.9242 - val_acc: 0.7450\n",
      "Epoch 660/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8548 - acc: 0.7976 - val_loss: 0.9277 - val_acc: 0.7500\n",
      "Epoch 661/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8536 - acc: 0.7960 - val_loss: 0.9203 - val_acc: 0.7480\n",
      "Epoch 662/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8527 - acc: 0.7966 - val_loss: 0.9316 - val_acc: 0.7430\n",
      "Epoch 663/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8535 - acc: 0.7963 - val_loss: 0.9290 - val_acc: 0.7450\n",
      "Epoch 664/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8525 - acc: 0.7943 - val_loss: 0.9256 - val_acc: 0.7520\n",
      "Epoch 665/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8531 - acc: 0.7960 - val_loss: 0.9436 - val_acc: 0.7430\n",
      "Epoch 666/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8531 - acc: 0.7946 - val_loss: 0.9343 - val_acc: 0.7450\n",
      "Epoch 667/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8531 - acc: 0.7973 - val_loss: 0.9343 - val_acc: 0.7450\n",
      "Epoch 668/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8517 - acc: 0.7956 - val_loss: 0.9262 - val_acc: 0.7410\n",
      "Epoch 669/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8524 - acc: 0.7984 - val_loss: 0.9216 - val_acc: 0.7460\n",
      "Epoch 670/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8521 - acc: 0.7959 - val_loss: 0.9317 - val_acc: 0.7360\n",
      "Epoch 671/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8532 - acc: 0.7995 - val_loss: 0.9502 - val_acc: 0.7420\n",
      "Epoch 672/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8540 - acc: 0.7954 - val_loss: 0.9353 - val_acc: 0.7490\n",
      "Epoch 673/1000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.8551 - acc: 0.7944 - val_loss: 0.9258 - val_acc: 0.7510\n",
      "Epoch 674/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8519 - acc: 0.7976 - val_loss: 0.9209 - val_acc: 0.7420A: 0s - loss: 0.8456 - acc: 0.\n",
      "Epoch 675/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8526 - acc: 0.7980 - val_loss: 0.9259 - val_acc: 0.7490\n",
      "Epoch 676/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8503 - acc: 0.8007 - val_loss: 0.9886 - val_acc: 0.7280\n",
      "Epoch 677/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8573 - acc: 0.7933 - val_loss: 0.9239 - val_acc: 0.7390A: 0s - loss: 0.8678 - acc: 0.7\n",
      "Epoch 678/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8525 - acc: 0.7949 - val_loss: 0.9541 - val_acc: 0.7400\n",
      "Epoch 679/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8532 - acc: 0.7964 - val_loss: 0.9505 - val_acc: 0.7480\n",
      "Epoch 680/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8525 - acc: 0.7981 - val_loss: 0.9293 - val_acc: 0.7430\n",
      "Epoch 681/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8516 - acc: 0.7949 - val_loss: 0.9392 - val_acc: 0.7460\n",
      "Epoch 682/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8502 - acc: 0.7979 - val_loss: 0.9325 - val_acc: 0.7530\n",
      "Epoch 683/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8504 - acc: 0.7987 - val_loss: 0.9433 - val_acc: 0.7350\n",
      "Epoch 684/1000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.8485 - acc: 0.7991 - val_loss: 0.9245 - val_acc: 0.7460\n",
      "Epoch 685/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8492 - acc: 0.7974 - val_loss: 0.9358 - val_acc: 0.7410\n",
      "Epoch 686/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8500 - acc: 0.7990 - val_loss: 0.9334 - val_acc: 0.7450\n",
      "Epoch 687/1000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 0.8503 - acc: 0.7995 - val_loss: 0.9296 - val_acc: 0.7400\n",
      "Epoch 688/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8498 - acc: 0.8021 - val_loss: 0.9575 - val_acc: 0.7330\n",
      "Epoch 689/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8493 - acc: 0.7977 - val_loss: 0.9449 - val_acc: 0.7330\n",
      "Epoch 690/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8514 - acc: 0.7968 - val_loss: 0.9356 - val_acc: 0.7520\n",
      "Epoch 691/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8503 - acc: 0.7998 - val_loss: 0.9254 - val_acc: 0.7450\n",
      "Epoch 692/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8494 - acc: 0.7981 - val_loss: 0.9493 - val_acc: 0.7490\n",
      "Epoch 693/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8521 - acc: 0.7998 - val_loss: 0.9346 - val_acc: 0.7420\n",
      "Epoch 694/1000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.8484 - acc: 0.8011 - val_loss: 0.9501 - val_acc: 0.7440\n",
      "Epoch 695/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8509 - acc: 0.7974 - val_loss: 0.9298 - val_acc: 0.7430\n",
      "Epoch 696/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8491 - acc: 0.7987 - val_loss: 0.9335 - val_acc: 0.7460\n",
      "Epoch 697/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8490 - acc: 0.7961 - val_loss: 0.9391 - val_acc: 0.7470\n",
      "Epoch 698/1000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.8511 - acc: 0.7981 - val_loss: 0.9524 - val_acc: 0.7400\n",
      "Epoch 699/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8486 - acc: 0.7990 - val_loss: 0.9504 - val_acc: 0.7490\n",
      "Epoch 700/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8482 - acc: 0.7987 - val_loss: 0.9256 - val_acc: 0.7500\n",
      "Epoch 701/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8455 - acc: 0.7996 - val_loss: 0.9362 - val_acc: 0.7450\n",
      "Epoch 702/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8475 - acc: 0.7996 - val_loss: 0.9240 - val_acc: 0.7470\n",
      "Epoch 703/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8470 - acc: 0.7999 - val_loss: 0.9304 - val_acc: 0.7480\n",
      "Epoch 704/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8500 - acc: 0.7974 - val_loss: 0.9306 - val_acc: 0.7490\n",
      "Epoch 705/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8484 - acc: 0.7989 - val_loss: 0.9360 - val_acc: 0.7520\n",
      "Epoch 706/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8467 - acc: 0.8011 - val_loss: 0.9484 - val_acc: 0.7400\n",
      "Epoch 707/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8516 - acc: 0.7941 - val_loss: 0.9294 - val_acc: 0.7530\n",
      "Epoch 708/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8454 - acc: 0.7991 - val_loss: 0.9550 - val_acc: 0.7500\n",
      "Epoch 709/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8489 - acc: 0.7986 - val_loss: 0.9570 - val_acc: 0.7390\n",
      "Epoch 710/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8474 - acc: 0.7975 - val_loss: 0.9249 - val_acc: 0.7550\n",
      "Epoch 711/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8486 - acc: 0.7994 - val_loss: 0.9556 - val_acc: 0.7470\n",
      "Epoch 712/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8475 - acc: 0.7989 - val_loss: 0.9493 - val_acc: 0.7490\n",
      "Epoch 713/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8458 - acc: 0.8017 - val_loss: 0.9227 - val_acc: 0.7500\n",
      "Epoch 714/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8466 - acc: 0.8016 - val_loss: 0.9282 - val_acc: 0.7420\n",
      "Epoch 715/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8452 - acc: 0.7993 - val_loss: 0.9925 - val_acc: 0.7280\n",
      "Epoch 716/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8498 - acc: 0.7985 - val_loss: 0.9391 - val_acc: 0.7440\n",
      "Epoch 717/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8451 - acc: 0.8010 - val_loss: 0.9489 - val_acc: 0.7430\n",
      "Epoch 718/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8462 - acc: 0.7999 - val_loss: 0.9211 - val_acc: 0.7520\n",
      "Epoch 719/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8454 - acc: 0.8003 - val_loss: 0.9252 - val_acc: 0.7450\n",
      "Epoch 720/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8484 - acc: 0.7985 - val_loss: 0.9391 - val_acc: 0.7490\n",
      "Epoch 721/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8449 - acc: 0.7991 - val_loss: 0.9837 - val_acc: 0.7350\n",
      "Epoch 722/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8461 - acc: 0.8006 - val_loss: 0.9357 - val_acc: 0.7480\n",
      "Epoch 723/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8463 - acc: 0.8011 - val_loss: 0.9257 - val_acc: 0.7530\n",
      "Epoch 724/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8467 - acc: 0.7993 - val_loss: 0.9514 - val_acc: 0.7380\n",
      "Epoch 725/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8468 - acc: 0.7976 - val_loss: 0.9327 - val_acc: 0.7440\n",
      "Epoch 726/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8448 - acc: 0.8025 - val_loss: 0.9395 - val_acc: 0.7500\n",
      "Epoch 727/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8471 - acc: 0.7985 - val_loss: 0.9712 - val_acc: 0.7370\n",
      "Epoch 728/1000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 0.8465 - acc: 0.8015 - val_loss: 0.9648 - val_acc: 0.7380\n",
      "Epoch 729/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8491 - acc: 0.8007 - val_loss: 0.9293 - val_acc: 0.7530: 0s - loss: 0.8510 - acc: 0.8\n",
      "Epoch 730/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8465 - acc: 0.8005 - val_loss: 0.9287 - val_acc: 0.7440\n",
      "Epoch 731/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8469 - acc: 0.8031 - val_loss: 0.9356 - val_acc: 0.7460\n",
      "Epoch 732/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8471 - acc: 0.8015 - val_loss: 0.9242 - val_acc: 0.7450\n",
      "Epoch 733/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8445 - acc: 0.8009 - val_loss: 0.9209 - val_acc: 0.7500\n",
      "Epoch 734/1000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 0.8443 - acc: 0.8006 - val_loss: 0.9589 - val_acc: 0.7400\n",
      "Epoch 735/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8459 - acc: 0.7998 - val_loss: 0.9226 - val_acc: 0.7490\n",
      "Epoch 736/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8432 - acc: 0.8023 - val_loss: 0.9216 - val_acc: 0.7550\n",
      "Epoch 737/1000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.8446 - acc: 0.8013 - val_loss: 0.9232 - val_acc: 0.7520\n",
      "Epoch 738/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8449 - acc: 0.8033 - val_loss: 0.9415 - val_acc: 0.7520\n",
      "Epoch 739/1000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 0.8454 - acc: 0.8006 - val_loss: 0.9258 - val_acc: 0.7470\n",
      "Epoch 740/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8476 - acc: 0.7985 - val_loss: 0.9221 - val_acc: 0.7550\n",
      "Epoch 741/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8451 - acc: 0.8025 - val_loss: 0.9394 - val_acc: 0.7430\n",
      "Epoch 742/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8445 - acc: 0.8016 - val_loss: 0.9208 - val_acc: 0.7570\n",
      "Epoch 743/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8415 - acc: 0.8024 - val_loss: 0.9393 - val_acc: 0.7550\n",
      "Epoch 744/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8417 - acc: 0.8020 - val_loss: 0.9269 - val_acc: 0.7490\n",
      "Epoch 745/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8411 - acc: 0.8028 - val_loss: 0.9194 - val_acc: 0.7470\n",
      "Epoch 746/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8437 - acc: 0.8020 - val_loss: 0.9233 - val_acc: 0.7470\n",
      "Epoch 747/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8414 - acc: 0.8030 - val_loss: 1.0055 - val_acc: 0.7260\n",
      "Epoch 748/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8435 - acc: 0.8004 - val_loss: 0.9313 - val_acc: 0.7430\n",
      "Epoch 749/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8407 - acc: 0.8017 - val_loss: 0.9406 - val_acc: 0.7470\n",
      "Epoch 750/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8445 - acc: 0.8003 - val_loss: 0.9380 - val_acc: 0.7460\n",
      "Epoch 751/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8422 - acc: 0.8023 - val_loss: 0.9213 - val_acc: 0.7540\n",
      "Epoch 752/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8422 - acc: 0.8023 - val_loss: 0.9417 - val_acc: 0.7380\n",
      "Epoch 753/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8449 - acc: 0.7999 - val_loss: 0.9499 - val_acc: 0.7430\n",
      "Epoch 754/1000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.8444 - acc: 0.8044 - val_loss: 0.9278 - val_acc: 0.7420\n",
      "Epoch 755/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8433 - acc: 0.8011 - val_loss: 0.9323 - val_acc: 0.7380\n",
      "Epoch 756/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8413 - acc: 0.7998 - val_loss: 0.9263 - val_acc: 0.7400\n",
      "Epoch 757/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8408 - acc: 0.8040 - val_loss: 0.9264 - val_acc: 0.7520\n",
      "Epoch 758/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8405 - acc: 0.8019 - val_loss: 0.9421 - val_acc: 0.7490\n",
      "Epoch 759/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8450 - acc: 0.8004 - val_loss: 1.0021 - val_acc: 0.7240\n",
      "Epoch 760/1000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 0.8475 - acc: 0.7993 - val_loss: 0.9318 - val_acc: 0.7540\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 761/1000\n",
      "8000/8000 [==============================] - 0s 48us/step - loss: 0.8424 - acc: 0.8023 - val_loss: 1.0174 - val_acc: 0.7210\n",
      "Epoch 762/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8467 - acc: 0.8013 - val_loss: 0.9211 - val_acc: 0.7570\n",
      "Epoch 763/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8438 - acc: 0.8007 - val_loss: 0.9218 - val_acc: 0.7450\n",
      "Epoch 764/1000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.8403 - acc: 0.8025 - val_loss: 0.9285 - val_acc: 0.7530\n",
      "Epoch 765/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8439 - acc: 0.8046 - val_loss: 0.9485 - val_acc: 0.7520\n",
      "Epoch 766/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8450 - acc: 0.7989 - val_loss: 0.9262 - val_acc: 0.7560\n",
      "Epoch 767/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8407 - acc: 0.8017 - val_loss: 0.9247 - val_acc: 0.7560\n",
      "Epoch 768/1000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.8411 - acc: 0.8064 - val_loss: 0.9678 - val_acc: 0.7350\n",
      "Epoch 769/1000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.8404 - acc: 0.8017 - val_loss: 0.9311 - val_acc: 0.7470\n",
      "Epoch 770/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8438 - acc: 0.7990 - val_loss: 0.9521 - val_acc: 0.7430\n",
      "Epoch 771/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8411 - acc: 0.8028 - val_loss: 0.9236 - val_acc: 0.7460\n",
      "Epoch 772/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8420 - acc: 0.8056 - val_loss: 0.9242 - val_acc: 0.7460\n",
      "Epoch 773/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8404 - acc: 0.8033 - val_loss: 0.9527 - val_acc: 0.7370\n",
      "Epoch 774/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8403 - acc: 0.8037 - val_loss: 0.9330 - val_acc: 0.7510\n",
      "Epoch 775/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8415 - acc: 0.8025 - val_loss: 0.9228 - val_acc: 0.7520\n",
      "Epoch 776/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8401 - acc: 0.8049 - val_loss: 0.9368 - val_acc: 0.7570\n",
      "Epoch 777/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8420 - acc: 0.8047 - val_loss: 0.9349 - val_acc: 0.7410\n",
      "Epoch 778/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8429 - acc: 0.8020 - val_loss: 0.9198 - val_acc: 0.7530\n",
      "Epoch 779/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8388 - acc: 0.8049 - val_loss: 0.9420 - val_acc: 0.7400\n",
      "Epoch 780/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8408 - acc: 0.8035 - val_loss: 0.9251 - val_acc: 0.7480\n",
      "Epoch 781/1000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.8376 - acc: 0.8067 - val_loss: 0.9288 - val_acc: 0.7570\n",
      "Epoch 782/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8380 - acc: 0.8053 - val_loss: 0.9173 - val_acc: 0.7540\n",
      "Epoch 783/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8384 - acc: 0.8030 - val_loss: 0.9463 - val_acc: 0.7400\n",
      "Epoch 784/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8430 - acc: 0.8034 - val_loss: 0.9239 - val_acc: 0.7420\n",
      "Epoch 785/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8415 - acc: 0.8030 - val_loss: 0.9509 - val_acc: 0.7410\n",
      "Epoch 786/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8412 - acc: 0.8001 - val_loss: 0.9553 - val_acc: 0.7340\n",
      "Epoch 787/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8392 - acc: 0.8009 - val_loss: 0.9297 - val_acc: 0.7510A: 0s - loss: 0.8426 - acc: 0.8\n",
      "Epoch 788/1000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.8368 - acc: 0.8047 - val_loss: 0.9375 - val_acc: 0.7480\n",
      "Epoch 789/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8398 - acc: 0.8054 - val_loss: 0.9356 - val_acc: 0.7390\n",
      "Epoch 790/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8370 - acc: 0.8019 - val_loss: 0.9240 - val_acc: 0.7540\n",
      "Epoch 791/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8370 - acc: 0.8049 - val_loss: 0.9323 - val_acc: 0.7500\n",
      "Epoch 792/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8397 - acc: 0.8056 - val_loss: 1.0428 - val_acc: 0.7230\n",
      "Epoch 793/1000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.8448 - acc: 0.8024 - val_loss: 0.9433 - val_acc: 0.7470\n",
      "Epoch 794/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8396 - acc: 0.8049 - val_loss: 0.9243 - val_acc: 0.7480\n",
      "Epoch 795/1000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.8378 - acc: 0.8069 - val_loss: 0.9224 - val_acc: 0.7480\n",
      "Epoch 796/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8374 - acc: 0.8043 - val_loss: 0.9264 - val_acc: 0.7380\n",
      "Epoch 797/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8374 - acc: 0.8043 - val_loss: 0.9263 - val_acc: 0.7550\n",
      "Epoch 798/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8466 - acc: 0.7996 - val_loss: 0.9251 - val_acc: 0.7450\n",
      "Epoch 799/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8359 - acc: 0.8061 - val_loss: 0.9263 - val_acc: 0.7520\n",
      "Epoch 800/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8398 - acc: 0.8035 - val_loss: 0.9359 - val_acc: 0.7450\n",
      "Epoch 801/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8381 - acc: 0.8059 - val_loss: 0.9229 - val_acc: 0.7450\n",
      "Epoch 802/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8357 - acc: 0.8084 - val_loss: 0.9490 - val_acc: 0.7530\n",
      "Epoch 803/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8386 - acc: 0.8043 - val_loss: 0.9211 - val_acc: 0.7490\n",
      "Epoch 804/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8356 - acc: 0.8055 - val_loss: 0.9221 - val_acc: 0.7550\n",
      "Epoch 805/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8368 - acc: 0.8051 - val_loss: 0.9396 - val_acc: 0.7480\n",
      "Epoch 806/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8380 - acc: 0.8039 - val_loss: 0.9643 - val_acc: 0.7410\n",
      "Epoch 807/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8363 - acc: 0.8070 - val_loss: 1.0406 - val_acc: 0.7130\n",
      "Epoch 808/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8401 - acc: 0.8070 - val_loss: 0.9266 - val_acc: 0.7380\n",
      "Epoch 809/1000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.8340 - acc: 0.8077 - val_loss: 0.9977 - val_acc: 0.7250\n",
      "Epoch 810/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8418 - acc: 0.8020 - val_loss: 0.9250 - val_acc: 0.7490\n",
      "Epoch 811/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8367 - acc: 0.8090 - val_loss: 0.9333 - val_acc: 0.7390\n",
      "Epoch 812/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8360 - acc: 0.8054 - val_loss: 0.9464 - val_acc: 0.7450\n",
      "Epoch 813/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8366 - acc: 0.8047 - val_loss: 0.9322 - val_acc: 0.7490\n",
      "Epoch 814/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8372 - acc: 0.8026 - val_loss: 0.9710 - val_acc: 0.7400\n",
      "Epoch 815/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8362 - acc: 0.8066 - val_loss: 0.9223 - val_acc: 0.7580\n",
      "Epoch 816/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8379 - acc: 0.8031 - val_loss: 0.9308 - val_acc: 0.7460\n",
      "Epoch 817/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8396 - acc: 0.8006 - val_loss: 0.9160 - val_acc: 0.7550\n",
      "Epoch 818/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8341 - acc: 0.8085 - val_loss: 0.9814 - val_acc: 0.7210\n",
      "Epoch 819/1000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.8398 - acc: 0.8036 - val_loss: 0.9485 - val_acc: 0.7520\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 820/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8352 - acc: 0.8054 - val_loss: 0.9436 - val_acc: 0.7440\n",
      "Epoch 821/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8345 - acc: 0.8055 - val_loss: 0.9364 - val_acc: 0.7550\n",
      "Epoch 822/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8368 - acc: 0.8039 - val_loss: 0.9309 - val_acc: 0.7460\n",
      "Epoch 823/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8353 - acc: 0.8055 - val_loss: 0.9222 - val_acc: 0.7570\n",
      "Epoch 824/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8336 - acc: 0.8065 - val_loss: 0.9182 - val_acc: 0.7590\n",
      "Epoch 825/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8355 - acc: 0.8043 - val_loss: 0.9398 - val_acc: 0.7490\n",
      "Epoch 826/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8336 - acc: 0.8061 - val_loss: 0.9543 - val_acc: 0.7430\n",
      "Epoch 827/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8369 - acc: 0.8036 - val_loss: 0.9498 - val_acc: 0.7380\n",
      "Epoch 828/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8372 - acc: 0.8066 - val_loss: 0.9595 - val_acc: 0.7470\n",
      "Epoch 829/1000\n",
      "8000/8000 [==============================] - 0s 41us/step - loss: 0.8358 - acc: 0.8069 - val_loss: 0.9273 - val_acc: 0.7570\n",
      "Epoch 830/1000\n",
      "8000/8000 [==============================] - 0s 40us/step - loss: 0.8354 - acc: 0.8076 - val_loss: 0.9186 - val_acc: 0.7530\n",
      "Epoch 831/1000\n",
      "8000/8000 [==============================] - 0s 41us/step - loss: 0.8350 - acc: 0.8077 - val_loss: 0.9226 - val_acc: 0.7500\n",
      "Epoch 832/1000\n",
      "8000/8000 [==============================] - 0s 40us/step - loss: 0.8398 - acc: 0.8036 - val_loss: 0.9289 - val_acc: 0.7610\n",
      "Epoch 833/1000\n",
      "8000/8000 [==============================] - 0s 41us/step - loss: 0.8333 - acc: 0.8091 - val_loss: 0.9660 - val_acc: 0.7480\n",
      "Epoch 834/1000\n",
      "8000/8000 [==============================] - 0s 41us/step - loss: 0.8410 - acc: 0.8030 - val_loss: 0.9328 - val_acc: 0.7440\n",
      "Epoch 835/1000\n",
      "8000/8000 [==============================] - 0s 41us/step - loss: 0.8329 - acc: 0.8049 - val_loss: 0.9261 - val_acc: 0.7580\n",
      "Epoch 836/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8335 - acc: 0.8103 - val_loss: 0.9484 - val_acc: 0.7500\n",
      "Epoch 837/1000\n",
      "8000/8000 [==============================] - 0s 41us/step - loss: 0.8326 - acc: 0.8084 - val_loss: 0.9354 - val_acc: 0.7500\n",
      "Epoch 838/1000\n",
      "8000/8000 [==============================] - 0s 41us/step - loss: 0.8384 - acc: 0.8039 - val_loss: 0.9317 - val_acc: 0.7450\n",
      "Epoch 839/1000\n",
      "8000/8000 [==============================] - 0s 40us/step - loss: 0.8343 - acc: 0.8074 - val_loss: 0.9531 - val_acc: 0.7440\n",
      "Epoch 840/1000\n",
      "8000/8000 [==============================] - 0s 41us/step - loss: 0.8358 - acc: 0.8063 - val_loss: 0.9288 - val_acc: 0.7420\n",
      "Epoch 841/1000\n",
      "8000/8000 [==============================] - 0s 41us/step - loss: 0.8311 - acc: 0.8081 - val_loss: 0.9916 - val_acc: 0.7360\n",
      "Epoch 842/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8357 - acc: 0.8055 - val_loss: 0.9299 - val_acc: 0.7540\n",
      "Epoch 843/1000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 0.8322 - acc: 0.8055 - val_loss: 0.9275 - val_acc: 0.7530\n",
      "Epoch 844/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8383 - acc: 0.8044 - val_loss: 0.9652 - val_acc: 0.7410\n",
      "Epoch 845/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8306 - acc: 0.8099 - val_loss: 0.9313 - val_acc: 0.7550\n",
      "Epoch 846/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8333 - acc: 0.8080 - val_loss: 0.9245 - val_acc: 0.7600\n",
      "Epoch 847/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8331 - acc: 0.8069 - val_loss: 0.9385 - val_acc: 0.7380\n",
      "Epoch 848/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8321 - acc: 0.8080 - val_loss: 0.9388 - val_acc: 0.7500\n",
      "Epoch 849/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8330 - acc: 0.8091 - val_loss: 0.9557 - val_acc: 0.7430\n",
      "Epoch 850/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8383 - acc: 0.8064 - val_loss: 0.9420 - val_acc: 0.7350\n",
      "Epoch 851/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8307 - acc: 0.8100 - val_loss: 0.9295 - val_acc: 0.7440\n",
      "Epoch 852/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8306 - acc: 0.8100 - val_loss: 0.9798 - val_acc: 0.7420\n",
      "Epoch 853/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8378 - acc: 0.8031 - val_loss: 1.0365 - val_acc: 0.7230\n",
      "Epoch 854/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8382 - acc: 0.8049 - val_loss: 0.9203 - val_acc: 0.7460\n",
      "Epoch 855/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8297 - acc: 0.8086 - val_loss: 0.9257 - val_acc: 0.7520\n",
      "Epoch 856/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8340 - acc: 0.8070 - val_loss: 0.9774 - val_acc: 0.7440\n",
      "Epoch 857/1000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 0.8304 - acc: 0.8104 - val_loss: 0.9407 - val_acc: 0.7600\n",
      "Epoch 858/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8305 - acc: 0.8076 - val_loss: 0.9176 - val_acc: 0.7560\n",
      "Epoch 859/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8338 - acc: 0.8099 - val_loss: 0.9392 - val_acc: 0.7470\n",
      "Epoch 860/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8322 - acc: 0.8063 - val_loss: 0.9214 - val_acc: 0.7620\n",
      "Epoch 861/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8288 - acc: 0.8126 - val_loss: 0.9346 - val_acc: 0.7590\n",
      "Epoch 862/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8311 - acc: 0.8055 - val_loss: 0.9189 - val_acc: 0.7560\n",
      "Epoch 863/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8326 - acc: 0.8059 - val_loss: 0.9212 - val_acc: 0.7480\n",
      "Epoch 864/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8313 - acc: 0.8075 - val_loss: 0.9416 - val_acc: 0.7520\n",
      "Epoch 865/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8301 - acc: 0.8096 - val_loss: 0.9371 - val_acc: 0.7530\n",
      "Epoch 866/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8301 - acc: 0.8109 - val_loss: 0.9341 - val_acc: 0.7570\n",
      "Epoch 867/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8304 - acc: 0.8096 - val_loss: 0.9298 - val_acc: 0.7370\n",
      "Epoch 868/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8328 - acc: 0.8093 - val_loss: 0.9299 - val_acc: 0.7530\n",
      "Epoch 869/1000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 0.8384 - acc: 0.8043 - val_loss: 0.9287 - val_acc: 0.7500\n",
      "Epoch 870/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8291 - acc: 0.8099 - val_loss: 0.9448 - val_acc: 0.7530\n",
      "Epoch 871/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8316 - acc: 0.8103 - val_loss: 0.9769 - val_acc: 0.7400\n",
      "Epoch 872/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8327 - acc: 0.8077 - val_loss: 0.9478 - val_acc: 0.7490\n",
      "Epoch 873/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8341 - acc: 0.8077 - val_loss: 0.9241 - val_acc: 0.7440\n",
      "Epoch 874/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8292 - acc: 0.8086 - val_loss: 0.9379 - val_acc: 0.7290\n",
      "Epoch 875/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8301 - acc: 0.8103 - val_loss: 1.0104 - val_acc: 0.7190\n",
      "Epoch 876/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8317 - acc: 0.8079 - val_loss: 0.9438 - val_acc: 0.7590\n",
      "Epoch 877/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8369 - acc: 0.8083 - val_loss: 0.9342 - val_acc: 0.7590\n",
      "Epoch 878/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8296 - acc: 0.8130 - val_loss: 0.9369 - val_acc: 0.7520\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 879/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8270 - acc: 0.8116 - val_loss: 0.9214 - val_acc: 0.7470\n",
      "Epoch 880/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8298 - acc: 0.8085 - val_loss: 0.9222 - val_acc: 0.7550\n",
      "Epoch 881/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8326 - acc: 0.8088 - val_loss: 0.9437 - val_acc: 0.7560\n",
      "Epoch 882/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8317 - acc: 0.8067 - val_loss: 0.9914 - val_acc: 0.7180\n",
      "Epoch 883/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8333 - acc: 0.8100 - val_loss: 0.9220 - val_acc: 0.7540\n",
      "Epoch 884/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8284 - acc: 0.8106 - val_loss: 0.9277 - val_acc: 0.7530\n",
      "Epoch 885/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8365 - acc: 0.8061 - val_loss: 0.9876 - val_acc: 0.7270\n",
      "Epoch 886/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8298 - acc: 0.8113 - val_loss: 0.9217 - val_acc: 0.7510\n",
      "Epoch 887/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8274 - acc: 0.8113 - val_loss: 0.9242 - val_acc: 0.7520\n",
      "Epoch 888/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8275 - acc: 0.8110 - val_loss: 0.9669 - val_acc: 0.7450\n",
      "Epoch 889/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8350 - acc: 0.8077 - val_loss: 0.9321 - val_acc: 0.7500\n",
      "Epoch 890/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8293 - acc: 0.8096 - val_loss: 0.9332 - val_acc: 0.7540\n",
      "Epoch 891/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8298 - acc: 0.8081 - val_loss: 0.9523 - val_acc: 0.7450\n",
      "Epoch 892/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8349 - acc: 0.8066 - val_loss: 0.9199 - val_acc: 0.7500\n",
      "Epoch 893/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8278 - acc: 0.8114 - val_loss: 0.9523 - val_acc: 0.7440\n",
      "Epoch 894/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8289 - acc: 0.8120 - val_loss: 0.9245 - val_acc: 0.7560\n",
      "Epoch 895/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8304 - acc: 0.8101 - val_loss: 0.9169 - val_acc: 0.7470\n",
      "Epoch 896/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8258 - acc: 0.8125 - val_loss: 0.9623 - val_acc: 0.7420\n",
      "Epoch 897/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8302 - acc: 0.8099 - val_loss: 0.9259 - val_acc: 0.7460\n",
      "Epoch 898/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8241 - acc: 0.8141 - val_loss: 0.9565 - val_acc: 0.7370\n",
      "Epoch 899/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8296 - acc: 0.8113 - val_loss: 0.9362 - val_acc: 0.7530\n",
      "Epoch 900/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8314 - acc: 0.8095 - val_loss: 0.9361 - val_acc: 0.7560\n",
      "Epoch 901/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8282 - acc: 0.8120 - val_loss: 1.0280 - val_acc: 0.7350\n",
      "Epoch 902/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8345 - acc: 0.8080 - val_loss: 0.9460 - val_acc: 0.7450\n",
      "Epoch 903/1000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.8288 - acc: 0.8081 - val_loss: 0.9209 - val_acc: 0.7620A: 0s - loss: 0.8288 - acc: 0.80\n",
      "Epoch 904/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8296 - acc: 0.8093 - val_loss: 0.9722 - val_acc: 0.7410\n",
      "Epoch 905/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8291 - acc: 0.8123 - val_loss: 0.9166 - val_acc: 0.7590\n",
      "Epoch 906/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8296 - acc: 0.8101 - val_loss: 0.9246 - val_acc: 0.7380\n",
      "Epoch 907/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8308 - acc: 0.8088 - val_loss: 0.9275 - val_acc: 0.7500\n",
      "Epoch 908/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8325 - acc: 0.8080 - val_loss: 0.9507 - val_acc: 0.7390\n",
      "Epoch 909/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8270 - acc: 0.8129 - val_loss: 0.9304 - val_acc: 0.7580\n",
      "Epoch 910/1000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.8247 - acc: 0.8127 - val_loss: 0.9278 - val_acc: 0.7590\n",
      "Epoch 911/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8307 - acc: 0.8080 - val_loss: 0.9538 - val_acc: 0.7370\n",
      "Epoch 912/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8319 - acc: 0.8080 - val_loss: 0.9391 - val_acc: 0.7510\n",
      "Epoch 913/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8261 - acc: 0.8127 - val_loss: 0.9169 - val_acc: 0.7540\n",
      "Epoch 914/1000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 0.8223 - acc: 0.8126 - val_loss: 0.9696 - val_acc: 0.7360\n",
      "Epoch 915/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8296 - acc: 0.8114 - val_loss: 0.9664 - val_acc: 0.7380\n",
      "Epoch 916/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8247 - acc: 0.8161 - val_loss: 0.9230 - val_acc: 0.7570\n",
      "Epoch 917/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8243 - acc: 0.8124 - val_loss: 0.9480 - val_acc: 0.7470\n",
      "Epoch 918/1000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.8265 - acc: 0.8139 - val_loss: 0.9430 - val_acc: 0.7520\n",
      "Epoch 919/1000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 0.8250 - acc: 0.8130 - val_loss: 0.9165 - val_acc: 0.7480\n",
      "Epoch 920/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8280 - acc: 0.8150 - val_loss: 0.9250 - val_acc: 0.7550\n",
      "Epoch 921/1000\n",
      "8000/8000 [==============================] - 0s 47us/step - loss: 0.8239 - acc: 0.8132 - val_loss: 0.9157 - val_acc: 0.7430\n",
      "Epoch 922/1000\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 0.8255 - acc: 0.8119 - val_loss: 0.9228 - val_acc: 0.7470\n",
      "Epoch 923/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8227 - acc: 0.8136 - val_loss: 0.9172 - val_acc: 0.7510\n",
      "Epoch 924/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8222 - acc: 0.8123 - val_loss: 0.9812 - val_acc: 0.7200\n",
      "Epoch 925/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8258 - acc: 0.8134 - val_loss: 0.9385 - val_acc: 0.7530\n",
      "Epoch 926/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8239 - acc: 0.8104 - val_loss: 0.9891 - val_acc: 0.7320\n",
      "Epoch 927/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8252 - acc: 0.8139 - val_loss: 0.9194 - val_acc: 0.7500\n",
      "Epoch 928/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8219 - acc: 0.8171 - val_loss: 0.9351 - val_acc: 0.7420\n",
      "Epoch 929/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8258 - acc: 0.8106 - val_loss: 0.9214 - val_acc: 0.7420\n",
      "Epoch 930/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8247 - acc: 0.8121 - val_loss: 0.9440 - val_acc: 0.7520\n",
      "Epoch 931/1000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.8273 - acc: 0.8111 - val_loss: 0.9351 - val_acc: 0.7490\n",
      "Epoch 932/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8274 - acc: 0.8120 - val_loss: 0.9778 - val_acc: 0.7340\n",
      "Epoch 933/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8265 - acc: 0.8151 - val_loss: 1.0064 - val_acc: 0.7300\n",
      "Epoch 934/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8279 - acc: 0.8121 - val_loss: 0.9746 - val_acc: 0.7510\n",
      "Epoch 935/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8239 - acc: 0.8113 - val_loss: 0.9299 - val_acc: 0.7490\n",
      "Epoch 936/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8250 - acc: 0.8123 - val_loss: 0.9434 - val_acc: 0.7540\n",
      "Epoch 937/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8261 - acc: 0.8165 - val_loss: 0.9571 - val_acc: 0.7410\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 938/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8269 - acc: 0.8110 - val_loss: 0.9520 - val_acc: 0.7450\n",
      "Epoch 939/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8256 - acc: 0.8123 - val_loss: 0.9292 - val_acc: 0.7410\n",
      "Epoch 940/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8245 - acc: 0.8147 - val_loss: 0.9468 - val_acc: 0.7470\n",
      "Epoch 941/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8235 - acc: 0.8143 - val_loss: 0.9330 - val_acc: 0.7540\n",
      "Epoch 942/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8217 - acc: 0.8174 - val_loss: 0.9221 - val_acc: 0.7610\n",
      "Epoch 943/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8204 - acc: 0.8160 - val_loss: 0.9435 - val_acc: 0.7490\n",
      "Epoch 944/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8240 - acc: 0.8132 - val_loss: 0.9866 - val_acc: 0.7440\n",
      "Epoch 945/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8276 - acc: 0.8111 - val_loss: 0.9509 - val_acc: 0.7500\n",
      "Epoch 946/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8244 - acc: 0.8124 - val_loss: 0.9468 - val_acc: 0.7490\n",
      "Epoch 947/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8281 - acc: 0.8137 - val_loss: 0.9305 - val_acc: 0.7460\n",
      "Epoch 948/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8283 - acc: 0.8086 - val_loss: 0.9229 - val_acc: 0.7560\n",
      "Epoch 949/1000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.8220 - acc: 0.8143 - val_loss: 0.9730 - val_acc: 0.7420\n",
      "Epoch 950/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8246 - acc: 0.8116 - val_loss: 0.9321 - val_acc: 0.7370\n",
      "Epoch 951/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8220 - acc: 0.8139 - val_loss: 0.9322 - val_acc: 0.7420\n",
      "Epoch 952/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8256 - acc: 0.8116 - val_loss: 0.9219 - val_acc: 0.7480\n",
      "Epoch 953/1000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.8212 - acc: 0.8121 - val_loss: 0.9791 - val_acc: 0.7180\n",
      "Epoch 954/1000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.8231 - acc: 0.8139 - val_loss: 0.9571 - val_acc: 0.7400\n",
      "Epoch 955/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8258 - acc: 0.8125 - val_loss: 0.9170 - val_acc: 0.7500\n",
      "Epoch 956/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8215 - acc: 0.8135 - val_loss: 0.9552 - val_acc: 0.7470\n",
      "Epoch 957/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8267 - acc: 0.8125 - val_loss: 0.9194 - val_acc: 0.7470\n",
      "Epoch 958/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8218 - acc: 0.8150 - val_loss: 0.9140 - val_acc: 0.7470\n",
      "Epoch 959/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8199 - acc: 0.8146 - val_loss: 0.9341 - val_acc: 0.7530\n",
      "Epoch 960/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8238 - acc: 0.8165 - val_loss: 0.9521 - val_acc: 0.7520\n",
      "Epoch 961/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8218 - acc: 0.8146 - val_loss: 0.9299 - val_acc: 0.7580\n",
      "Epoch 962/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8204 - acc: 0.8170 - val_loss: 0.9231 - val_acc: 0.7560\n",
      "Epoch 963/1000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.8231 - acc: 0.8137 - val_loss: 0.9495 - val_acc: 0.7480\n",
      "Epoch 964/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8200 - acc: 0.8177 - val_loss: 0.9735 - val_acc: 0.7220\n",
      "Epoch 965/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8230 - acc: 0.8166 - val_loss: 0.9236 - val_acc: 0.7500\n",
      "Epoch 966/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8225 - acc: 0.8139 - val_loss: 0.9142 - val_acc: 0.7470\n",
      "Epoch 967/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8231 - acc: 0.8160 - val_loss: 0.9181 - val_acc: 0.7470\n",
      "Epoch 968/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8198 - acc: 0.8126 - val_loss: 0.9210 - val_acc: 0.7500\n",
      "Epoch 969/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8200 - acc: 0.8165 - val_loss: 0.9162 - val_acc: 0.7580\n",
      "Epoch 970/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8232 - acc: 0.8149 - val_loss: 0.9426 - val_acc: 0.7490\n",
      "Epoch 971/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8203 - acc: 0.8169 - val_loss: 0.9291 - val_acc: 0.7480\n",
      "Epoch 972/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8188 - acc: 0.8157 - val_loss: 1.0139 - val_acc: 0.7340\n",
      "Epoch 973/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8228 - acc: 0.8124 - val_loss: 0.9334 - val_acc: 0.7480\n",
      "Epoch 974/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8229 - acc: 0.8143 - val_loss: 0.9644 - val_acc: 0.7380\n",
      "Epoch 975/1000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.8239 - acc: 0.8099 - val_loss: 0.9228 - val_acc: 0.7540\n",
      "Epoch 976/1000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.8209 - acc: 0.8160 - val_loss: 0.9214 - val_acc: 0.7490\n",
      "Epoch 977/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8194 - acc: 0.8150 - val_loss: 0.9914 - val_acc: 0.7380\n",
      "Epoch 978/1000\n",
      "8000/8000 [==============================] - 0s 45us/step - loss: 0.8230 - acc: 0.8207 - val_loss: 0.9532 - val_acc: 0.7480\n",
      "Epoch 979/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8195 - acc: 0.8144 - val_loss: 0.9380 - val_acc: 0.7570\n",
      "Epoch 980/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8223 - acc: 0.8166 - val_loss: 0.9711 - val_acc: 0.7310\n",
      "Epoch 981/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8257 - acc: 0.8143 - val_loss: 1.0159 - val_acc: 0.7130\n",
      "Epoch 982/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8236 - acc: 0.8136 - val_loss: 0.9421 - val_acc: 0.7500\n",
      "Epoch 983/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8186 - acc: 0.8153 - val_loss: 0.9633 - val_acc: 0.7460\n",
      "Epoch 984/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8297 - acc: 0.8100 - val_loss: 0.9612 - val_acc: 0.7370\n",
      "Epoch 985/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8195 - acc: 0.8165 - val_loss: 1.1104 - val_acc: 0.6920\n",
      "Epoch 986/1000\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 0.8319 - acc: 0.810 - 0s 42us/step - loss: 0.8317 - acc: 0.8109 - val_loss: 0.9264 - val_acc: 0.7640\n",
      "Epoch 987/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8213 - acc: 0.8134 - val_loss: 0.9198 - val_acc: 0.7540\n",
      "Epoch 988/1000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.8236 - acc: 0.8136 - val_loss: 0.9342 - val_acc: 0.7500\n",
      "Epoch 989/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8256 - acc: 0.8116 - val_loss: 0.9237 - val_acc: 0.7470\n",
      "Epoch 990/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8171 - acc: 0.8167 - val_loss: 0.9360 - val_acc: 0.7590\n",
      "Epoch 991/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8201 - acc: 0.8165 - val_loss: 0.9779 - val_acc: 0.7370\n",
      "Epoch 992/1000\n",
      "8000/8000 [==============================] - 0s 44us/step - loss: 0.8234 - acc: 0.8137 - val_loss: 0.9473 - val_acc: 0.7490\n",
      "Epoch 993/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8198 - acc: 0.8147 - val_loss: 0.9226 - val_acc: 0.7440\n",
      "Epoch 994/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8193 - acc: 0.8173 - val_loss: 0.9277 - val_acc: 0.7530\n",
      "Epoch 995/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8256 - acc: 0.8110 - val_loss: 0.9579 - val_acc: 0.7340\n",
      "Epoch 996/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8198 - acc: 0.8171 - val_loss: 0.9238 - val_acc: 0.7480\n",
      "Epoch 997/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8202 - acc: 0.8166 - val_loss: 0.9763 - val_acc: 0.7380\n",
      "Epoch 998/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8207 - acc: 0.8175 - val_loss: 0.9212 - val_acc: 0.7570\n",
      "Epoch 999/1000\n",
      "8000/8000 [==============================] - 0s 42us/step - loss: 0.8169 - acc: 0.8177 - val_loss: 0.9481 - val_acc: 0.7470\n",
      "Epoch 1000/1000\n",
      "8000/8000 [==============================] - 0s 43us/step - loss: 0.8168 - acc: 0.8166 - val_loss: 0.9199 - val_acc: 0.7450\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu',kernel_regularizer=regularizers.l1(0.005), input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, kernel_regularizer=regularizers.l1(0.005), activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L1_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=1000,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "L1_model_dict = L1_model.history\n",
    "plt.clf()\n",
    "\n",
    "acc_values = L1_model_dict['acc'] \n",
    "val_acc_values = L1_model_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L1')\n",
    "plt.plot(epochs, val_acc_values, 'g,', label='Validation acc L1')\n",
    "plt.title('Training & validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 24us/step\n",
      "1500/1500 [==============================] - 0s 26us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "\n",
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8237653533299764, 0.7967999999682108]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.966706668694814, 0.7499999998410543]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is about the best we've seen so far, but we were training for quite a while! Let's see if dropout regularization can do even better and/or be more efficient!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 1000 samples\n",
      "Epoch 1/200\n",
      "8000/8000 [==============================] - 1s 186us/step - loss: 1.9668 - acc: 0.1470 - val_loss: 1.9344 - val_acc: 0.1620 0s - loss: 1.9675 - acc: 0.146\n",
      "Epoch 2/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 1.9511 - acc: 0.1556 - val_loss: 1.9259 - val_acc: 0.1810\n",
      "Epoch 3/200\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 1.9412 - acc: 0.1584 - val_loss: 1.9189 - val_acc: 0.1940\n",
      "Epoch 4/200\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 1.9357 - acc: 0.1665 - val_loss: 1.9122 - val_acc: 0.2040\n",
      "Epoch 5/200\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 1.9264 - acc: 0.1792 - val_loss: 1.9053 - val_acc: 0.2100\n",
      "Epoch 6/200\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 1.9244 - acc: 0.177 - 0s 58us/step - loss: 1.9238 - acc: 0.1789 - val_loss: 1.8989 - val_acc: 0.2240\n",
      "Epoch 7/200\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 1.9175 - acc: 0.1807 - val_loss: 1.8920 - val_acc: 0.2270\n",
      "Epoch 8/200\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 1.9084 - acc: 0.1872 - val_loss: 1.8834 - val_acc: 0.2280\n",
      "Epoch 9/200\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 1.9027 - acc: 0.1945 - val_loss: 1.8748 - val_acc: 0.2340\n",
      "Epoch 10/200\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 1.8981 - acc: 0.2003 - val_loss: 1.8662 - val_acc: 0.2390\n",
      "Epoch 11/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 1.8880 - acc: 0.2006 - val_loss: 1.8576 - val_acc: 0.2450\n",
      "Epoch 12/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 1.8818 - acc: 0.2142 - val_loss: 1.8471 - val_acc: 0.2570\n",
      "Epoch 13/200\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 1.8744 - acc: 0.2170 - val_loss: 1.8357 - val_acc: 0.2620\n",
      "Epoch 14/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 1.8615 - acc: 0.2335 - val_loss: 1.8228 - val_acc: 0.2870\n",
      "Epoch 15/200\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 1.8533 - acc: 0.2279 - val_loss: 1.8092 - val_acc: 0.3080\n",
      "Epoch 16/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 1.8481 - acc: 0.2406 - val_loss: 1.7964 - val_acc: 0.3200\n",
      "Epoch 17/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 1.8397 - acc: 0.2516 - val_loss: 1.7820 - val_acc: 0.3380\n",
      "Epoch 18/200\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 1.8288 - acc: 0.2482 - val_loss: 1.7669 - val_acc: 0.3680\n",
      "Epoch 19/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 1.8106 - acc: 0.2771 - val_loss: 1.7493 - val_acc: 0.3770\n",
      "Epoch 20/200\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 1.7981 - acc: 0.2850 - val_loss: 1.7312 - val_acc: 0.3950\n",
      "Epoch 21/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 1.7925 - acc: 0.2920 - val_loss: 1.7152 - val_acc: 0.4220\n",
      "Epoch 22/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 1.7834 - acc: 0.2849 - val_loss: 1.6977 - val_acc: 0.4360\n",
      "Epoch 23/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 1.7625 - acc: 0.3049 - val_loss: 1.6751 - val_acc: 0.4520\n",
      "Epoch 24/200\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 1.7492 - acc: 0.3122 - val_loss: 1.6554 - val_acc: 0.4790\n",
      "Epoch 25/200\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 1.7395 - acc: 0.3209 - val_loss: 1.6345 - val_acc: 0.4880\n",
      "Epoch 26/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 1.7239 - acc: 0.3276 - val_loss: 1.6117 - val_acc: 0.5020\n",
      "Epoch 27/200\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 1.7096 - acc: 0.3351 - val_loss: 1.5898 - val_acc: 0.5040\n",
      "Epoch 28/200\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 1.6816 - acc: 0.3476 - val_loss: 1.5652 - val_acc: 0.5240\n",
      "Epoch 29/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 1.6645 - acc: 0.3679 - val_loss: 1.5385 - val_acc: 0.5350\n",
      "Epoch 30/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 1.6561 - acc: 0.3681 - val_loss: 1.5144 - val_acc: 0.5470\n",
      "Epoch 31/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 1.6376 - acc: 0.3764 - val_loss: 1.4912 - val_acc: 0.5560\n",
      "Epoch 32/200\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 1.6209 - acc: 0.3845 - val_loss: 1.4670 - val_acc: 0.5630\n",
      "Epoch 33/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 1.6016 - acc: 0.3867 - val_loss: 1.4441 - val_acc: 0.5790\n",
      "Epoch 34/200\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 1.5808 - acc: 0.3976 - val_loss: 1.4182 - val_acc: 0.5820\n",
      "Epoch 35/200\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 1.5824 - acc: 0.3911 - val_loss: 1.3966 - val_acc: 0.5870\n",
      "Epoch 36/200\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 1.5552 - acc: 0.4059 - val_loss: 1.3713 - val_acc: 0.5980\n",
      "Epoch 37/200\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 1.5385 - acc: 0.4170 - val_loss: 1.3486 - val_acc: 0.6100\n",
      "Epoch 38/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 1.5248 - acc: 0.4208 - val_loss: 1.3273 - val_acc: 0.6180\n",
      "Epoch 39/200\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 1.4948 - acc: 0.4385 - val_loss: 1.3036 - val_acc: 0.6230\n",
      "Epoch 40/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 1.4795 - acc: 0.4484 - val_loss: 1.2779 - val_acc: 0.6260\n",
      "Epoch 41/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 1.4684 - acc: 0.4483 - val_loss: 1.2593 - val_acc: 0.6350\n",
      "Epoch 42/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 1.4561 - acc: 0.4510 - val_loss: 1.2365 - val_acc: 0.6400\n",
      "Epoch 43/200\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 1.4391 - acc: 0.4694 - val_loss: 1.2157 - val_acc: 0.6430\n",
      "Epoch 44/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 1.4221 - acc: 0.4690 - val_loss: 1.1961 - val_acc: 0.6490\n",
      "Epoch 45/200\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 1.4080 - acc: 0.4645 - val_loss: 1.1788 - val_acc: 0.6580\n",
      "Epoch 46/200\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 1.4042 - acc: 0.4685 - val_loss: 1.1617 - val_acc: 0.6560\n",
      "Epoch 47/200\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 1.3844 - acc: 0.4811 - val_loss: 1.1442 - val_acc: 0.6610\n",
      "Epoch 48/200\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 1.3712 - acc: 0.4930 - val_loss: 1.1282 - val_acc: 0.6690\n",
      "Epoch 49/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 1.3618 - acc: 0.4979 - val_loss: 1.1111 - val_acc: 0.6750\n",
      "Epoch 50/200\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 1.3360 - acc: 0.5029 - val_loss: 1.0986 - val_acc: 0.6820\n",
      "Epoch 51/200\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 1.3380 - acc: 0.4909 - val_loss: 1.0832 - val_acc: 0.6800\n",
      "Epoch 52/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 1.3140 - acc: 0.5125 - val_loss: 1.0652 - val_acc: 0.6810\n",
      "Epoch 53/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 1.3109 - acc: 0.5109 - val_loss: 1.0530 - val_acc: 0.6860\n",
      "Epoch 54/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 1.2958 - acc: 0.5184 - val_loss: 1.0412 - val_acc: 0.6930\n",
      "Epoch 55/200\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 1.2869 - acc: 0.5145 - val_loss: 1.0276 - val_acc: 0.6900\n",
      "Epoch 56/200\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 1.2726 - acc: 0.5255 - val_loss: 1.0152 - val_acc: 0.6970\n",
      "Epoch 57/200\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 1.2717 - acc: 0.5302 - val_loss: 1.0070 - val_acc: 0.6990\n",
      "Epoch 58/200\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 1.2517 - acc: 0.5364 - val_loss: 0.9923 - val_acc: 0.6990\n",
      "Epoch 59/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 1.2334 - acc: 0.5386 - val_loss: 0.9794 - val_acc: 0.7010\n",
      "Epoch 60/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 57us/step - loss: 1.2316 - acc: 0.5349 - val_loss: 0.9702 - val_acc: 0.7020\n",
      "Epoch 61/200\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 1.2205 - acc: 0.5477 - val_loss: 0.9616 - val_acc: 0.7060\n",
      "Epoch 62/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 1.2188 - acc: 0.5505 - val_loss: 0.9514 - val_acc: 0.7030\n",
      "Epoch 63/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 1.2202 - acc: 0.5485 - val_loss: 0.9435 - val_acc: 0.7100\n",
      "Epoch 64/200\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 1.1886 - acc: 0.5525 - val_loss: 0.9323 - val_acc: 0.7090\n",
      "Epoch 65/200\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 1.1904 - acc: 0.5509 - val_loss: 0.9255 - val_acc: 0.7110\n",
      "Epoch 66/200\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 1.1868 - acc: 0.5564 - val_loss: 0.9156 - val_acc: 0.7100\n",
      "Epoch 67/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 1.1736 - acc: 0.5659 - val_loss: 0.9058 - val_acc: 0.7110\n",
      "Epoch 68/200\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 1.1650 - acc: 0.5720 - val_loss: 0.9007 - val_acc: 0.7150\n",
      "Epoch 69/200\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 1.1598 - acc: 0.5760 - val_loss: 0.8911 - val_acc: 0.7150\n",
      "Epoch 70/200\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 1.1541 - acc: 0.5743 - val_loss: 0.8803 - val_acc: 0.7180\n",
      "Epoch 71/200\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 1.1366 - acc: 0.5789 - val_loss: 0.8714 - val_acc: 0.7180\n",
      "Epoch 72/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 1.1398 - acc: 0.5744 - val_loss: 0.8666 - val_acc: 0.7220\n",
      "Epoch 73/200\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 1.1337 - acc: 0.5766 - val_loss: 0.8597 - val_acc: 0.7180\n",
      "Epoch 74/200\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 1.1230 - acc: 0.5861 - val_loss: 0.8525 - val_acc: 0.7250\n",
      "Epoch 75/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 1.1085 - acc: 0.5926 - val_loss: 0.8478 - val_acc: 0.7300\n",
      "Epoch 76/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 1.1079 - acc: 0.5879 - val_loss: 0.8409 - val_acc: 0.7250\n",
      "Epoch 77/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 1.1084 - acc: 0.5969 - val_loss: 0.8358 - val_acc: 0.7190\n",
      "Epoch 78/200\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 1.1019 - acc: 0.5886 - val_loss: 0.8325 - val_acc: 0.7260\n",
      "Epoch 79/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 1.0925 - acc: 0.5929 - val_loss: 0.8265 - val_acc: 0.7270\n",
      "Epoch 80/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 1.0941 - acc: 0.5945 - val_loss: 0.8240 - val_acc: 0.7210\n",
      "Epoch 81/200\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 1.0789 - acc: 0.6022 - val_loss: 0.8164 - val_acc: 0.7300\n",
      "Epoch 82/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 1.0709 - acc: 0.6040 - val_loss: 0.8106 - val_acc: 0.7280\n",
      "Epoch 83/200\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 1.0759 - acc: 0.6041 - val_loss: 0.8075 - val_acc: 0.7290\n",
      "Epoch 84/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 1.0604 - acc: 0.6096 - val_loss: 0.8037 - val_acc: 0.7270\n",
      "Epoch 85/200\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 1.0638 - acc: 0.6134 - val_loss: 0.7965 - val_acc: 0.7330\n",
      "Epoch 86/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 1.0528 - acc: 0.6160 - val_loss: 0.7890 - val_acc: 0.7350\n",
      "Epoch 87/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 1.0413 - acc: 0.6200 - val_loss: 0.7845 - val_acc: 0.7330\n",
      "Epoch 88/200\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 1.0394 - acc: 0.6118 - val_loss: 0.7830 - val_acc: 0.7320\n",
      "Epoch 89/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 1.0295 - acc: 0.6208 - val_loss: 0.7778 - val_acc: 0.7320\n",
      "Epoch 90/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 1.0327 - acc: 0.6198 - val_loss: 0.7740 - val_acc: 0.7310\n",
      "Epoch 91/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 1.0243 - acc: 0.6272 - val_loss: 0.7716 - val_acc: 0.7340\n",
      "Epoch 92/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 1.0194 - acc: 0.6256 - val_loss: 0.7655 - val_acc: 0.7340\n",
      "Epoch 93/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 1.0367 - acc: 0.6151 - val_loss: 0.7637 - val_acc: 0.7360\n",
      "Epoch 94/200\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 0.9970 - acc: 0.6298 - val_loss: 0.7597 - val_acc: 0.7340\n",
      "Epoch 95/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 1.0078 - acc: 0.6255 - val_loss: 0.7554 - val_acc: 0.7350\n",
      "Epoch 96/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 0.9986 - acc: 0.6325 - val_loss: 0.7505 - val_acc: 0.7360\n",
      "Epoch 97/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 1.0103 - acc: 0.6214 - val_loss: 0.7477 - val_acc: 0.7340\n",
      "Epoch 98/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 0.9903 - acc: 0.6305 - val_loss: 0.7436 - val_acc: 0.7400\n",
      "Epoch 99/200\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 1.0051 - acc: 0.6268 - val_loss: 0.7410 - val_acc: 0.7380\n",
      "Epoch 100/200\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 0.9886 - acc: 0.6362 - val_loss: 0.7362 - val_acc: 0.7410\n",
      "Epoch 101/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 0.9778 - acc: 0.6448 - val_loss: 0.7327 - val_acc: 0.7420\n",
      "Epoch 102/200\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 0.9912 - acc: 0.6358 - val_loss: 0.7330 - val_acc: 0.7410\n",
      "Epoch 103/200\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 0.9815 - acc: 0.6346 - val_loss: 0.7310 - val_acc: 0.7450\n",
      "Epoch 104/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 0.9748 - acc: 0.6358 - val_loss: 0.7268 - val_acc: 0.7430\n",
      "Epoch 105/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 0.9708 - acc: 0.6412 - val_loss: 0.7248 - val_acc: 0.7420\n",
      "Epoch 106/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 0.9703 - acc: 0.6410 - val_loss: 0.7227 - val_acc: 0.7470\n",
      "Epoch 107/200\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 0.9609 - acc: 0.6446 - val_loss: 0.7185 - val_acc: 0.7440\n",
      "Epoch 108/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 0.9569 - acc: 0.6471 - val_loss: 0.7154 - val_acc: 0.7450\n",
      "Epoch 109/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 0.9553 - acc: 0.6480 - val_loss: 0.7112 - val_acc: 0.7440\n",
      "Epoch 110/200\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 0.9576 - acc: 0.6512 - val_loss: 0.7102 - val_acc: 0.7440\n",
      "Epoch 111/200\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 0.9474 - acc: 0.6490 - val_loss: 0.7070 - val_acc: 0.7460\n",
      "Epoch 112/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 0.9382 - acc: 0.6516 - val_loss: 0.7066 - val_acc: 0.7460\n",
      "Epoch 113/200\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 0.9437 - acc: 0.6521 - val_loss: 0.7051 - val_acc: 0.7430\n",
      "Epoch 114/200\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 0.9438 - acc: 0.6571 - val_loss: 0.7018 - val_acc: 0.7430\n",
      "Epoch 115/200\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 0.9227 - acc: 0.6611 - val_loss: 0.6984 - val_acc: 0.7460\n",
      "Epoch 116/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 0.9277 - acc: 0.6591 - val_loss: 0.6972 - val_acc: 0.7490\n",
      "Epoch 117/200\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 0.9298 - acc: 0.6574 - val_loss: 0.6960 - val_acc: 0.7450\n",
      "Epoch 118/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 0.9192 - acc: 0.6583 - val_loss: 0.6927 - val_acc: 0.7480\n",
      "Epoch 119/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 55us/step - loss: 0.9304 - acc: 0.6598 - val_loss: 0.6924 - val_acc: 0.7480\n",
      "Epoch 120/200\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 0.9173 - acc: 0.6633 - val_loss: 0.6900 - val_acc: 0.7500\n",
      "Epoch 121/200\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 0.9239 - acc: 0.6606 - val_loss: 0.6888 - val_acc: 0.7490\n",
      "Epoch 122/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 0.9166 - acc: 0.6626 - val_loss: 0.6892 - val_acc: 0.7490\n",
      "Epoch 123/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 0.9198 - acc: 0.6654 - val_loss: 0.6839 - val_acc: 0.7480\n",
      "Epoch 124/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 0.9071 - acc: 0.6675 - val_loss: 0.6809 - val_acc: 0.7470\n",
      "Epoch 125/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 0.8995 - acc: 0.6720 - val_loss: 0.6779 - val_acc: 0.7480\n",
      "Epoch 126/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 0.9036 - acc: 0.6641 - val_loss: 0.6771 - val_acc: 0.7500\n",
      "Epoch 127/200\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 0.9036 - acc: 0.6661 - val_loss: 0.6759 - val_acc: 0.7510\n",
      "Epoch 128/200\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 0.8905 - acc: 0.6709 - val_loss: 0.6745 - val_acc: 0.7490\n",
      "Epoch 129/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 0.8925 - acc: 0.6663 - val_loss: 0.6730 - val_acc: 0.7540\n",
      "Epoch 130/200\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 0.8961 - acc: 0.6704 - val_loss: 0.6692 - val_acc: 0.7500\n",
      "Epoch 131/200\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 0.9036 - acc: 0.6654 - val_loss: 0.6704 - val_acc: 0.7520\n",
      "Epoch 132/200\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 0.8982 - acc: 0.6636 - val_loss: 0.6701 - val_acc: 0.7540\n",
      "Epoch 133/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 0.8895 - acc: 0.6765 - val_loss: 0.6654 - val_acc: 0.7470\n",
      "Epoch 134/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 0.8921 - acc: 0.6675 - val_loss: 0.6655 - val_acc: 0.7480\n",
      "Epoch 135/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 0.8764 - acc: 0.6754 - val_loss: 0.6646 - val_acc: 0.7530\n",
      "Epoch 136/200\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 0.8669 - acc: 0.6801 - val_loss: 0.6613 - val_acc: 0.7500\n",
      "Epoch 137/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 0.8932 - acc: 0.6688 - val_loss: 0.6602 - val_acc: 0.7530\n",
      "Epoch 138/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 0.8775 - acc: 0.6779 - val_loss: 0.6601 - val_acc: 0.7510\n",
      "Epoch 139/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 0.8578 - acc: 0.6906 - val_loss: 0.6572 - val_acc: 0.7540\n",
      "Epoch 140/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 0.8713 - acc: 0.6790 - val_loss: 0.6529 - val_acc: 0.7540\n",
      "Epoch 141/200\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 0.8557 - acc: 0.6894 - val_loss: 0.6525 - val_acc: 0.7550\n",
      "Epoch 142/200\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 0.8727 - acc: 0.6779 - val_loss: 0.6510 - val_acc: 0.7520\n",
      "Epoch 143/200\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 0.8478 - acc: 0.6911 - val_loss: 0.6503 - val_acc: 0.7540\n",
      "Epoch 144/200\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 0.8477 - acc: 0.6861 - val_loss: 0.6479 - val_acc: 0.7510\n",
      "Epoch 145/200\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 0.8466 - acc: 0.6849 - val_loss: 0.6465 - val_acc: 0.7580\n",
      "Epoch 146/200\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 0.8502 - acc: 0.6882 - val_loss: 0.6454 - val_acc: 0.7580\n",
      "Epoch 147/200\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 0.8621 - acc: 0.6863 - val_loss: 0.6466 - val_acc: 0.7580\n",
      "Epoch 148/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 0.8510 - acc: 0.6873 - val_loss: 0.6442 - val_acc: 0.7580\n",
      "Epoch 149/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 0.8446 - acc: 0.6809 - val_loss: 0.6448 - val_acc: 0.7580\n",
      "Epoch 150/200\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 0.8426 - acc: 0.6885 - val_loss: 0.6415 - val_acc: 0.7590\n",
      "Epoch 151/200\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 0.8478 - acc: 0.6868 - val_loss: 0.6387 - val_acc: 0.7590\n",
      "Epoch 152/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 0.8437 - acc: 0.6889 - val_loss: 0.6414 - val_acc: 0.7600\n",
      "Epoch 153/200\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 0.8448 - acc: 0.6887 - val_loss: 0.6407 - val_acc: 0.7570\n",
      "Epoch 154/200\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 0.8341 - acc: 0.6957 - val_loss: 0.6402 - val_acc: 0.7560\n",
      "Epoch 155/200\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 0.8401 - acc: 0.6897 - val_loss: 0.6402 - val_acc: 0.7570\n",
      "Epoch 156/200\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 0.8405 - acc: 0.6923 - val_loss: 0.6407 - val_acc: 0.7540\n",
      "Epoch 157/200\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 0.8236 - acc: 0.6953 - val_loss: 0.6362 - val_acc: 0.7570\n",
      "Epoch 158/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 0.8278 - acc: 0.6895 - val_loss: 0.6348 - val_acc: 0.7560\n",
      "Epoch 159/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 0.8345 - acc: 0.6936 - val_loss: 0.6326 - val_acc: 0.7580\n",
      "Epoch 160/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 0.8317 - acc: 0.6921 - val_loss: 0.6337 - val_acc: 0.7580\n",
      "Epoch 161/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 0.8152 - acc: 0.6975 - val_loss: 0.6329 - val_acc: 0.7550\n",
      "Epoch 162/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 0.8118 - acc: 0.7011 - val_loss: 0.6295 - val_acc: 0.7610\n",
      "Epoch 163/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 0.8288 - acc: 0.6955 - val_loss: 0.6315 - val_acc: 0.7550\n",
      "Epoch 164/200\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 0.8086 - acc: 0.6993 - val_loss: 0.6315 - val_acc: 0.7580\n",
      "Epoch 165/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 0.8202 - acc: 0.6987 - val_loss: 0.6279 - val_acc: 0.7590\n",
      "Epoch 166/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 0.8062 - acc: 0.7034 - val_loss: 0.6268 - val_acc: 0.7570\n",
      "Epoch 167/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 0.8009 - acc: 0.7025 - val_loss: 0.6227 - val_acc: 0.7600\n",
      "Epoch 168/200\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 0.8121 - acc: 0.6987 - val_loss: 0.6227 - val_acc: 0.7610\n",
      "Epoch 169/200\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 0.8041 - acc: 0.7056 - val_loss: 0.6213 - val_acc: 0.7650\n",
      "Epoch 170/200\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 0.7973 - acc: 0.7071 - val_loss: 0.6222 - val_acc: 0.7620\n",
      "Epoch 171/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 0.8048 - acc: 0.7061 - val_loss: 0.6216 - val_acc: 0.7660\n",
      "Epoch 172/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 0.7946 - acc: 0.7109 - val_loss: 0.6204 - val_acc: 0.7620\n",
      "Epoch 173/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 0.8097 - acc: 0.7026 - val_loss: 0.6203 - val_acc: 0.7610\n",
      "Epoch 174/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 0.7830 - acc: 0.7121 - val_loss: 0.6200 - val_acc: 0.7590\n",
      "Epoch 175/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 0.7875 - acc: 0.7160 - val_loss: 0.6183 - val_acc: 0.7620\n",
      "Epoch 176/200\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 0.7987 - acc: 0.7026 - val_loss: 0.6170 - val_acc: 0.7620\n",
      "Epoch 177/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 0.8039 - acc: 0.7064 - val_loss: 0.6168 - val_acc: 0.7630\n",
      "Epoch 178/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 56us/step - loss: 0.7836 - acc: 0.7094 - val_loss: 0.6154 - val_acc: 0.7610\n",
      "Epoch 179/200\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 0.7963 - acc: 0.7111 - val_loss: 0.6185 - val_acc: 0.7610\n",
      "Epoch 180/200\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 0.7752 - acc: 0.7130 - val_loss: 0.6172 - val_acc: 0.7600\n",
      "Epoch 181/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 0.7818 - acc: 0.7191 - val_loss: 0.6107 - val_acc: 0.7670\n",
      "Epoch 182/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 0.7918 - acc: 0.7146 - val_loss: 0.6116 - val_acc: 0.7640\n",
      "Epoch 183/200\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 0.7734 - acc: 0.719 - 0s 55us/step - loss: 0.7764 - acc: 0.7186 - val_loss: 0.6102 - val_acc: 0.7650\n",
      "Epoch 184/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 0.7848 - acc: 0.7099 - val_loss: 0.6128 - val_acc: 0.7600\n",
      "Epoch 185/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 0.7702 - acc: 0.7174 - val_loss: 0.6107 - val_acc: 0.7580\n",
      "Epoch 186/200\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 0.7680 - acc: 0.7185 - val_loss: 0.6088 - val_acc: 0.7610\n",
      "Epoch 187/200\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 0.7755 - acc: 0.7107 - val_loss: 0.6097 - val_acc: 0.7590\n",
      "Epoch 188/200\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 0.7723 - acc: 0.7154 - val_loss: 0.6081 - val_acc: 0.7620: 0s - loss: 0.7671 - acc: 0\n",
      "Epoch 189/200\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 0.7796 - acc: 0.7140 - val_loss: 0.6099 - val_acc: 0.7620\n",
      "Epoch 190/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 0.7707 - acc: 0.7137 - val_loss: 0.6082 - val_acc: 0.7610\n",
      "Epoch 191/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 0.7730 - acc: 0.7151 - val_loss: 0.6066 - val_acc: 0.7640\n",
      "Epoch 192/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 0.7676 - acc: 0.7196 - val_loss: 0.6057 - val_acc: 0.7620\n",
      "Epoch 193/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 0.7616 - acc: 0.7230 - val_loss: 0.6043 - val_acc: 0.7580\n",
      "Epoch 194/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 0.7563 - acc: 0.7258 - val_loss: 0.6034 - val_acc: 0.7600\n",
      "Epoch 195/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 0.7663 - acc: 0.7163 - val_loss: 0.6041 - val_acc: 0.7630\n",
      "Epoch 196/200\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 0.7677 - acc: 0.7201 - val_loss: 0.6047 - val_acc: 0.7620\n",
      "Epoch 197/200\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 0.7665 - acc: 0.7173 - val_loss: 0.6044 - val_acc: 0.7590\n",
      "Epoch 198/200\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 0.7664 - acc: 0.7152 - val_loss: 0.6018 - val_acc: 0.7660\n",
      "Epoch 199/200\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 0.7622 - acc: 0.7205 - val_loss: 0.6021 - val_acc: 0.7630\n",
      "Epoch 200/200\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 0.7527 - acc: 0.7204 - val_loss: 0.5987 - val_acc: 0.7620\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dropout(0.3, input_shape=(2000,)))\n",
    "model.add(layers.Dense(50, activation='relu')) #2 hidden layers\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "dropout_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=200,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dropout_model_dict = dropout_model.history\n",
    "plt.clf()\n",
    "\n",
    "acc_values = dropout_model_dict['acc'] \n",
    "val_acc_values = dropout_model_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc')\n",
    "plt.plot(epochs, val_acc_values, 'g,', label='Validation acc')\n",
    "plt.title('Training & validation accuracy Dropout')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 33us/step\n",
      "1000/1000 [==============================] - 0s 36us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4637945830821991, 0.836]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6121579809188843, 0.769]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see here that the validation performance has improved again! the variance did become higher again compared to L1-regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigger Data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture, one of the solutions to high variance was just getting more data. We actually *have* more data, but took a subset of 10,000 units before. Let's now quadruple our data set, and see what happens. Note that we are really just lucky here, and getting more data isn't always possible, but this is a useful exercise in order to understand the power of big data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Bank_complaints.csv')\n",
    "random.seed(123)\n",
    "df = df.sample(40000)\n",
    "df.index = range(40000)\n",
    "product = df[\"Product\"]\n",
    "complaints = df[\"Consumer complaint narrative\"]\n",
    "\n",
    "#one-hot encoding of the complaints\n",
    "tokenizer = Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(complaints)\n",
    "sequences = tokenizer.texts_to_sequences(complaints)\n",
    "one_hot_results= tokenizer.texts_to_matrix(complaints, mode='binary')\n",
    "word_index = tokenizer.word_index\n",
    "np.shape(one_hot_results)\n",
    "\n",
    "#one-hot encoding of products\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(product)\n",
    "list(le.classes_)\n",
    "product_cat = le.transform(product) \n",
    "product_onehot = to_categorical(product_cat)\n",
    "\n",
    "# train test split\n",
    "test_index = random.sample(range(1,40000), 4000)\n",
    "test = one_hot_results[test_index]\n",
    "train = np.delete(one_hot_results, test_index, 0)\n",
    "label_test = product_onehot[test_index]\n",
    "label_train = np.delete(product_onehot, test_index, 0)\n",
    "\n",
    "#Validation set\n",
    "random.seed(123)\n",
    "val = train[:3000]\n",
    "train_final = train[3000:]\n",
    "label_val = label_train[:3000]\n",
    "label_train_final = label_train[3000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 33000 samples, validate on 3000 samples\n",
      "Epoch 1/120\n",
      "33000/33000 [==============================] - 2s 63us/step - loss: 1.9038 - acc: 0.2067 - val_loss: 1.8667 - val_acc: 0.2417\n",
      "Epoch 2/120\n",
      "33000/33000 [==============================] - 1s 39us/step - loss: 1.8195 - acc: 0.2877 - val_loss: 1.7589 - val_acc: 0.3473\n",
      "Epoch 3/120\n",
      "33000/33000 [==============================] - 1s 41us/step - loss: 1.6767 - acc: 0.4079 - val_loss: 1.5814 - val_acc: 0.4827\n",
      "Epoch 4/120\n",
      "33000/33000 [==============================] - ETA: 0s - loss: 1.4818 - acc: 0.523 - 1s 40us/step - loss: 1.4790 - acc: 0.5246 - val_loss: 1.3704 - val_acc: 0.5763\n",
      "Epoch 5/120\n",
      "33000/33000 [==============================] - 1s 41us/step - loss: 1.2650 - acc: 0.6138 - val_loss: 1.1672 - val_acc: 0.6423\n",
      "Epoch 6/120\n",
      "33000/33000 [==============================] - 1s 40us/step - loss: 1.0804 - acc: 0.6700 - val_loss: 1.0129 - val_acc: 0.6783\n",
      "Epoch 7/120\n",
      "33000/33000 [==============================] - 1s 39us/step - loss: 0.9467 - acc: 0.7001 - val_loss: 0.9054 - val_acc: 0.7013\n",
      "Epoch 8/120\n",
      "33000/33000 [==============================] - 1s 39us/step - loss: 0.8544 - acc: 0.7205 - val_loss: 0.8322 - val_acc: 0.7203\n",
      "Epoch 9/120\n",
      "33000/33000 [==============================] - 1s 39us/step - loss: 0.7897 - acc: 0.7340 - val_loss: 0.7790 - val_acc: 0.7373\n",
      "Epoch 10/120\n",
      "33000/33000 [==============================] - 1s 39us/step - loss: 0.7430 - acc: 0.7440 - val_loss: 0.7403 - val_acc: 0.7433\n",
      "Epoch 11/120\n",
      "33000/33000 [==============================] - 1s 41us/step - loss: 0.7077 - acc: 0.7515 - val_loss: 0.7128 - val_acc: 0.7473loss: 0.7134 \n",
      "Epoch 12/120\n",
      "33000/33000 [==============================] - 1s 41us/step - loss: 0.6805 - acc: 0.7583 - val_loss: 0.6892 - val_acc: 0.7527\n",
      "Epoch 13/120\n",
      "33000/33000 [==============================] - 1s 39us/step - loss: 0.6583 - acc: 0.7630 - val_loss: 0.6719 - val_acc: 0.7557\n",
      "Epoch 14/120\n",
      "33000/33000 [==============================] - 1s 41us/step - loss: 0.6397 - acc: 0.7692 - val_loss: 0.6572 - val_acc: 0.7607\n",
      "Epoch 15/120\n",
      "33000/33000 [==============================] - 1s 39us/step - loss: 0.6239 - acc: 0.7743 - val_loss: 0.6461 - val_acc: 0.7590\n",
      "Epoch 16/120\n",
      "33000/33000 [==============================] - ETA: 0s - loss: 0.6103 - acc: 0.778 - 1s 40us/step - loss: 0.6099 - acc: 0.7788 - val_loss: 0.6341 - val_acc: 0.7660\n",
      "Epoch 17/120\n",
      "33000/33000 [==============================] - 1s 39us/step - loss: 0.5978 - acc: 0.7830 - val_loss: 0.6250 - val_acc: 0.7693\n",
      "Epoch 18/120\n",
      "33000/33000 [==============================] - 1s 39us/step - loss: 0.5870 - acc: 0.7866 - val_loss: 0.6176 - val_acc: 0.7727\n",
      "Epoch 19/120\n",
      "33000/33000 [==============================] - 1s 39us/step - loss: 0.5769 - acc: 0.7911 - val_loss: 0.6090 - val_acc: 0.7720\n",
      "Epoch 20/120\n",
      "33000/33000 [==============================] - 1s 41us/step - loss: 0.5676 - acc: 0.7927 - val_loss: 0.6030 - val_acc: 0.7727\n",
      "Epoch 21/120\n",
      "33000/33000 [==============================] - 1s 39us/step - loss: 0.5590 - acc: 0.7971 - val_loss: 0.5998 - val_acc: 0.7800\n",
      "Epoch 22/120\n",
      "33000/33000 [==============================] - 1s 40us/step - loss: 0.5508 - acc: 0.7998 - val_loss: 0.5936 - val_acc: 0.7780\n",
      "Epoch 23/120\n",
      "33000/33000 [==============================] - 1s 40us/step - loss: 0.5434 - acc: 0.8023 - val_loss: 0.5888 - val_acc: 0.7830\n",
      "Epoch 24/120\n",
      "33000/33000 [==============================] - 1s 41us/step - loss: 0.5363 - acc: 0.8052 - val_loss: 0.5846 - val_acc: 0.7860\n",
      "Epoch 25/120\n",
      "33000/33000 [==============================] - 1s 41us/step - loss: 0.5296 - acc: 0.8077 - val_loss: 0.5785 - val_acc: 0.7887\n",
      "Epoch 26/120\n",
      "33000/33000 [==============================] - 1s 41us/step - loss: 0.5231 - acc: 0.8106 - val_loss: 0.5788 - val_acc: 0.7890\n",
      "Epoch 27/120\n",
      "33000/33000 [==============================] - 1s 42us/step - loss: 0.5174 - acc: 0.8132 - val_loss: 0.5744 - val_acc: 0.7910\n",
      "Epoch 28/120\n",
      "33000/33000 [==============================] - 1s 42us/step - loss: 0.5115 - acc: 0.8164 - val_loss: 0.5712 - val_acc: 0.7927\n",
      "Epoch 29/120\n",
      "33000/33000 [==============================] - 1s 42us/step - loss: 0.5061 - acc: 0.8177 - val_loss: 0.5693 - val_acc: 0.7933\n",
      "Epoch 30/120\n",
      "33000/33000 [==============================] - ETA: 0s - loss: 0.5013 - acc: 0.821 - 1s 40us/step - loss: 0.5008 - acc: 0.8203 - val_loss: 0.5638 - val_acc: 0.7967\n",
      "Epoch 31/120\n",
      "33000/33000 [==============================] - 1s 45us/step - loss: 0.4957 - acc: 0.8212 - val_loss: 0.5612 - val_acc: 0.7977\n",
      "Epoch 32/120\n",
      "33000/33000 [==============================] - 1s 41us/step - loss: 0.4909 - acc: 0.8232 - val_loss: 0.5597 - val_acc: 0.7950\n",
      "Epoch 33/120\n",
      "33000/33000 [==============================] - 1s 41us/step - loss: 0.4861 - acc: 0.8254 - val_loss: 0.5589 - val_acc: 0.7967\n",
      "Epoch 34/120\n",
      "33000/33000 [==============================] - 1s 40us/step - loss: 0.4818 - acc: 0.8269 - val_loss: 0.5545 - val_acc: 0.8013\n",
      "Epoch 35/120\n",
      "33000/33000 [==============================] - 1s 41us/step - loss: 0.4773 - acc: 0.8285 - val_loss: 0.5558 - val_acc: 0.7967\n",
      "Epoch 36/120\n",
      "33000/33000 [==============================] - 1s 41us/step - loss: 0.4732 - acc: 0.8302 - val_loss: 0.5517 - val_acc: 0.8003\n",
      "Epoch 37/120\n",
      "33000/33000 [==============================] - 1s 42us/step - loss: 0.4693 - acc: 0.8316 - val_loss: 0.5504 - val_acc: 0.8017\n",
      "Epoch 38/120\n",
      "33000/33000 [==============================] - 1s 44us/step - loss: 0.4653 - acc: 0.8343 - val_loss: 0.5488 - val_acc: 0.8027\n",
      "Epoch 39/120\n",
      "33000/33000 [==============================] - 1s 41us/step - loss: 0.4611 - acc: 0.8352 - val_loss: 0.5478 - val_acc: 0.8010\n",
      "Epoch 40/120\n",
      "33000/33000 [==============================] - 1s 40us/step - loss: 0.4575 - acc: 0.8371 - val_loss: 0.5458 - val_acc: 0.8050\n",
      "Epoch 41/120\n",
      "33000/33000 [==============================] - 1s 40us/step - loss: 0.4541 - acc: 0.8383 - val_loss: 0.5482 - val_acc: 0.7997\n",
      "Epoch 42/120\n",
      "33000/33000 [==============================] - 1s 40us/step - loss: 0.4505 - acc: 0.8406 - val_loss: 0.5483 - val_acc: 0.7987\n",
      "Epoch 43/120\n",
      "33000/33000 [==============================] - 1s 39us/step - loss: 0.4473 - acc: 0.8411 - val_loss: 0.5449 - val_acc: 0.8010\n",
      "Epoch 44/120\n",
      "33000/33000 [==============================] - 1s 40us/step - loss: 0.4438 - acc: 0.8429 - val_loss: 0.5449 - val_acc: 0.8040\n",
      "Epoch 45/120\n",
      "33000/33000 [==============================] - ETA: 0s - loss: 0.4411 - acc: 0.8436- ETA: 0s - loss: 0.4414 - acc: 0.8 - 1s 40us/step - loss: 0.4409 - acc: 0.8434 - val_loss: 0.5441 - val_acc: 0.8023\n",
      "Epoch 46/120\n",
      "33000/33000 [==============================] - 1s 39us/step - loss: 0.4376 - acc: 0.8444 - val_loss: 0.5412 - val_acc: 0.8063\n",
      "Epoch 47/120\n",
      "33000/33000 [==============================] - 1s 40us/step - loss: 0.4347 - acc: 0.8459 - val_loss: 0.5409 - val_acc: 0.8047\n",
      "Epoch 48/120\n",
      "33000/33000 [==============================] - 1s 39us/step - loss: 0.4315 - acc: 0.8474 - val_loss: 0.5418 - val_acc: 0.8047\n",
      "Epoch 49/120\n",
      "33000/33000 [==============================] - 1s 40us/step - loss: 0.4288 - acc: 0.8485 - val_loss: 0.5413 - val_acc: 0.8050\n",
      "Epoch 50/120\n",
      "33000/33000 [==============================] - 1s 39us/step - loss: 0.4259 - acc: 0.8491 - val_loss: 0.5393 - val_acc: 0.8087\n",
      "Epoch 51/120\n",
      "33000/33000 [==============================] - 1s 40us/step - loss: 0.4230 - acc: 0.8501 - val_loss: 0.5438 - val_acc: 0.8060\n",
      "Epoch 52/120\n",
      "33000/33000 [==============================] - 1s 40us/step - loss: 0.4204 - acc: 0.8512 - val_loss: 0.5400 - val_acc: 0.8087\n",
      "Epoch 53/120\n",
      "33000/33000 [==============================] - 1s 40us/step - loss: 0.4178 - acc: 0.8520 - val_loss: 0.5443 - val_acc: 0.8067\n",
      "Epoch 54/120\n",
      "33000/33000 [==============================] - ETA: 0s - loss: 0.4161 - acc: 0.852 - 1s 40us/step - loss: 0.4156 - acc: 0.8526 - val_loss: 0.5397 - val_acc: 0.8077\n",
      "Epoch 55/120\n",
      "33000/33000 [==============================] - 1s 39us/step - loss: 0.4128 - acc: 0.8535 - val_loss: 0.5361 - val_acc: 0.8113\n",
      "Epoch 56/120\n",
      "33000/33000 [==============================] - 1s 40us/step - loss: 0.4102 - acc: 0.8548 - val_loss: 0.5377 - val_acc: 0.8097\n",
      "Epoch 57/120\n",
      "33000/33000 [==============================] - 1s 40us/step - loss: 0.4079 - acc: 0.8554 - val_loss: 0.5378 - val_acc: 0.8087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/120\n",
      "33000/33000 [==============================] - 1s 39us/step - loss: 0.4057 - acc: 0.8571 - val_loss: 0.5385 - val_acc: 0.8137\n",
      "Epoch 59/120\n",
      "33000/33000 [==============================] - 1s 41us/step - loss: 0.4031 - acc: 0.8578 - val_loss: 0.5375 - val_acc: 0.8120\n",
      "Epoch 60/120\n",
      "33000/33000 [==============================] - 1s 40us/step - loss: 0.4006 - acc: 0.8582 - val_loss: 0.5383 - val_acc: 0.8113\n",
      "Epoch 61/120\n",
      "33000/33000 [==============================] - 1s 39us/step - loss: 0.3983 - acc: 0.8594 - val_loss: 0.5376 - val_acc: 0.8140\n",
      "Epoch 62/120\n",
      "33000/33000 [==============================] - 1s 40us/step - loss: 0.3961 - acc: 0.8602 - val_loss: 0.5401 - val_acc: 0.8097\n",
      "Epoch 63/120\n",
      "33000/33000 [==============================] - 1s 39us/step - loss: 0.3941 - acc: 0.8618 - val_loss: 0.5373 - val_acc: 0.8150\n",
      "Epoch 64/120\n",
      "33000/33000 [==============================] - 1s 39us/step - loss: 0.3918 - acc: 0.8621 - val_loss: 0.5372 - val_acc: 0.8150\n",
      "Epoch 65/120\n",
      "33000/33000 [==============================] - 1s 40us/step - loss: 0.3898 - acc: 0.8619 - val_loss: 0.5377 - val_acc: 0.8130\n",
      "Epoch 66/120\n",
      "33000/33000 [==============================] - 1s 39us/step - loss: 0.3876 - acc: 0.8629 - val_loss: 0.5356 - val_acc: 0.8170\n",
      "Epoch 67/120\n",
      "33000/33000 [==============================] - 1s 40us/step - loss: 0.3860 - acc: 0.8635 - val_loss: 0.5415 - val_acc: 0.8100\n",
      "Epoch 68/120\n",
      "33000/33000 [==============================] - 1s 39us/step - loss: 0.3835 - acc: 0.8642 - val_loss: 0.5377 - val_acc: 0.8200\n",
      "Epoch 69/120\n",
      "33000/33000 [==============================] - 1s 39us/step - loss: 0.3818 - acc: 0.8649 - val_loss: 0.5415 - val_acc: 0.8113\n",
      "Epoch 70/120\n",
      "33000/33000 [==============================] - 1s 40us/step - loss: 0.3798 - acc: 0.8655 - val_loss: 0.5388 - val_acc: 0.8143\n",
      "Epoch 71/120\n",
      "33000/33000 [==============================] - 1s 40us/step - loss: 0.3779 - acc: 0.8669 - val_loss: 0.5414 - val_acc: 0.8150\n",
      "Epoch 72/120\n",
      "33000/33000 [==============================] - 1s 40us/step - loss: 0.3763 - acc: 0.8668 - val_loss: 0.5394 - val_acc: 0.8173\n",
      "Epoch 73/120\n",
      "33000/33000 [==============================] - 1s 40us/step - loss: 0.3740 - acc: 0.8687 - val_loss: 0.5417 - val_acc: 0.8160\n",
      "Epoch 74/120\n",
      "33000/33000 [==============================] - 1s 40us/step - loss: 0.3723 - acc: 0.8680 - val_loss: 0.5422 - val_acc: 0.8187\n",
      "Epoch 75/120\n",
      "33000/33000 [==============================] - 1s 40us/step - loss: 0.3707 - acc: 0.8700 - val_loss: 0.5421 - val_acc: 0.8140\n",
      "Epoch 76/120\n",
      "33000/33000 [==============================] - 1s 39us/step - loss: 0.3691 - acc: 0.8704 - val_loss: 0.5481 - val_acc: 0.8120\n",
      "Epoch 77/120\n",
      "33000/33000 [==============================] - 1s 40us/step - loss: 0.3672 - acc: 0.8701 - val_loss: 0.5423 - val_acc: 0.8170\n",
      "Epoch 78/120\n",
      "33000/33000 [==============================] - 1s 39us/step - loss: 0.3653 - acc: 0.8713 - val_loss: 0.5515 - val_acc: 0.8103\n",
      "Epoch 79/120\n",
      "33000/33000 [==============================] - 1s 39us/step - loss: 0.3635 - acc: 0.8708 - val_loss: 0.5438 - val_acc: 0.8157\n",
      "Epoch 80/120\n",
      "33000/33000 [==============================] - 1s 39us/step - loss: 0.3619 - acc: 0.8722 - val_loss: 0.5468 - val_acc: 0.8133\n",
      "Epoch 81/120\n",
      "33000/33000 [==============================] - 1s 39us/step - loss: 0.3601 - acc: 0.8726 - val_loss: 0.5471 - val_acc: 0.8147\n",
      "Epoch 82/120\n",
      "33000/33000 [==============================] - ETA: 0s - loss: 0.3597 - acc: 0.8734- ETA: 1s - loss: 0 - 1s 40us/step - loss: 0.3586 - acc: 0.8738 - val_loss: 0.5426 - val_acc: 0.8200\n",
      "Epoch 83/120\n",
      "33000/33000 [==============================] - 1s 39us/step - loss: 0.3568 - acc: 0.8745 - val_loss: 0.5485 - val_acc: 0.8137\n",
      "Epoch 84/120\n",
      "33000/33000 [==============================] - ETA: 0s - loss: 0.3561 - acc: 0.874 - 1s 40us/step - loss: 0.3553 - acc: 0.8746 - val_loss: 0.5452 - val_acc: 0.8177\n",
      "Epoch 85/120\n",
      "33000/33000 [==============================] - ETA: 0s - loss: 0.3531 - acc: 0.8761- ETA: 1s - loss: 0 - 1s 40us/step - loss: 0.3538 - acc: 0.8762 - val_loss: 0.5453 - val_acc: 0.8163\n",
      "Epoch 86/120\n",
      "33000/33000 [==============================] - 1s 40us/step - loss: 0.3519 - acc: 0.8755 - val_loss: 0.5448 - val_acc: 0.8200\n",
      "Epoch 87/120\n",
      "33000/33000 [==============================] - 1s 40us/step - loss: 0.3508 - acc: 0.8771 - val_loss: 0.5495 - val_acc: 0.8150\n",
      "Epoch 88/120\n",
      "33000/33000 [==============================] - 1s 40us/step - loss: 0.3492 - acc: 0.8774 - val_loss: 0.5494 - val_acc: 0.8150\n",
      "Epoch 89/120\n",
      "33000/33000 [==============================] - 1s 40us/step - loss: 0.3476 - acc: 0.8775 - val_loss: 0.5519 - val_acc: 0.8147\n",
      "Epoch 90/120\n",
      "33000/33000 [==============================] - 1s 40us/step - loss: 0.3459 - acc: 0.8783 - val_loss: 0.5504 - val_acc: 0.8183\n",
      "Epoch 91/120\n",
      "33000/33000 [==============================] - 1s 40us/step - loss: 0.3446 - acc: 0.8798 - val_loss: 0.5500 - val_acc: 0.8170\n",
      "Epoch 92/120\n",
      "33000/33000 [==============================] - 1s 40us/step - loss: 0.3432 - acc: 0.8794 - val_loss: 0.5525 - val_acc: 0.8153\n",
      "Epoch 93/120\n",
      "33000/33000 [==============================] - 1s 39us/step - loss: 0.3415 - acc: 0.8804 - val_loss: 0.5565 - val_acc: 0.8147\n",
      "Epoch 94/120\n",
      "33000/33000 [==============================] - 1s 40us/step - loss: 0.3399 - acc: 0.8811 - val_loss: 0.5523 - val_acc: 0.8167\n",
      "Epoch 95/120\n",
      "33000/33000 [==============================] - 1s 39us/step - loss: 0.3386 - acc: 0.8813 - val_loss: 0.5543 - val_acc: 0.8163\n",
      "Epoch 96/120\n",
      "33000/33000 [==============================] - 1s 39us/step - loss: 0.3372 - acc: 0.8818 - val_loss: 0.5567 - val_acc: 0.8163\n",
      "Epoch 97/120\n",
      "33000/33000 [==============================] - 1s 39us/step - loss: 0.3360 - acc: 0.8825 - val_loss: 0.5559 - val_acc: 0.8177\n",
      "Epoch 98/120\n",
      "33000/33000 [==============================] - 1s 41us/step - loss: 0.3345 - acc: 0.8824 - val_loss: 0.5557 - val_acc: 0.8163\n",
      "Epoch 99/120\n",
      "33000/33000 [==============================] - 1s 40us/step - loss: 0.3333 - acc: 0.8830 - val_loss: 0.5566 - val_acc: 0.8147\n",
      "Epoch 100/120\n",
      "33000/33000 [==============================] - ETA: 0s - loss: 0.3322 - acc: 0.883 - 1s 40us/step - loss: 0.3319 - acc: 0.8838 - val_loss: 0.5596 - val_acc: 0.8140\n",
      "Epoch 101/120\n",
      "33000/33000 [==============================] - 1s 40us/step - loss: 0.3307 - acc: 0.8830 - val_loss: 0.5624 - val_acc: 0.8133\n",
      "Epoch 102/120\n",
      "33000/33000 [==============================] - 1s 40us/step - loss: 0.3290 - acc: 0.8845 - val_loss: 0.5636 - val_acc: 0.8150\n",
      "Epoch 103/120\n",
      "33000/33000 [==============================] - 1s 44us/step - loss: 0.3275 - acc: 0.8864 - val_loss: 0.5630 - val_acc: 0.8123\n",
      "Epoch 104/120\n",
      "33000/33000 [==============================] - 1s 40us/step - loss: 0.3266 - acc: 0.8860 - val_loss: 0.5609 - val_acc: 0.8140\n",
      "Epoch 105/120\n",
      "33000/33000 [==============================] - 1s 40us/step - loss: 0.3249 - acc: 0.8864 - val_loss: 0.5628 - val_acc: 0.8113\n",
      "Epoch 106/120\n",
      "33000/33000 [==============================] - 1s 39us/step - loss: 0.3235 - acc: 0.8865 - val_loss: 0.5655 - val_acc: 0.8157\n",
      "Epoch 107/120\n",
      "33000/33000 [==============================] - 1s 40us/step - loss: 0.3226 - acc: 0.8876 - val_loss: 0.5658 - val_acc: 0.8160\n",
      "Epoch 108/120\n",
      "33000/33000 [==============================] - 1s 39us/step - loss: 0.3211 - acc: 0.8878 - val_loss: 0.5687 - val_acc: 0.8147\n",
      "Epoch 109/120\n",
      "33000/33000 [==============================] - 1s 39us/step - loss: 0.3199 - acc: 0.8887 - val_loss: 0.5684 - val_acc: 0.8140\n",
      "Epoch 110/120\n",
      "33000/33000 [==============================] - 1s 39us/step - loss: 0.3184 - acc: 0.8888 - val_loss: 0.5676 - val_acc: 0.8150\n",
      "Epoch 111/120\n",
      "33000/33000 [==============================] - 1s 39us/step - loss: 0.3172 - acc: 0.8892 - val_loss: 0.5688 - val_acc: 0.8107\n",
      "Epoch 112/120\n",
      "33000/33000 [==============================] - 1s 39us/step - loss: 0.3161 - acc: 0.8888 - val_loss: 0.5717 - val_acc: 0.8137\n",
      "Epoch 113/120\n",
      "33000/33000 [==============================] - 1s 39us/step - loss: 0.3146 - acc: 0.8905 - val_loss: 0.5708 - val_acc: 0.8110\n",
      "Epoch 114/120\n",
      "33000/33000 [==============================] - 1s 40us/step - loss: 0.3136 - acc: 0.8900 - val_loss: 0.5710 - val_acc: 0.8157\n",
      "Epoch 115/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33000/33000 [==============================] - 1s 40us/step - loss: 0.3126 - acc: 0.8917 - val_loss: 0.5712 - val_acc: 0.8127\n",
      "Epoch 116/120\n",
      "33000/33000 [==============================] - 1s 40us/step - loss: 0.3115 - acc: 0.8911 - val_loss: 0.5749 - val_acc: 0.8130\n",
      "Epoch 117/120\n",
      "33000/33000 [==============================] - 1s 39us/step - loss: 0.3100 - acc: 0.8919 - val_loss: 0.5789 - val_acc: 0.8130\n",
      "Epoch 118/120\n",
      "33000/33000 [==============================] - 1s 38us/step - loss: 0.3092 - acc: 0.8920 - val_loss: 0.5746 - val_acc: 0.8110\n",
      "Epoch 119/120\n",
      "33000/33000 [==============================] - 1s 38us/step - loss: 0.3076 - acc: 0.8932 - val_loss: 0.5772 - val_acc: 0.8130\n",
      "Epoch 120/120\n",
      "33000/33000 [==============================] - 1s 39us/step - loss: 0.3064 - acc: 0.8937 - val_loss: 0.5799 - val_acc: 0.8123\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu', input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "moredata_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33000/33000 [==============================] - 1s 30us/step\n",
      "4000/4000 [==============================] - 0s 32us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "results_test = model.evaluate(test, label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.30179671092105637, 0.8958484848484849]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5763684837818146, 0.79875]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the same amount of epochs, we were able to get a fairly similar validation accuracy of 89.67 (compared to 88.55 in obtained in the first model in this lab). Our test set accuracy went up from 75.8 to a staggering 80.225% though, without any other regularization technique. You can still consider early stopping, L1, L2 and dropout here. It's clear that having more data has a strong impact on model performance!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "* https://github.com/susanli2016/Machine-Learning-with-Python/blob/master/Consumer_complaints.ipynb\n",
    "* https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/\n",
    "* https://catalog.data.gov/dataset/consumer-complaint-database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary  \n",
    "\n",
    "In this lesson, we not only built an initial deep-learning model, we then used a validation set to tune our model using various types of regularization. From here, we'll continue to describe more practice and theory regarding tuning and optimizing deep-learning networks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
